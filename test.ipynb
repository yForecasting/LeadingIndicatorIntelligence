{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0369c9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwight/miniconda3/lib/python3.8/site-packages/tslearn/clustering/kmeans.py:16: UserWarning: Scikit-learn <0.24 will be deprecated in a future release of tslearn\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Native libraries\n",
    "import os\n",
    "import math\n",
    "# Essential Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fredapi import Fred\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "# Algorithms\n",
    "from statistics import mean, stdev, variance\n",
    "from minisom import MiniSom\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.cluster import KMeans, MeanShift, estimate_bandwidth\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "703bb0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"monthly_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7d14fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fred = Fred(api_key='3c6ba58d26525f17af95af4fabed24be')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c88e827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv2 = csv[50:-20]\n",
    "csv2 = csv2.drop(['SMU06419406562200001.1', 'SMU21000006056170001SA.1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01deecf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNRATE</th>\n",
       "      <th>CPIAUCSL</th>\n",
       "      <th>FEDFUNDS</th>\n",
       "      <th>CSUSHPINSA</th>\n",
       "      <th>M2SL</th>\n",
       "      <th>M1SL</th>\n",
       "      <th>PSAVERT</th>\n",
       "      <th>PAYEMS</th>\n",
       "      <th>INDPRO</th>\n",
       "      <th>TB3MS</th>\n",
       "      <th>...</th>\n",
       "      <th>ESTPIEAMP01GPM</th>\n",
       "      <th>BROSERPA158MFRBDAL</th>\n",
       "      <th>LARTTULA158MFRBDAL</th>\n",
       "      <th>LAUCN310910000000004</th>\n",
       "      <th>LAUCN271470000000004</th>\n",
       "      <th>LFWATTFECAM647N</th>\n",
       "      <th>LFWA55FECAM647S</th>\n",
       "      <th>LFWA55FEKRM647S</th>\n",
       "      <th>LFWA55MAAUM647N</th>\n",
       "      <th>LFWA55MAAUM647S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.4</td>\n",
       "      <td>191.700</td>\n",
       "      <td>1.93</td>\n",
       "      <td>157.497</td>\n",
       "      <td>6399.4</td>\n",
       "      <td>1374.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>132503.0</td>\n",
       "      <td>97.6137</td>\n",
       "      <td>2.07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.202429</td>\n",
       "      <td>-1.713722</td>\n",
       "      <td>2.689583</td>\n",
       "      <td>12.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>12971900.0</td>\n",
       "      <td>1725500.0</td>\n",
       "      <td>2.143286e+06</td>\n",
       "      <td>1063251.0</td>\n",
       "      <td>1.062949e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.4</td>\n",
       "      <td>192.400</td>\n",
       "      <td>2.50</td>\n",
       "      <td>161.924</td>\n",
       "      <td>6432.5</td>\n",
       "      <td>1371.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>133032.0</td>\n",
       "      <td>99.4639</td>\n",
       "      <td>2.54</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201410</td>\n",
       "      <td>1.561165</td>\n",
       "      <td>16.658304</td>\n",
       "      <td>25.0</td>\n",
       "      <td>983.0</td>\n",
       "      <td>13010400.0</td>\n",
       "      <td>1745200.0</td>\n",
       "      <td>2.138804e+06</td>\n",
       "      <td>1072885.0</td>\n",
       "      <td>1.073819e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.1</td>\n",
       "      <td>193.600</td>\n",
       "      <td>3.00</td>\n",
       "      <td>169.544</td>\n",
       "      <td>6473.1</td>\n",
       "      <td>1366.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>133690.0</td>\n",
       "      <td>99.6033</td>\n",
       "      <td>2.84</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100604</td>\n",
       "      <td>1.090706</td>\n",
       "      <td>-4.866917</td>\n",
       "      <td>11.0</td>\n",
       "      <td>747.0</td>\n",
       "      <td>13057400.0</td>\n",
       "      <td>1763600.0</td>\n",
       "      <td>2.166473e+06</td>\n",
       "      <td>1081863.0</td>\n",
       "      <td>1.081815e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>196.100</td>\n",
       "      <td>3.50</td>\n",
       "      <td>175.922</td>\n",
       "      <td>6569.8</td>\n",
       "      <td>1377.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>134498.0</td>\n",
       "      <td>99.9435</td>\n",
       "      <td>3.44</td>\n",
       "      <td>...</td>\n",
       "      <td>1.206030</td>\n",
       "      <td>9.230125</td>\n",
       "      <td>-3.218818</td>\n",
       "      <td>9.0</td>\n",
       "      <td>726.0</td>\n",
       "      <td>13110300.0</td>\n",
       "      <td>1782100.0</td>\n",
       "      <td>2.175409e+06</td>\n",
       "      <td>1090390.0</td>\n",
       "      <td>1.090723e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>198.100</td>\n",
       "      <td>4.00</td>\n",
       "      <td>179.675</td>\n",
       "      <td>6654.5</td>\n",
       "      <td>1376.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>134993.0</td>\n",
       "      <td>100.3216</td>\n",
       "      <td>3.88</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.638735</td>\n",
       "      <td>6.844361</td>\n",
       "      <td>8.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>13152700.0</td>\n",
       "      <td>1798800.0</td>\n",
       "      <td>2.175877e+06</td>\n",
       "      <td>1099186.0</td>\n",
       "      <td>1.098971e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>3.7</td>\n",
       "      <td>256.118</td>\n",
       "      <td>2.13</td>\n",
       "      <td>211.802</td>\n",
       "      <td>14938.8</td>\n",
       "      <td>3844.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>151108.0</td>\n",
       "      <td>109.8543</td>\n",
       "      <td>1.95</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.223881</td>\n",
       "      <td>2.935424</td>\n",
       "      <td>-1.687233</td>\n",
       "      <td>13.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>15569800.0</td>\n",
       "      <td>2620100.0</td>\n",
       "      <td>3.998115e+06</td>\n",
       "      <td>1437108.0</td>\n",
       "      <td>1.438242e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>3.6</td>\n",
       "      <td>257.989</td>\n",
       "      <td>1.55</td>\n",
       "      <td>212.222</td>\n",
       "      <td>15254.4</td>\n",
       "      <td>3955.6</td>\n",
       "      <td>7.5</td>\n",
       "      <td>151758.0</td>\n",
       "      <td>110.0388</td>\n",
       "      <td>1.54</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150376</td>\n",
       "      <td>5.135172</td>\n",
       "      <td>-4.344529</td>\n",
       "      <td>11.0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>15624700.0</td>\n",
       "      <td>2625300.0</td>\n",
       "      <td>4.015310e+06</td>\n",
       "      <td>1444387.0</td>\n",
       "      <td>1.443890e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>3.5</td>\n",
       "      <td>258.824</td>\n",
       "      <td>1.58</td>\n",
       "      <td>213.295</td>\n",
       "      <td>15473.4</td>\n",
       "      <td>4027.6</td>\n",
       "      <td>8.3</td>\n",
       "      <td>152523.0</td>\n",
       "      <td>109.2966</td>\n",
       "      <td>1.52</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.452489</td>\n",
       "      <td>4.145019</td>\n",
       "      <td>2.584733</td>\n",
       "      <td>13.0</td>\n",
       "      <td>752.0</td>\n",
       "      <td>15663700.0</td>\n",
       "      <td>2629400.0</td>\n",
       "      <td>4.020187e+06</td>\n",
       "      <td>1450121.0</td>\n",
       "      <td>1.449740e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>13.3</td>\n",
       "      <td>255.942</td>\n",
       "      <td>0.05</td>\n",
       "      <td>218.569</td>\n",
       "      <td>17893.0</td>\n",
       "      <td>16275.9</td>\n",
       "      <td>24.7</td>\n",
       "      <td>132994.0</td>\n",
       "      <td>92.0613</td>\n",
       "      <td>0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152555</td>\n",
       "      <td>58.708814</td>\n",
       "      <td>23.355997</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1953.0</td>\n",
       "      <td>15693800.0</td>\n",
       "      <td>2633500.0</td>\n",
       "      <td>4.032882e+06</td>\n",
       "      <td>1453775.0</td>\n",
       "      <td>1.453747e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>8.4</td>\n",
       "      <td>259.511</td>\n",
       "      <td>0.10</td>\n",
       "      <td>224.139</td>\n",
       "      <td>18381.8</td>\n",
       "      <td>16906.0</td>\n",
       "      <td>14.6</td>\n",
       "      <td>141149.0</td>\n",
       "      <td>102.8885</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076161</td>\n",
       "      <td>22.084257</td>\n",
       "      <td>2.479332</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1073.0</td>\n",
       "      <td>15731700.0</td>\n",
       "      <td>2635800.0</td>\n",
       "      <td>4.053895e+06</td>\n",
       "      <td>1455757.0</td>\n",
       "      <td>1.456060e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 87827 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    UNRATE  CPIAUCSL  FEDFUNDS  CSUSHPINSA     M2SL     M1SL  PSAVERT  \\\n",
       "0      5.4   191.700      1.93     157.497   6399.4   1374.2      4.0   \n",
       "1      5.4   192.400      2.50     161.924   6432.5   1371.1      3.2   \n",
       "2      5.1   193.600      3.00     169.544   6473.1   1366.0      3.5   \n",
       "3      4.9   196.100      3.50     175.922   6569.8   1377.8      2.6   \n",
       "4      5.0   198.100      4.00     179.675   6654.5   1376.1      3.5   \n",
       "..     ...       ...       ...         ...      ...      ...      ...   \n",
       "59     3.7   256.118      2.13     211.802  14938.8   3844.0      7.3   \n",
       "60     3.6   257.989      1.55     212.222  15254.4   3955.6      7.5   \n",
       "61     3.5   258.824      1.58     213.295  15473.4   4027.6      8.3   \n",
       "62    13.3   255.942      0.05     218.569  17893.0  16275.9     24.7   \n",
       "63     8.4   259.511      0.10     224.139  18381.8  16906.0     14.6   \n",
       "\n",
       "      PAYEMS    INDPRO  TB3MS  ...  ESTPIEAMP01GPM  BROSERPA158MFRBDAL  \\\n",
       "0   132503.0   97.6137   2.07  ...       -0.202429           -1.713722   \n",
       "1   133032.0   99.4639   2.54  ...       -0.201410            1.561165   \n",
       "2   133690.0   99.6033   2.84  ...       -0.100604            1.090706   \n",
       "3   134498.0   99.9435   3.44  ...        1.206030            9.230125   \n",
       "4   134993.0  100.3216   3.88  ...        0.000000            2.638735   \n",
       "..       ...       ...    ...  ...             ...                 ...   \n",
       "59  151108.0  109.8543   1.95  ...       -0.223881            2.935424   \n",
       "60  151758.0  110.0388   1.54  ...        0.150376            5.135172   \n",
       "61  152523.0  109.2966   1.52  ...       -0.452489            4.145019   \n",
       "62  132994.0   92.0613   0.13  ...       -0.152555           58.708814   \n",
       "63  141149.0  102.8885   0.10  ...       -0.076161           22.084257   \n",
       "\n",
       "    LARTTULA158MFRBDAL  LAUCN310910000000004  LAUCN271470000000004  \\\n",
       "0             2.689583                  12.0                 712.0   \n",
       "1            16.658304                  25.0                 983.0   \n",
       "2            -4.866917                  11.0                 747.0   \n",
       "3            -3.218818                   9.0                 726.0   \n",
       "4             6.844361                   8.0                 713.0   \n",
       "..                 ...                   ...                   ...   \n",
       "59           -1.687233                  13.0                 642.0   \n",
       "60           -4.344529                  11.0                 505.0   \n",
       "61            2.584733                  13.0                 752.0   \n",
       "62           23.355997                  10.0                1953.0   \n",
       "63            2.479332                   8.0                1073.0   \n",
       "\n",
       "    LFWATTFECAM647N  LFWA55FECAM647S  LFWA55FEKRM647S  LFWA55MAAUM647N  \\\n",
       "0        12971900.0        1725500.0     2.143286e+06        1063251.0   \n",
       "1        13010400.0        1745200.0     2.138804e+06        1072885.0   \n",
       "2        13057400.0        1763600.0     2.166473e+06        1081863.0   \n",
       "3        13110300.0        1782100.0     2.175409e+06        1090390.0   \n",
       "4        13152700.0        1798800.0     2.175877e+06        1099186.0   \n",
       "..              ...              ...              ...              ...   \n",
       "59       15569800.0        2620100.0     3.998115e+06        1437108.0   \n",
       "60       15624700.0        2625300.0     4.015310e+06        1444387.0   \n",
       "61       15663700.0        2629400.0     4.020187e+06        1450121.0   \n",
       "62       15693800.0        2633500.0     4.032882e+06        1453775.0   \n",
       "63       15731700.0        2635800.0     4.053895e+06        1455757.0   \n",
       "\n",
       "    LFWA55MAAUM647S  \n",
       "0      1.062949e+06  \n",
       "1      1.073819e+06  \n",
       "2      1.081815e+06  \n",
       "3      1.090723e+06  \n",
       "4      1.098971e+06  \n",
       "..              ...  \n",
       "59     1.438242e+06  \n",
       "60     1.443890e+06  \n",
       "61     1.449740e+06  \n",
       "62     1.453747e+06  \n",
       "63     1.456060e+06  \n",
       "\n",
       "[64 rows x 87827 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = csv2.dropna(axis=\"columns\")[8::3].reset_index().drop([\"index\", \"Unnamed: 0\"], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aef69c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>5438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>6056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>5773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>6236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sales\n",
       "0    1308\n",
       "1    1054\n",
       "2    1258\n",
       "3    1362\n",
       "4    1225\n",
       "..    ...\n",
       "60   4797\n",
       "61   5438\n",
       "62   6056\n",
       "63   5773\n",
       "64   6236\n",
       "\n",
       "[65 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.read_csv(\"micron.csv\").drop(\"date\", axis=1)[::-1].reset_index().drop(\"index\", axis=1)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "286567d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = data.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f58a136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = full\n",
    "scaler = MinMaxScaler()\n",
    "full = MinMaxScaler().fit_transform(full)\n",
    "som_x = som_y = math.ceil(math.sqrt(math.sqrt(len(full))))\n",
    "full = np.transpose(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f40a2a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOM clustering executed in 0.0m 5.419386863708496s \n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "som_x = som_y = math.ceil(math.sqrt(math.sqrt(len(full))))\n",
    "# I didn't see its significance but to make the map square,\n",
    "# I calculated square root of map size which is \n",
    "# the square root of the number of series\n",
    "# for the row and column counts of som\n",
    "\n",
    "som = MiniSom(som_x, som_y,len(full[0]), sigma=0.3, learning_rate = 0.1)\n",
    "\n",
    "som.random_weights_init(full)\n",
    "som.train(full, 50000)\n",
    "time_elapsed = time.time() - since\n",
    "print(f\"SOM clustering executed in {time_elapsed // 60}m {time_elapsed % 60}s \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79a65763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA clustering executed in 0.0m 0.2759852409362793s \n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "mySeries_transformed = pca.fit_transform(full)\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print(f\"PCA clustering executed in {time_elapsed // 60}m {time_elapsed % 60}s \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91bde7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n",
      "Kmeans on PCA clustering executed in 0.0m 41.30145812034607s \n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "cluster_count = math.ceil(math.sqrt(len(full))) \n",
    "print(cluster_count)\n",
    "kmeans = KMeans(n_clusters=60,max_iter=5000)\n",
    "\n",
    "labels = kmeans.fit_predict(mySeries_transformed)\n",
    "time_elapsed = time.time() - since\n",
    "print(f\"Kmeans on PCA clustering executed in {time_elapsed // 60}m {time_elapsed % 60}s \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39f32117",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_map = []\n",
    "i = 0\n",
    "cluster_x = []\n",
    "cluster_dict = {}\n",
    "for idx in names:\n",
    "    winner_node = som.winner(full[i])\n",
    "    name = f\"Cluster {winner_node[0] * som_y + winner_node[1] + 1}\"\n",
    "    cluster_map.append((idx, name))\n",
    "\n",
    "    cluster_x.append(name)\n",
    "    i += 1\n",
    "\n",
    "cluster_x = Counter(cluster_x)\n",
    "clusters = pd.DataFrame(cluster_map, columns=[\"Name\", \"Cluster\"]).sort_values(by=\"Cluster\").set_index(\"Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b532b2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IMP5200',\n",
       " 'IPG311N',\n",
       " 'LAUCN080870000000005',\n",
       " 'SMU15279806500000001',\n",
       " 'SMU15279806500000001SA',\n",
       " 'SMU17000003231100001',\n",
       " 'SMU18000003200000008',\n",
       " 'SMU01000003200000030SA',\n",
       " 'CP0932HUM086NEST',\n",
       " 'SMU01000003200000007SA',\n",
       " 'SMU01000003200000007',\n",
       " 'EXPMANNH',\n",
       " 'SMU29000005000000008',\n",
       " 'IPG311S',\n",
       " 'CP0931DEM086NEST',\n",
       " 'SMU25731046055000001SA',\n",
       " 'SMU18000003200000008SA',\n",
       " 'WPS011101',\n",
       " 'CP0511MTM086NEST',\n",
       " 'SMU49362609092000001SA',\n",
       " 'SMU45348204300000001SA',\n",
       " 'SMU44772004300000001SA',\n",
       " 'IRLLOCOXGORSTSAM',\n",
       " 'APMAINNOM086NEST',\n",
       " 'SMU55333403000000008SA',\n",
       " 'SMU26000002023830030',\n",
       " 'SMU27000004245200030',\n",
       " 'EXPMANND',\n",
       " 'SMU29000005000000008SA',\n",
       " 'SMU29000004100000008',\n",
       " 'SMU12367406055000001',\n",
       " 'SMU34000003231100001',\n",
       " 'SMU55333403000000030',\n",
       " 'SMU29000004100000008SA',\n",
       " 'SMU27000003231160001SA',\n",
       " 'HANF206TRADN',\n",
       " 'SMU42000002023700001SA',\n",
       " 'SMU45348204300000001',\n",
       " 'SMU25731046055000001',\n",
       " 'SMU29000005000000030',\n",
       " 'SMU27000004245200030SA',\n",
       " 'SMU53000004349300001',\n",
       " 'SMU26000002023830007SA',\n",
       " 'WPU011101',\n",
       " 'SMU53365009092161101SA',\n",
       " 'SMU18000003200000030',\n",
       " 'SMU29000002023600008',\n",
       " 'SMU29000005000000030SA',\n",
       " 'SMU51472606055000001',\n",
       " 'PCU321912321912',\n",
       " 'SMU55333403000000030SA',\n",
       " 'CP0510MTM086NEST',\n",
       " 'KANK117PBSVN',\n",
       " 'SMU26000002023830007',\n",
       " 'SMU53000003231100006SA',\n",
       " 'SMU51472606055000001SA',\n",
       " 'LAUCN133110000000005',\n",
       " 'SMU44772004340008901SA',\n",
       " 'SMU55333403000000008',\n",
       " 'SMU18000003200000030SA',\n",
       " 'MANORF1LFN',\n",
       " 'SMU01000003200000030',\n",
       " 'SMU54000003100000030SA',\n",
       " 'SMU54000003100000030',\n",
       " 'SMU25716543000000030SA',\n",
       " 'KANK117PBSV']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = clusters.loc[\"sales\"].values\n",
    "value = clusters[\"Cluster\"].values\n",
    "index = list(np.where(value ==cluster)[0])\n",
    "names = list(clusters.index.values[index])\n",
    "names.remove(\"sales\")\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f4d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = {}\n",
    "for name in data:\n",
    "    x = data[name]\n",
    "    \n",
    "    corrs[abs(x.corr(target[\"sales\"]))] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0397054f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a499be",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for num in sorted(corrs)[:-40:-1]:\n",
    "    names.append(corrs[num])\n",
    "for num in sorted(corrs)[:10:1]:\n",
    "    names.append(corrs[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0cd12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62411cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = data.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a30586",
   "metadata": {},
   "outputs": [],
   "source": [
    "full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc580370",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = full.iloc[:,:-1],full.iloc[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7620c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1c00dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c43a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3,learning_rate = 0.2,\n",
    "                max_depth = 10, alpha = 5,n_estimators = 300, tree_method=\"gpu_hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48089956",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b69ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xg_reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0591da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafd479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Lasso(alpha=0.4, max_iter=200000)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ce57c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "584a998e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Bad Request.  Invalid value for variable series_id.  Series IDs should be 25 or less alphanumeric characters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/fredapi/fred.py\u001b[0m in \u001b[0;36m__fetch_data\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    641\u001b[0m                 'http', request, response, code, msg, hdrs)\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 400: Bad Request",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-a6ad9faaf4bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2/1/2021\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2/1/2021\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/fredapi/fred.py\u001b[0m in \u001b[0;36mget_series\u001b[0;34m(self, series_id, observation_start, observation_end, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0murl\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'&'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murlencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__fetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No data exists for series id: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseries_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/fredapi/fred.py\u001b[0m in \u001b[0;36m__fetch_data\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Bad Request.  Invalid value for variable series_id.  Series IDs should be 25 or less alphanumeric characters."
     ]
    }
   ],
   "source": [
    "values = {}\n",
    "for name in names:\n",
    "    d = fred.get_series(name, observation_start=\"2/1/2021\", observation_end = \"2/1/2021\")\n",
    "    values[name] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c156de",
   "metadata": {},
   "outputs": [],
   "source": [
    "values[\"sales\"] = 6236\n",
    "test = pd.DataFrame(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d5b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.from_numpy(test.to_numpy())\n",
    "test_data2 = torch.from_numpy(full[:-3:-1].to_numpy())\n",
    "test_data = torch.cat([test_data2, test_data]).unsqueeze(0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4752e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.load_state_dict(torch.load(\"model.pt\"))\n",
    "model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2918052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d5189e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "     \n",
    "\n",
    "        self.output_block = nn.Sequential(nn.Conv1d(3, 16, 2, 1, 0), nn.ReLU(), \n",
    "                                          nn.Conv1d(16, 32, 2, 1, 0), nn.ReLU(), \n",
    "                                          nn.Conv1d(32, 32, 2, 1, 0), nn.ReLU(), \n",
    "                                          nn.Conv1d(32, 16, 2, 1, 0), nn.ReLU())\n",
    "        self.fc = nn.Sequential(nn.LazyLinear(512), nn.ReLU(), nn.Linear(512, 256), nn.ReLU(), nn.Linear(256, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        net = self.output_block(x)\n",
    "        net = net.view(net.shape[0], -1)\n",
    "        net = self.fc(net)\n",
    "        \n",
    "        return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a396928d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>5438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>6056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>5773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>6236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sales\n",
       "3    1362\n",
       "4    1225\n",
       "5    1312\n",
       "6    1373\n",
       "7    1530\n",
       "..    ...\n",
       "60   4797\n",
       "61   5438\n",
       "62   6056\n",
       "63   5773\n",
       "64   6236\n",
       "\n",
       "[62 rows x 1 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "16e1a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, df, target):\n",
    "        data = df[1:].reset_index(drop=True)\n",
    "        sales = target[:-1].reset_index(drop=True)\n",
    "        self.data = data.join(sales)\n",
    "        self.target = target[3:].to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)-3\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data.iloc[idx:idx+3].to_numpy()\n",
    "        target = self.target[idx]\n",
    "        return torch.tensor(data).float(), torch.tensor(target).float()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "32c5f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = int(len(data) * 0.8)\n",
    "train, test = data[:idx], data[idx:]\n",
    "traint, testt = target[:idx], target[idx:]\n",
    "trainset = dataset(train, traint)\n",
    "testset = dataset(test, testt)\n",
    "train_loader = DataLoader(trainset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5d1bd6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=25, factor=0.5, min_lr=0.000001)\n",
    "def Train(epochs, model, train_loader):\n",
    "    valid_loss_min = np.Inf\n",
    "    min_rmse = np.Inf\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        valid_loss = 0.0\n",
    "        rmse = 0.0\n",
    "        model.train()\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "       \n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            output = model(images)\n",
    "        \n",
    "            loss = torch.sqrt(criterion(output,labels))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "        model.eval()\n",
    "        for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "       \n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            output = model(images)\n",
    "        \n",
    "            loss = torch.sqrt(criterion(output,labels))\n",
    "          \n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "                \n",
    "        scheduler.step(valid_loss)\n",
    "        print(f\"Train Loss: {train_loss.item()}\")\n",
    "        print(f\"Valid Loss: {valid_loss.item()}\")\n",
    "        if valid_loss.item() < valid_loss_min:\n",
    "            valid_loss_min = valid_loss.item()\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "    print(f\"Min RMSE: {valid_loss_min}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "970d5267",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1909.0980224609375\n",
      "Valid Loss: 4860.9033203125\n",
      "Train Loss: 1539.817138671875\n",
      "Valid Loss: 3125.787109375\n",
      "Train Loss: 1479.136474609375\n",
      "Valid Loss: 4563.7060546875\n",
      "Train Loss: 1701.6622314453125\n",
      "Valid Loss: 4160.05810546875\n",
      "Train Loss: 1390.8046875\n",
      "Valid Loss: 3421.848876953125\n",
      "Train Loss: 1422.8558349609375\n",
      "Valid Loss: 4416.681640625\n",
      "Train Loss: 1272.353515625\n",
      "Valid Loss: 3476.4189453125\n",
      "Train Loss: 1139.6719970703125\n",
      "Valid Loss: 4672.57568359375\n",
      "Train Loss: 1451.4915771484375\n",
      "Valid Loss: 4324.1904296875\n",
      "Train Loss: 1283.2880859375\n",
      "Valid Loss: 2497.315673828125\n",
      "Train Loss: 1174.651123046875\n",
      "Valid Loss: 4050.953125\n",
      "Train Loss: 1246.9923095703125\n",
      "Valid Loss: 2587.421875\n",
      "Train Loss: 1279.2352294921875\n",
      "Valid Loss: 4798.984375\n",
      "Train Loss: 1120.805908203125\n",
      "Valid Loss: 2230.7822265625\n",
      "Train Loss: 1052.0771484375\n",
      "Valid Loss: 4384.22607421875\n",
      "Train Loss: 1016.8716430664062\n",
      "Valid Loss: 1846.4305419921875\n",
      "Train Loss: 1020.7130126953125\n",
      "Valid Loss: 2266.962890625\n",
      "Train Loss: 746.6888427734375\n",
      "Valid Loss: 2252.307373046875\n",
      "Train Loss: 527.399169921875\n",
      "Valid Loss: 1523.27197265625\n",
      "Train Loss: 503.7666320800781\n",
      "Valid Loss: 747.0509033203125\n",
      "Train Loss: 573.3424072265625\n",
      "Valid Loss: 1756.2049560546875\n",
      "Train Loss: 511.2911376953125\n",
      "Valid Loss: 832.3997192382812\n",
      "Train Loss: 377.70208740234375\n",
      "Valid Loss: 584.0735473632812\n",
      "Train Loss: 347.38201904296875\n",
      "Valid Loss: 672.5779418945312\n",
      "Train Loss: 422.7816162109375\n",
      "Valid Loss: 747.1814575195312\n",
      "Train Loss: 489.14459228515625\n",
      "Valid Loss: 1266.10986328125\n",
      "Train Loss: 444.9444274902344\n",
      "Valid Loss: 1028.2686767578125\n",
      "Train Loss: 506.7679443359375\n",
      "Valid Loss: 750.816162109375\n",
      "Train Loss: 382.17486572265625\n",
      "Valid Loss: 659.2901611328125\n",
      "Train Loss: 324.7730712890625\n",
      "Valid Loss: 780.7346801757812\n",
      "Train Loss: 319.238525390625\n",
      "Valid Loss: 618.3861083984375\n",
      "Train Loss: 253.6350555419922\n",
      "Valid Loss: 720.251220703125\n",
      "Train Loss: 292.24090576171875\n",
      "Valid Loss: 876.4027099609375\n",
      "Train Loss: 333.11328125\n",
      "Valid Loss: 804.4573974609375\n",
      "Train Loss: 310.20404052734375\n",
      "Valid Loss: 725.8688354492188\n",
      "Train Loss: 271.1172180175781\n",
      "Valid Loss: 831.7117919921875\n",
      "Train Loss: 276.554931640625\n",
      "Valid Loss: 1190.84814453125\n",
      "Train Loss: 489.1710205078125\n",
      "Valid Loss: 678.738037109375\n",
      "Train Loss: 410.8895568847656\n",
      "Valid Loss: 1347.6142578125\n",
      "Train Loss: 515.2396240234375\n",
      "Valid Loss: 991.7446899414062\n",
      "Train Loss: 490.2669982910156\n",
      "Valid Loss: 1233.3902587890625\n",
      "Train Loss: 446.71466064453125\n",
      "Valid Loss: 954.7186889648438\n",
      "Train Loss: 421.36053466796875\n",
      "Valid Loss: 869.65234375\n",
      "Train Loss: 304.0542907714844\n",
      "Valid Loss: 1007.207275390625\n",
      "Train Loss: 474.5203552246094\n",
      "Valid Loss: 1536.9261474609375\n",
      "Train Loss: 379.1510925292969\n",
      "Valid Loss: 934.186279296875\n",
      "Train Loss: 283.79083251953125\n",
      "Valid Loss: 929.1077880859375\n",
      "Train Loss: 266.8258361816406\n",
      "Valid Loss: 591.3953247070312\n",
      "Train Loss: 209.14593505859375\n",
      "Valid Loss: 904.1670532226562\n",
      "Train Loss: 253.29771423339844\n",
      "Valid Loss: 781.9163818359375\n",
      "Train Loss: 330.3692321777344\n",
      "Valid Loss: 669.729736328125\n",
      "Train Loss: 243.85928344726562\n",
      "Valid Loss: 711.5462646484375\n",
      "Train Loss: 316.19622802734375\n",
      "Valid Loss: 936.6803588867188\n",
      "Train Loss: 227.49722290039062\n",
      "Valid Loss: 763.3106079101562\n",
      "Train Loss: 210.99093627929688\n",
      "Valid Loss: 684.5735473632812\n",
      "Train Loss: 223.8616180419922\n",
      "Valid Loss: 613.409912109375\n",
      "Train Loss: 211.72601318359375\n",
      "Valid Loss: 731.0467529296875\n",
      "Train Loss: 222.39979553222656\n",
      "Valid Loss: 857.2979125976562\n",
      "Train Loss: 302.0617370605469\n",
      "Valid Loss: 782.6279907226562\n",
      "Train Loss: 243.9640350341797\n",
      "Valid Loss: 691.0752563476562\n",
      "Train Loss: 236.95452880859375\n",
      "Valid Loss: 676.460205078125\n",
      "Train Loss: 267.7806701660156\n",
      "Valid Loss: 801.3345947265625\n",
      "Train Loss: 274.3075256347656\n",
      "Valid Loss: 537.5588989257812\n",
      "Train Loss: 202.74166870117188\n",
      "Valid Loss: 779.0050659179688\n",
      "Train Loss: 239.11228942871094\n",
      "Valid Loss: 846.9392700195312\n",
      "Train Loss: 200.7880859375\n",
      "Valid Loss: 724.5189819335938\n",
      "Train Loss: 218.01010131835938\n",
      "Valid Loss: 685.413818359375\n",
      "Train Loss: 273.72772216796875\n",
      "Valid Loss: 743.9647827148438\n",
      "Train Loss: 259.4525451660156\n",
      "Valid Loss: 775.53955078125\n",
      "Train Loss: 184.15711975097656\n",
      "Valid Loss: 669.71044921875\n",
      "Train Loss: 226.74783325195312\n",
      "Valid Loss: 760.4751586914062\n",
      "Train Loss: 202.076416015625\n",
      "Valid Loss: 742.877685546875\n",
      "Train Loss: 266.16290283203125\n",
      "Valid Loss: 797.2626953125\n",
      "Train Loss: 244.50559997558594\n",
      "Valid Loss: 770.9625854492188\n",
      "Train Loss: 209.39158630371094\n",
      "Valid Loss: 868.8178100585938\n",
      "Train Loss: 192.5089874267578\n",
      "Valid Loss: 865.078369140625\n",
      "Train Loss: 218.2130889892578\n",
      "Valid Loss: 976.9923706054688\n",
      "Train Loss: 262.173583984375\n",
      "Valid Loss: 1169.9053955078125\n",
      "Train Loss: 219.66062927246094\n",
      "Valid Loss: 660.649658203125\n",
      "Train Loss: 174.9173126220703\n",
      "Valid Loss: 949.830810546875\n",
      "Train Loss: 201.07131958007812\n",
      "Valid Loss: 762.1068725585938\n",
      "Train Loss: 205.41148376464844\n",
      "Valid Loss: 968.25\n",
      "Train Loss: 184.01669311523438\n",
      "Valid Loss: 606.8551025390625\n",
      "Train Loss: 193.7281951904297\n",
      "Valid Loss: 762.3453369140625\n",
      "Train Loss: 184.99212646484375\n",
      "Valid Loss: 826.8927612304688\n",
      "Train Loss: 162.7608642578125\n",
      "Valid Loss: 851.38623046875\n",
      "Train Loss: 170.09402465820312\n",
      "Valid Loss: 972.44287109375\n",
      "Train Loss: 171.97691345214844\n",
      "Valid Loss: 871.0706176757812\n",
      "Train Loss: 170.69924926757812\n",
      "Valid Loss: 1019.7275390625\n",
      "Train Loss: 166.43319702148438\n",
      "Valid Loss: 804.9884643554688\n",
      "Train Loss: 180.59230041503906\n",
      "Valid Loss: 632.6658935546875\n",
      "Train Loss: 161.14051818847656\n",
      "Valid Loss: 1002.29248046875\n",
      "Train Loss: 163.59353637695312\n",
      "Valid Loss: 626.8829345703125\n",
      "Train Loss: 192.52987670898438\n",
      "Valid Loss: 662.09716796875\n",
      "Train Loss: 179.78575134277344\n",
      "Valid Loss: 1114.196044921875\n",
      "Train Loss: 221.87327575683594\n",
      "Valid Loss: 778.8484497070312\n",
      "Train Loss: 175.38858032226562\n",
      "Valid Loss: 960.3028564453125\n",
      "Train Loss: 203.85153198242188\n",
      "Valid Loss: 748.1793823242188\n",
      "Train Loss: 176.85775756835938\n",
      "Valid Loss: 812.4479370117188\n",
      "Train Loss: 174.92013549804688\n",
      "Valid Loss: 785.2286376953125\n",
      "Train Loss: 205.61288452148438\n",
      "Valid Loss: 761.8204956054688\n",
      "Train Loss: 183.9883575439453\n",
      "Valid Loss: 791.0089111328125\n",
      "Train Loss: 175.23355102539062\n",
      "Valid Loss: 834.8055419921875\n",
      "Train Loss: 153.12916564941406\n",
      "Valid Loss: 764.9796752929688\n",
      "Train Loss: 146.72100830078125\n",
      "Valid Loss: 782.7647094726562\n",
      "Train Loss: 156.45623779296875\n",
      "Valid Loss: 616.489501953125\n",
      "Train Loss: 201.13648986816406\n",
      "Valid Loss: 1095.838134765625\n",
      "Train Loss: 175.3142852783203\n",
      "Valid Loss: 745.8250122070312\n",
      "Train Loss: 202.94419860839844\n",
      "Valid Loss: 755.7551879882812\n",
      "Train Loss: 192.94955444335938\n",
      "Valid Loss: 708.2607421875\n",
      "Train Loss: 208.8421630859375\n",
      "Valid Loss: 696.5447387695312\n",
      "Train Loss: 169.0293426513672\n",
      "Valid Loss: 1118.4222412109375\n",
      "Train Loss: 173.2869110107422\n",
      "Valid Loss: 696.9821166992188\n",
      "Train Loss: 139.92596435546875\n",
      "Valid Loss: 745.9564208984375\n",
      "Train Loss: 152.34034729003906\n",
      "Valid Loss: 881.4716796875\n",
      "Train Loss: 143.0717315673828\n",
      "Valid Loss: 609.196044921875\n",
      "Train Loss: 150.218505859375\n",
      "Valid Loss: 831.8203735351562\n",
      "Train Loss: 142.5399627685547\n",
      "Valid Loss: 898.887451171875\n",
      "Train Loss: 137.00474548339844\n",
      "Valid Loss: 735.2770385742188\n",
      "Train Loss: 147.3955078125\n",
      "Valid Loss: 1032.9488525390625\n",
      "Train Loss: 156.4252471923828\n",
      "Valid Loss: 802.482177734375\n",
      "Train Loss: 146.95962524414062\n",
      "Valid Loss: 856.6210327148438\n",
      "Train Loss: 135.24032592773438\n",
      "Valid Loss: 753.4282836914062\n",
      "Train Loss: 135.29641723632812\n",
      "Valid Loss: 639.072998046875\n",
      "Train Loss: 141.23992919921875\n",
      "Valid Loss: 722.3165893554688\n",
      "Train Loss: 136.81668090820312\n",
      "Valid Loss: 822.7708740234375\n",
      "Train Loss: 131.02320861816406\n",
      "Valid Loss: 739.851318359375\n",
      "Train Loss: 129.20132446289062\n",
      "Valid Loss: 825.5969848632812\n",
      "Train Loss: 137.19940185546875\n",
      "Valid Loss: 878.4057006835938\n",
      "Train Loss: 132.17967224121094\n",
      "Valid Loss: 751.9664306640625\n",
      "Train Loss: 136.04832458496094\n",
      "Valid Loss: 817.6179809570312\n",
      "Train Loss: 144.70664978027344\n",
      "Valid Loss: 881.9076538085938\n",
      "Train Loss: 129.58828735351562\n",
      "Valid Loss: 1061.917724609375\n",
      "Train Loss: 138.92625427246094\n",
      "Valid Loss: 766.9277954101562\n",
      "Train Loss: 132.76161193847656\n",
      "Valid Loss: 747.4649658203125\n",
      "Train Loss: 170.07962036132812\n",
      "Valid Loss: 923.1560668945312\n",
      "Train Loss: 138.2533721923828\n",
      "Valid Loss: 880.1381225585938\n",
      "Train Loss: 142.48886108398438\n",
      "Valid Loss: 865.4824829101562\n",
      "Train Loss: 128.16143798828125\n",
      "Valid Loss: 901.0773315429688\n",
      "Train Loss: 176.29513549804688\n",
      "Valid Loss: 875.7713012695312\n",
      "Train Loss: 151.30032348632812\n",
      "Valid Loss: 765.72412109375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 123.72540283203125\n",
      "Valid Loss: 863.6663818359375\n",
      "Train Loss: 123.79112243652344\n",
      "Valid Loss: 800.5851440429688\n",
      "Train Loss: 123.38768005371094\n",
      "Valid Loss: 1067.212890625\n",
      "Train Loss: 130.05218505859375\n",
      "Valid Loss: 677.6962890625\n",
      "Train Loss: 125.4349136352539\n",
      "Valid Loss: 1059.8778076171875\n",
      "Train Loss: 118.3251953125\n",
      "Valid Loss: 796.4824829101562\n",
      "Train Loss: 126.9022216796875\n",
      "Valid Loss: 1057.194580078125\n",
      "Train Loss: 123.50830841064453\n",
      "Valid Loss: 877.8069458007812\n",
      "Train Loss: 120.07618713378906\n",
      "Valid Loss: 755.4863891601562\n",
      "Train Loss: 122.2348403930664\n",
      "Valid Loss: 913.6035766601562\n",
      "Train Loss: 119.61326599121094\n",
      "Valid Loss: 923.7738647460938\n",
      "Train Loss: 123.96630859375\n",
      "Valid Loss: 778.0222778320312\n",
      "Train Loss: 117.31217956542969\n",
      "Valid Loss: 1075.2857666015625\n",
      "Train Loss: 115.39347076416016\n",
      "Valid Loss: 890.9816284179688\n",
      "Train Loss: 119.67304992675781\n",
      "Valid Loss: 781.2518920898438\n",
      "Train Loss: 120.36311340332031\n",
      "Valid Loss: 811.6633911132812\n",
      "Train Loss: 118.59669494628906\n",
      "Valid Loss: 676.80615234375\n",
      "Train Loss: 131.79257202148438\n",
      "Valid Loss: 858.137451171875\n",
      "Train Loss: 127.25373840332031\n",
      "Valid Loss: 780.7448120117188\n",
      "Train Loss: 140.80137634277344\n",
      "Valid Loss: 657.7922973632812\n",
      "Train Loss: 128.8927001953125\n",
      "Valid Loss: 832.9968872070312\n",
      "Train Loss: 131.126953125\n",
      "Valid Loss: 802.0841674804688\n",
      "Train Loss: 127.58806610107422\n",
      "Valid Loss: 776.5327758789062\n",
      "Train Loss: 124.83219146728516\n",
      "Valid Loss: 745.1348876953125\n",
      "Train Loss: 122.79692077636719\n",
      "Valid Loss: 920.2440795898438\n",
      "Train Loss: 126.87973022460938\n",
      "Valid Loss: 913.8782348632812\n",
      "Train Loss: 119.8236312866211\n",
      "Valid Loss: 901.9506225585938\n",
      "Train Loss: 123.66773223876953\n",
      "Valid Loss: 692.7341918945312\n",
      "Train Loss: 115.17140197753906\n",
      "Valid Loss: 770.5904541015625\n",
      "Train Loss: 120.4441146850586\n",
      "Valid Loss: 860.5654296875\n",
      "Train Loss: 118.2060546875\n",
      "Valid Loss: 1098.155029296875\n",
      "Train Loss: 118.33765411376953\n",
      "Valid Loss: 788.6549072265625\n",
      "Train Loss: 116.19332122802734\n",
      "Valid Loss: 753.0137329101562\n",
      "Train Loss: 122.17886352539062\n",
      "Valid Loss: 820.863525390625\n",
      "Train Loss: 121.31013488769531\n",
      "Valid Loss: 918.2431640625\n",
      "Train Loss: 127.08863830566406\n",
      "Valid Loss: 1082.6895751953125\n",
      "Train Loss: 119.33761596679688\n",
      "Valid Loss: 962.7504272460938\n",
      "Train Loss: 118.54637145996094\n",
      "Valid Loss: 958.062744140625\n",
      "Train Loss: 109.05011749267578\n",
      "Valid Loss: 688.079345703125\n",
      "Train Loss: 119.52567291259766\n",
      "Valid Loss: 802.0074462890625\n",
      "Train Loss: 114.51138305664062\n",
      "Valid Loss: 902.600341796875\n",
      "Train Loss: 117.85944366455078\n",
      "Valid Loss: 794.169677734375\n",
      "Train Loss: 120.155029296875\n",
      "Valid Loss: 695.8173828125\n",
      "Train Loss: 112.57303619384766\n",
      "Valid Loss: 740.255859375\n",
      "Train Loss: 116.51039123535156\n",
      "Valid Loss: 746.0831909179688\n",
      "Train Loss: 108.86290740966797\n",
      "Valid Loss: 776.9072265625\n",
      "Train Loss: 114.04365539550781\n",
      "Valid Loss: 772.6134033203125\n",
      "Train Loss: 118.36498260498047\n",
      "Valid Loss: 769.1214599609375\n",
      "Train Loss: 119.20724487304688\n",
      "Valid Loss: 693.9588623046875\n",
      "Train Loss: 118.86834716796875\n",
      "Valid Loss: 752.5962524414062\n",
      "Train Loss: 110.26588439941406\n",
      "Valid Loss: 816.6492919921875\n",
      "Train Loss: 113.16619873046875\n",
      "Valid Loss: 890.9768676757812\n",
      "Train Loss: 114.0593490600586\n",
      "Valid Loss: 1101.211181640625\n",
      "Train Loss: 112.34207153320312\n",
      "Valid Loss: 808.9820556640625\n",
      "Train Loss: 115.64100646972656\n",
      "Valid Loss: 775.5117797851562\n",
      "Train Loss: 116.54954528808594\n",
      "Valid Loss: 1113.7623291015625\n",
      "Train Loss: 113.44449615478516\n",
      "Valid Loss: 815.5442504882812\n",
      "Train Loss: 116.42976379394531\n",
      "Valid Loss: 782.0477294921875\n",
      "Train Loss: 113.55661010742188\n",
      "Valid Loss: 1104.8133544921875\n",
      "Train Loss: 108.71611785888672\n",
      "Valid Loss: 792.9463500976562\n",
      "Train Loss: 115.8705062866211\n",
      "Valid Loss: 930.4804077148438\n",
      "Train Loss: 116.58857727050781\n",
      "Valid Loss: 785.1660766601562\n",
      "Train Loss: 115.04566955566406\n",
      "Valid Loss: 799.9757080078125\n",
      "Train Loss: 115.7542724609375\n",
      "Valid Loss: 745.21533203125\n",
      "Train Loss: 114.3896255493164\n",
      "Valid Loss: 871.7428588867188\n",
      "Train Loss: 112.19202423095703\n",
      "Valid Loss: 862.2901000976562\n",
      "Train Loss: 114.33294677734375\n",
      "Valid Loss: 774.113037109375\n",
      "Train Loss: 119.14098358154297\n",
      "Valid Loss: 1068.3056640625\n",
      "Train Loss: 114.36527252197266\n",
      "Valid Loss: 890.5536499023438\n",
      "Train Loss: 107.83108520507812\n",
      "Valid Loss: 743.3302001953125\n",
      "Train Loss: 115.2885513305664\n",
      "Valid Loss: 1080.0555419921875\n",
      "Train Loss: 116.76751708984375\n",
      "Valid Loss: 887.4439697265625\n",
      "Train Loss: 115.86117553710938\n",
      "Valid Loss: 969.8308715820312\n",
      "Train Loss: 110.60848236083984\n",
      "Valid Loss: 870.9967651367188\n",
      "Train Loss: 117.84591674804688\n",
      "Valid Loss: 774.3453979492188\n",
      "Train Loss: 114.45851135253906\n",
      "Valid Loss: 845.34521484375\n",
      "Train Loss: 113.11307525634766\n",
      "Valid Loss: 854.9931640625\n",
      "Train Loss: 114.11097717285156\n",
      "Valid Loss: 712.0408325195312\n",
      "Train Loss: 113.26299285888672\n",
      "Valid Loss: 959.5465698242188\n",
      "Train Loss: 108.39913940429688\n",
      "Valid Loss: 984.9032592773438\n",
      "Train Loss: 114.12319946289062\n",
      "Valid Loss: 934.6168212890625\n",
      "Train Loss: 117.29957580566406\n",
      "Valid Loss: 823.2824096679688\n",
      "Train Loss: 113.37493896484375\n",
      "Valid Loss: 792.1714477539062\n",
      "Train Loss: 110.15418243408203\n",
      "Valid Loss: 874.4534301757812\n",
      "Train Loss: 111.25162506103516\n",
      "Valid Loss: 786.363525390625\n",
      "Train Loss: 107.02871704101562\n",
      "Valid Loss: 1109.0616455078125\n",
      "Train Loss: 111.91268157958984\n",
      "Valid Loss: 852.3115234375\n",
      "Train Loss: 108.4453125\n",
      "Valid Loss: 778.1055908203125\n",
      "Train Loss: 115.32240295410156\n",
      "Valid Loss: 884.928955078125\n",
      "Train Loss: 113.35601806640625\n",
      "Valid Loss: 746.032958984375\n",
      "Train Loss: 110.97565460205078\n",
      "Valid Loss: 977.8915405273438\n",
      "Train Loss: 112.05619812011719\n",
      "Valid Loss: 813.4307250976562\n",
      "Train Loss: 113.05207824707031\n",
      "Valid Loss: 817.584228515625\n",
      "Train Loss: 109.70571899414062\n",
      "Valid Loss: 970.3925170898438\n",
      "Train Loss: 112.77670288085938\n",
      "Valid Loss: 902.1224975585938\n",
      "Train Loss: 114.94286346435547\n",
      "Valid Loss: 1105.1429443359375\n",
      "Train Loss: 113.60295104980469\n",
      "Valid Loss: 850.1919555664062\n",
      "Train Loss: 115.45033264160156\n",
      "Valid Loss: 705.462158203125\n",
      "Train Loss: 115.10662841796875\n",
      "Valid Loss: 791.8411865234375\n",
      "Train Loss: 110.08097839355469\n",
      "Valid Loss: 1114.4423828125\n",
      "Train Loss: 107.88919830322266\n",
      "Valid Loss: 804.3855590820312\n",
      "Train Loss: 108.53131103515625\n",
      "Valid Loss: 724.28271484375\n",
      "Train Loss: 116.00326538085938\n",
      "Valid Loss: 753.0108032226562\n",
      "Train Loss: 116.61199951171875\n",
      "Valid Loss: 879.82177734375\n",
      "Train Loss: 113.30625915527344\n",
      "Valid Loss: 817.295654296875\n",
      "Train Loss: 111.38542938232422\n",
      "Valid Loss: 952.4706420898438\n",
      "Train Loss: 114.1407241821289\n",
      "Valid Loss: 984.1724243164062\n",
      "Train Loss: 109.0887680053711\n",
      "Valid Loss: 775.6588134765625\n",
      "Train Loss: 115.8270034790039\n",
      "Valid Loss: 817.5782470703125\n",
      "Train Loss: 111.5693130493164\n",
      "Valid Loss: 863.3224487304688\n",
      "Train Loss: 113.40573120117188\n",
      "Valid Loss: 825.137451171875\n",
      "Train Loss: 115.49654388427734\n",
      "Valid Loss: 769.3560180664062\n",
      "Train Loss: 113.39706420898438\n",
      "Valid Loss: 825.9346923828125\n",
      "Train Loss: 110.86367797851562\n",
      "Valid Loss: 795.1640625\n",
      "Train Loss: 111.18661499023438\n",
      "Valid Loss: 972.9693603515625\n",
      "Train Loss: 117.36837005615234\n",
      "Valid Loss: 776.2963256835938\n",
      "Train Loss: 114.50080871582031\n",
      "Valid Loss: 768.994873046875\n",
      "Train Loss: 111.5997543334961\n",
      "Valid Loss: 775.57763671875\n",
      "Train Loss: 111.6846923828125\n",
      "Valid Loss: 720.7354736328125\n",
      "Train Loss: 111.87598419189453\n",
      "Valid Loss: 811.7035522460938\n",
      "Train Loss: 112.33885192871094\n",
      "Valid Loss: 901.236572265625\n",
      "Train Loss: 113.16264343261719\n",
      "Valid Loss: 785.7823486328125\n",
      "Train Loss: 113.65424346923828\n",
      "Valid Loss: 727.345947265625\n",
      "Train Loss: 114.02145385742188\n",
      "Valid Loss: 733.6048583984375\n",
      "Train Loss: 115.2100830078125\n",
      "Valid Loss: 798.2183227539062\n",
      "Train Loss: 115.35499572753906\n",
      "Valid Loss: 934.0541381835938\n",
      "Train Loss: 114.68714141845703\n",
      "Valid Loss: 991.485595703125\n",
      "Train Loss: 103.72732543945312\n",
      "Valid Loss: 725.016845703125\n",
      "Train Loss: 113.81069946289062\n",
      "Valid Loss: 969.9168701171875\n",
      "Train Loss: 105.87771606445312\n",
      "Valid Loss: 773.6677856445312\n",
      "Train Loss: 107.11023712158203\n",
      "Valid Loss: 915.2017211914062\n",
      "Train Loss: 110.69982147216797\n",
      "Valid Loss: 1113.1357421875\n",
      "Train Loss: 110.83244323730469\n",
      "Valid Loss: 736.2551879882812\n",
      "Train Loss: 113.71834564208984\n",
      "Valid Loss: 896.963623046875\n",
      "Train Loss: 110.86845397949219\n",
      "Valid Loss: 852.4762573242188\n",
      "Train Loss: 111.46498107910156\n",
      "Valid Loss: 733.1307373046875\n",
      "Train Loss: 112.12535095214844\n",
      "Valid Loss: 751.0198974609375\n",
      "Train Loss: 115.64785766601562\n",
      "Valid Loss: 977.0206909179688\n",
      "Train Loss: 113.66957092285156\n",
      "Valid Loss: 757.3713989257812\n",
      "Train Loss: 117.48028564453125\n",
      "Valid Loss: 867.1904296875\n",
      "Train Loss: 113.72167205810547\n",
      "Valid Loss: 905.8394165039062\n",
      "Train Loss: 114.40398406982422\n",
      "Valid Loss: 907.6826171875\n",
      "Train Loss: 115.99478149414062\n",
      "Valid Loss: 823.2962036132812\n",
      "Train Loss: 112.7783432006836\n",
      "Valid Loss: 907.2639770507812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 116.24423217773438\n",
      "Valid Loss: 913.983642578125\n",
      "Train Loss: 112.36417388916016\n",
      "Valid Loss: 801.04296875\n",
      "Train Loss: 110.4356918334961\n",
      "Valid Loss: 720.58203125\n",
      "Train Loss: 111.31568908691406\n",
      "Valid Loss: 798.945068359375\n",
      "Train Loss: 112.75006866455078\n",
      "Valid Loss: 773.8458251953125\n",
      "Train Loss: 111.91371154785156\n",
      "Valid Loss: 753.4432373046875\n",
      "Train Loss: 116.84313201904297\n",
      "Valid Loss: 847.7421875\n",
      "Train Loss: 116.8740005493164\n",
      "Valid Loss: 799.3917846679688\n",
      "Train Loss: 112.21647644042969\n",
      "Valid Loss: 745.767578125\n",
      "Train Loss: 109.15863800048828\n",
      "Valid Loss: 825.7821044921875\n",
      "Train Loss: 111.95008850097656\n",
      "Valid Loss: 909.0669555664062\n",
      "Train Loss: 109.27018737792969\n",
      "Valid Loss: 935.7445678710938\n",
      "Train Loss: 113.50057983398438\n",
      "Valid Loss: 753.9373168945312\n",
      "Train Loss: 111.62117767333984\n",
      "Valid Loss: 732.081298828125\n",
      "Train Loss: 112.65349578857422\n",
      "Valid Loss: 983.2135620117188\n",
      "Train Loss: 115.01866149902344\n",
      "Valid Loss: 725.5088500976562\n",
      "Train Loss: 111.79288482666016\n",
      "Valid Loss: 992.2783203125\n",
      "Train Loss: 113.87669372558594\n",
      "Valid Loss: 798.8255004882812\n",
      "Train Loss: 110.47090911865234\n",
      "Valid Loss: 808.9839477539062\n",
      "Train Loss: 112.45591735839844\n",
      "Valid Loss: 708.0646362304688\n",
      "Train Loss: 109.91681671142578\n",
      "Valid Loss: 824.4014282226562\n",
      "Train Loss: 108.68270111083984\n",
      "Valid Loss: 717.896240234375\n",
      "Train Loss: 105.02749633789062\n",
      "Valid Loss: 797.081298828125\n",
      "Train Loss: 109.98660278320312\n",
      "Valid Loss: 1117.327880859375\n",
      "Train Loss: 104.09904479980469\n",
      "Valid Loss: 1103.505859375\n",
      "Train Loss: 112.16072082519531\n",
      "Valid Loss: 991.0450439453125\n",
      "Train Loss: 104.68659973144531\n",
      "Valid Loss: 798.99755859375\n",
      "Train Loss: 108.6573257446289\n",
      "Valid Loss: 807.6121826171875\n",
      "Train Loss: 116.57101440429688\n",
      "Valid Loss: 1095.9232177734375\n",
      "Train Loss: 112.80253601074219\n",
      "Valid Loss: 772.470703125\n",
      "Train Loss: 111.01802062988281\n",
      "Valid Loss: 795.848876953125\n",
      "Train Loss: 106.33401489257812\n",
      "Valid Loss: 839.4808959960938\n",
      "Train Loss: 113.81810760498047\n",
      "Valid Loss: 769.8098754882812\n",
      "Train Loss: 112.95616912841797\n",
      "Valid Loss: 908.736083984375\n",
      "Train Loss: 110.06138610839844\n",
      "Valid Loss: 985.3290405273438\n",
      "Train Loss: 114.74610900878906\n",
      "Valid Loss: 725.1484985351562\n",
      "Train Loss: 115.08270263671875\n",
      "Valid Loss: 934.21826171875\n",
      "Train Loss: 109.82024383544922\n",
      "Valid Loss: 830.6001586914062\n",
      "Train Loss: 114.02383422851562\n",
      "Valid Loss: 771.9551391601562\n",
      "Train Loss: 114.75415802001953\n",
      "Valid Loss: 1101.700439453125\n",
      "Train Loss: 111.75286102294922\n",
      "Valid Loss: 1107.6541748046875\n",
      "Train Loss: 112.16629791259766\n",
      "Valid Loss: 858.3406372070312\n",
      "Train Loss: 112.35433197021484\n",
      "Valid Loss: 854.3214721679688\n",
      "Train Loss: 113.20987701416016\n",
      "Valid Loss: 817.0906372070312\n",
      "Train Loss: 112.50444793701172\n",
      "Valid Loss: 984.8048706054688\n",
      "Train Loss: 112.41703033447266\n",
      "Valid Loss: 987.9259643554688\n",
      "Train Loss: 108.21089935302734\n",
      "Valid Loss: 704.107177734375\n",
      "Train Loss: 106.88893127441406\n",
      "Valid Loss: 915.0320434570312\n",
      "Train Loss: 106.75624084472656\n",
      "Valid Loss: 988.2880859375\n",
      "Train Loss: 106.8587646484375\n",
      "Valid Loss: 736.431640625\n",
      "Train Loss: 107.85673522949219\n",
      "Valid Loss: 1109.5830078125\n",
      "Train Loss: 112.70758819580078\n",
      "Valid Loss: 758.1140747070312\n",
      "Train Loss: 110.34709167480469\n",
      "Valid Loss: 1112.9097900390625\n",
      "Train Loss: 111.26973724365234\n",
      "Valid Loss: 833.9130859375\n",
      "Train Loss: 107.22531127929688\n",
      "Valid Loss: 880.6109008789062\n",
      "Train Loss: 117.21067810058594\n",
      "Valid Loss: 735.5230102539062\n",
      "Train Loss: 112.33013916015625\n",
      "Valid Loss: 1116.279541015625\n",
      "Train Loss: 105.38401794433594\n",
      "Valid Loss: 863.6273803710938\n",
      "Train Loss: 111.68736267089844\n",
      "Valid Loss: 1083.994873046875\n",
      "Train Loss: 112.1783447265625\n",
      "Valid Loss: 978.5502319335938\n",
      "Train Loss: 111.47340393066406\n",
      "Valid Loss: 869.2549438476562\n",
      "Train Loss: 108.68598175048828\n",
      "Valid Loss: 815.5818481445312\n",
      "Train Loss: 110.25727844238281\n",
      "Valid Loss: 789.1835327148438\n",
      "Train Loss: 103.90841674804688\n",
      "Valid Loss: 1108.058837890625\n",
      "Train Loss: 109.92135620117188\n",
      "Valid Loss: 772.1824951171875\n",
      "Train Loss: 115.68077850341797\n",
      "Valid Loss: 776.8482055664062\n",
      "Train Loss: 110.95692443847656\n",
      "Valid Loss: 724.6514892578125\n",
      "Train Loss: 108.34130859375\n",
      "Valid Loss: 1107.958740234375\n",
      "Train Loss: 109.26680755615234\n",
      "Valid Loss: 1113.286376953125\n",
      "Train Loss: 108.99009704589844\n",
      "Valid Loss: 932.3928833007812\n",
      "Train Loss: 107.79594421386719\n",
      "Valid Loss: 872.9479370117188\n",
      "Train Loss: 111.66020965576172\n",
      "Valid Loss: 988.4892578125\n",
      "Train Loss: 114.0595703125\n",
      "Valid Loss: 1116.572265625\n",
      "Train Loss: 109.4468765258789\n",
      "Valid Loss: 989.8132934570312\n",
      "Train Loss: 114.88055419921875\n",
      "Valid Loss: 794.83544921875\n",
      "Train Loss: 109.23250579833984\n",
      "Valid Loss: 777.4246826171875\n",
      "Train Loss: 109.4876937866211\n",
      "Valid Loss: 774.2559204101562\n",
      "Train Loss: 113.7033920288086\n",
      "Valid Loss: 1115.3272705078125\n",
      "Train Loss: 110.5696792602539\n",
      "Valid Loss: 982.4263305664062\n",
      "Train Loss: 111.88500213623047\n",
      "Valid Loss: 746.0357666015625\n",
      "Train Loss: 110.58626556396484\n",
      "Valid Loss: 779.1334228515625\n",
      "Train Loss: 107.89057159423828\n",
      "Valid Loss: 1093.5919189453125\n",
      "Train Loss: 108.14588928222656\n",
      "Valid Loss: 737.0991821289062\n",
      "Train Loss: 114.93180847167969\n",
      "Valid Loss: 816.33251953125\n",
      "Train Loss: 112.98440551757812\n",
      "Valid Loss: 870.5466918945312\n",
      "Train Loss: 114.03141021728516\n",
      "Valid Loss: 764.1309204101562\n",
      "Train Loss: 107.73811340332031\n",
      "Valid Loss: 1118.1956787109375\n",
      "Train Loss: 109.60521697998047\n",
      "Valid Loss: 904.6591186523438\n",
      "Train Loss: 115.94625854492188\n",
      "Valid Loss: 777.9793090820312\n",
      "Train Loss: 115.26272583007812\n",
      "Valid Loss: 951.53857421875\n",
      "Train Loss: 113.76248168945312\n",
      "Valid Loss: 820.4974975585938\n",
      "Train Loss: 110.25178527832031\n",
      "Valid Loss: 798.86865234375\n",
      "Train Loss: 112.49168395996094\n",
      "Valid Loss: 710.2637939453125\n",
      "Train Loss: 108.1031494140625\n",
      "Valid Loss: 866.8903198242188\n",
      "Train Loss: 112.10798645019531\n",
      "Valid Loss: 904.7041015625\n",
      "Train Loss: 113.50350952148438\n",
      "Valid Loss: 1115.47265625\n",
      "Train Loss: 113.55335998535156\n",
      "Valid Loss: 824.343017578125\n",
      "Train Loss: 110.11392974853516\n",
      "Valid Loss: 979.3964233398438\n",
      "Train Loss: 108.01883697509766\n",
      "Valid Loss: 758.2181396484375\n",
      "Train Loss: 114.18943786621094\n",
      "Valid Loss: 1117.628662109375\n",
      "Train Loss: 112.19024658203125\n",
      "Valid Loss: 799.1184692382812\n",
      "Train Loss: 112.90341186523438\n",
      "Valid Loss: 848.9365844726562\n",
      "Train Loss: 112.13771057128906\n",
      "Valid Loss: 734.0216674804688\n",
      "Train Loss: 108.26725769042969\n",
      "Valid Loss: 828.9686279296875\n",
      "Train Loss: 113.95391845703125\n",
      "Valid Loss: 1116.6861572265625\n",
      "Train Loss: 103.33770751953125\n",
      "Valid Loss: 911.8403930664062\n",
      "Train Loss: 111.3641586303711\n",
      "Valid Loss: 783.8570556640625\n",
      "Train Loss: 103.09225463867188\n",
      "Valid Loss: 828.94921875\n",
      "Train Loss: 109.83470916748047\n",
      "Valid Loss: 864.8661499023438\n",
      "Train Loss: 112.05497741699219\n",
      "Valid Loss: 932.517822265625\n",
      "Train Loss: 112.3364028930664\n",
      "Valid Loss: 962.78369140625\n",
      "Train Loss: 109.7269515991211\n",
      "Valid Loss: 765.6123657226562\n",
      "Train Loss: 106.88124084472656\n",
      "Valid Loss: 772.3030395507812\n",
      "Train Loss: 112.65596771240234\n",
      "Valid Loss: 854.2662963867188\n",
      "Train Loss: 110.45256042480469\n",
      "Valid Loss: 977.9725952148438\n",
      "Train Loss: 114.79302215576172\n",
      "Valid Loss: 1110.3385009765625\n",
      "Train Loss: 107.09132385253906\n",
      "Valid Loss: 796.7720947265625\n",
      "Train Loss: 108.0550537109375\n",
      "Valid Loss: 714.7130126953125\n",
      "Train Loss: 106.66555786132812\n",
      "Valid Loss: 799.1195068359375\n",
      "Train Loss: 114.39871978759766\n",
      "Valid Loss: 759.2554321289062\n",
      "Train Loss: 113.43135070800781\n",
      "Valid Loss: 1115.5230712890625\n",
      "Train Loss: 114.57841491699219\n",
      "Valid Loss: 733.6345825195312\n",
      "Train Loss: 112.20066833496094\n",
      "Valid Loss: 981.8284301757812\n",
      "Train Loss: 110.87321472167969\n",
      "Valid Loss: 895.4374389648438\n",
      "Train Loss: 114.54960632324219\n",
      "Valid Loss: 808.5149536132812\n",
      "Train Loss: 108.36973571777344\n",
      "Valid Loss: 726.53173828125\n",
      "Train Loss: 112.80414581298828\n",
      "Valid Loss: 795.5955810546875\n",
      "Train Loss: 115.27021026611328\n",
      "Valid Loss: 1094.250732421875\n",
      "Train Loss: 114.25516510009766\n",
      "Valid Loss: 906.8427734375\n",
      "Train Loss: 106.64784240722656\n",
      "Valid Loss: 853.7977294921875\n",
      "Train Loss: 106.7397689819336\n",
      "Valid Loss: 976.425537109375\n",
      "Train Loss: 108.83610534667969\n",
      "Valid Loss: 830.1087036132812\n",
      "Train Loss: 106.24808502197266\n",
      "Valid Loss: 899.7216186523438\n",
      "Train Loss: 112.612548828125\n",
      "Valid Loss: 863.3009033203125\n",
      "Train Loss: 110.94078063964844\n",
      "Valid Loss: 801.02197265625\n",
      "Train Loss: 109.88159942626953\n",
      "Valid Loss: 781.397705078125\n",
      "Train Loss: 114.47329711914062\n",
      "Valid Loss: 787.3344116210938\n",
      "Train Loss: 110.14606475830078\n",
      "Valid Loss: 825.7754516601562\n",
      "Train Loss: 112.4925765991211\n",
      "Valid Loss: 869.5523681640625\n",
      "Train Loss: 111.10989379882812\n",
      "Valid Loss: 731.4481201171875\n",
      "Train Loss: 112.18759155273438\n",
      "Valid Loss: 935.6035766601562\n",
      "Train Loss: 110.28602600097656\n",
      "Valid Loss: 776.9671630859375\n",
      "Train Loss: 115.95238494873047\n",
      "Valid Loss: 978.6357421875\n",
      "Train Loss: 114.7216567993164\n",
      "Valid Loss: 964.2482299804688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 110.80066680908203\n",
      "Valid Loss: 914.0237426757812\n",
      "Train Loss: 114.0172119140625\n",
      "Valid Loss: 827.7887573242188\n",
      "Train Loss: 107.54045104980469\n",
      "Valid Loss: 726.353271484375\n",
      "Train Loss: 111.79805755615234\n",
      "Valid Loss: 994.3298950195312\n",
      "Train Loss: 111.98477935791016\n",
      "Valid Loss: 936.2490844726562\n",
      "Train Loss: 110.94713592529297\n",
      "Valid Loss: 890.6657104492188\n",
      "Train Loss: 105.63764190673828\n",
      "Valid Loss: 718.7747802734375\n",
      "Train Loss: 111.44446563720703\n",
      "Valid Loss: 874.5654907226562\n",
      "Train Loss: 111.95745086669922\n",
      "Valid Loss: 784.176513671875\n",
      "Train Loss: 115.40194702148438\n",
      "Valid Loss: 802.4317016601562\n",
      "Train Loss: 113.77745819091797\n",
      "Valid Loss: 986.9462280273438\n",
      "Train Loss: 114.40467071533203\n",
      "Valid Loss: 794.6248779296875\n",
      "Train Loss: 115.20021057128906\n",
      "Valid Loss: 789.280029296875\n",
      "Train Loss: 111.06356811523438\n",
      "Valid Loss: 893.263916015625\n",
      "Train Loss: 115.75180053710938\n",
      "Valid Loss: 705.5333862304688\n",
      "Train Loss: 114.4776382446289\n",
      "Valid Loss: 925.6724853515625\n",
      "Train Loss: 110.41936492919922\n",
      "Valid Loss: 699.5646362304688\n",
      "Train Loss: 109.37150573730469\n",
      "Valid Loss: 959.5057373046875\n",
      "Train Loss: 109.38411712646484\n",
      "Valid Loss: 989.9049682617188\n",
      "Train Loss: 106.27890014648438\n",
      "Valid Loss: 831.9185180664062\n",
      "Train Loss: 111.87106323242188\n",
      "Valid Loss: 770.3150634765625\n",
      "Train Loss: 112.16463470458984\n",
      "Valid Loss: 772.3687744140625\n",
      "Train Loss: 108.61431884765625\n",
      "Valid Loss: 827.0862426757812\n",
      "Train Loss: 113.60475158691406\n",
      "Valid Loss: 932.1651000976562\n",
      "Train Loss: 113.89466094970703\n",
      "Valid Loss: 917.0645141601562\n",
      "Train Loss: 111.99122619628906\n",
      "Valid Loss: 764.3780517578125\n",
      "Train Loss: 110.02922058105469\n",
      "Valid Loss: 987.0925903320312\n",
      "Train Loss: 110.6533203125\n",
      "Valid Loss: 932.7612915039062\n",
      "Train Loss: 107.30255126953125\n",
      "Valid Loss: 865.0255737304688\n",
      "Train Loss: 114.10536193847656\n",
      "Valid Loss: 733.556640625\n",
      "Train Loss: 109.48884582519531\n",
      "Valid Loss: 912.8558959960938\n",
      "Train Loss: 106.36482238769531\n",
      "Valid Loss: 1085.524658203125\n",
      "Train Loss: 107.8435287475586\n",
      "Valid Loss: 918.681396484375\n",
      "Train Loss: 114.08766174316406\n",
      "Valid Loss: 832.0110473632812\n",
      "Train Loss: 113.54370880126953\n",
      "Valid Loss: 733.865234375\n",
      "Train Loss: 113.31836700439453\n",
      "Valid Loss: 899.734375\n",
      "Train Loss: 110.27877807617188\n",
      "Valid Loss: 909.6675415039062\n",
      "Train Loss: 111.21772766113281\n",
      "Valid Loss: 785.6910400390625\n",
      "Train Loss: 111.16156005859375\n",
      "Valid Loss: 771.0645141601562\n",
      "Train Loss: 110.00466918945312\n",
      "Valid Loss: 890.6432495117188\n",
      "Train Loss: 114.05289459228516\n",
      "Valid Loss: 795.4169921875\n",
      "Train Loss: 108.14558410644531\n",
      "Valid Loss: 732.1661376953125\n",
      "Train Loss: 110.7765121459961\n",
      "Valid Loss: 892.1657104492188\n",
      "Train Loss: 111.87047576904297\n",
      "Valid Loss: 988.5064697265625\n",
      "Train Loss: 110.34259033203125\n",
      "Valid Loss: 744.6538696289062\n",
      "Train Loss: 107.015625\n",
      "Valid Loss: 779.5071411132812\n",
      "Train Loss: 112.11194610595703\n",
      "Valid Loss: 727.37646484375\n",
      "Train Loss: 114.72791290283203\n",
      "Valid Loss: 852.644775390625\n",
      "Train Loss: 110.88825988769531\n",
      "Valid Loss: 730.2793579101562\n",
      "Train Loss: 104.51881408691406\n",
      "Valid Loss: 977.09765625\n",
      "Train Loss: 109.71395874023438\n",
      "Valid Loss: 850.4736328125\n",
      "Train Loss: 112.65223693847656\n",
      "Valid Loss: 739.5833129882812\n",
      "Train Loss: 105.78780364990234\n",
      "Valid Loss: 780.4840087890625\n",
      "Train Loss: 111.00765991210938\n",
      "Valid Loss: 765.031982421875\n",
      "Train Loss: 112.68270874023438\n",
      "Valid Loss: 754.2110595703125\n",
      "Train Loss: 116.38545227050781\n",
      "Valid Loss: 780.7349853515625\n",
      "Train Loss: 107.14757537841797\n",
      "Valid Loss: 1118.882568359375\n",
      "Train Loss: 111.22921752929688\n",
      "Valid Loss: 911.2111206054688\n",
      "Train Loss: 111.73540496826172\n",
      "Valid Loss: 734.034912109375\n",
      "Train Loss: 112.44891357421875\n",
      "Valid Loss: 971.6973266601562\n",
      "Train Loss: 112.84464263916016\n",
      "Valid Loss: 709.9555053710938\n",
      "Train Loss: 109.777099609375\n",
      "Valid Loss: 912.6273803710938\n",
      "Train Loss: 115.17102813720703\n",
      "Valid Loss: 933.2720336914062\n",
      "Train Loss: 113.72819519042969\n",
      "Valid Loss: 754.2433471679688\n",
      "Train Loss: 110.53146362304688\n",
      "Valid Loss: 754.2490234375\n",
      "Train Loss: 110.63529205322266\n",
      "Valid Loss: 775.3009033203125\n",
      "Train Loss: 106.9393539428711\n",
      "Valid Loss: 852.0887451171875\n",
      "Train Loss: 113.68009185791016\n",
      "Valid Loss: 836.39453125\n",
      "Train Loss: 115.81391906738281\n",
      "Valid Loss: 801.3491821289062\n",
      "Train Loss: 111.65614318847656\n",
      "Valid Loss: 863.8052978515625\n",
      "Train Loss: 114.9657211303711\n",
      "Valid Loss: 1115.56982421875\n",
      "Train Loss: 108.89970397949219\n",
      "Valid Loss: 778.1393432617188\n",
      "Train Loss: 108.41401672363281\n",
      "Valid Loss: 826.3410034179688\n",
      "Train Loss: 112.38336181640625\n",
      "Valid Loss: 975.8223266601562\n",
      "Train Loss: 109.85435485839844\n",
      "Valid Loss: 711.6065673828125\n",
      "Train Loss: 111.94599914550781\n",
      "Valid Loss: 861.7719116210938\n",
      "Train Loss: 109.9501724243164\n",
      "Valid Loss: 899.9224243164062\n",
      "Train Loss: 114.5210952758789\n",
      "Valid Loss: 835.2434692382812\n",
      "Train Loss: 109.00103759765625\n",
      "Valid Loss: 861.1892700195312\n",
      "Train Loss: 110.36522674560547\n",
      "Valid Loss: 735.524658203125\n",
      "Train Loss: 111.85590362548828\n",
      "Valid Loss: 869.4617309570312\n",
      "Train Loss: 111.59524536132812\n",
      "Valid Loss: 913.888671875\n",
      "Train Loss: 115.11402893066406\n",
      "Valid Loss: 811.9976196289062\n",
      "Train Loss: 108.648193359375\n",
      "Valid Loss: 784.1680908203125\n",
      "Train Loss: 113.8256607055664\n",
      "Valid Loss: 730.22509765625\n",
      "Train Loss: 115.125732421875\n",
      "Valid Loss: 816.8851318359375\n",
      "Train Loss: 110.95146942138672\n",
      "Valid Loss: 917.4223022460938\n",
      "Train Loss: 111.38697052001953\n",
      "Valid Loss: 1115.79541015625\n",
      "Train Loss: 115.8838119506836\n",
      "Valid Loss: 855.7424926757812\n",
      "Train Loss: 113.80635833740234\n",
      "Valid Loss: 759.5880126953125\n",
      "Train Loss: 105.37384796142578\n",
      "Valid Loss: 995.965087890625\n",
      "Train Loss: 115.82478332519531\n",
      "Valid Loss: 714.7075805664062\n",
      "Train Loss: 115.50140380859375\n",
      "Valid Loss: 1119.47900390625\n",
      "Train Loss: 112.91319274902344\n",
      "Valid Loss: 725.222412109375\n",
      "Train Loss: 112.39456176757812\n",
      "Valid Loss: 936.7698364257812\n",
      "Train Loss: 106.69070434570312\n",
      "Valid Loss: 754.896240234375\n",
      "Train Loss: 109.11444091796875\n",
      "Valid Loss: 887.650146484375\n",
      "Train Loss: 109.15991973876953\n",
      "Valid Loss: 899.4479370117188\n",
      "Train Loss: 113.05760192871094\n",
      "Valid Loss: 794.1525268554688\n",
      "Train Loss: 111.92011260986328\n",
      "Valid Loss: 981.6781616210938\n",
      "Train Loss: 106.42168426513672\n",
      "Valid Loss: 768.3167114257812\n",
      "Train Loss: 112.56824493408203\n",
      "Valid Loss: 1103.81982421875\n",
      "Train Loss: 111.32313537597656\n",
      "Valid Loss: 775.49072265625\n",
      "Train Loss: 116.01044464111328\n",
      "Valid Loss: 983.4954223632812\n",
      "Train Loss: 115.52790832519531\n",
      "Valid Loss: 788.3982543945312\n",
      "Train Loss: 112.69314575195312\n",
      "Valid Loss: 713.384765625\n",
      "Train Loss: 109.2011489868164\n",
      "Valid Loss: 1112.34814453125\n",
      "Train Loss: 110.98713684082031\n",
      "Valid Loss: 687.9110107421875\n",
      "Train Loss: 113.16205596923828\n",
      "Valid Loss: 907.7009887695312\n",
      "Train Loss: 110.10487365722656\n",
      "Valid Loss: 1100.401123046875\n",
      "Train Loss: 106.42143249511719\n",
      "Valid Loss: 770.42041015625\n",
      "Train Loss: 114.22692108154297\n",
      "Valid Loss: 901.52783203125\n",
      "Train Loss: 110.83212280273438\n",
      "Valid Loss: 935.2957153320312\n",
      "Train Loss: 110.65615844726562\n",
      "Valid Loss: 938.56494140625\n",
      "Train Loss: 113.14093017578125\n",
      "Valid Loss: 798.1773681640625\n",
      "Train Loss: 110.99559020996094\n",
      "Valid Loss: 850.954345703125\n",
      "Train Loss: 110.3512954711914\n",
      "Valid Loss: 866.3406372070312\n",
      "Train Loss: 110.42684173583984\n",
      "Valid Loss: 746.771484375\n",
      "Train Loss: 110.48738861083984\n",
      "Valid Loss: 994.35546875\n",
      "Train Loss: 113.14683532714844\n",
      "Valid Loss: 820.4541625976562\n",
      "Train Loss: 105.9768295288086\n",
      "Valid Loss: 727.8005981445312\n",
      "Train Loss: 110.87589263916016\n",
      "Valid Loss: 967.1650390625\n",
      "Train Loss: 109.67750549316406\n",
      "Valid Loss: 796.3950805664062\n",
      "Train Loss: 116.47471618652344\n",
      "Valid Loss: 864.5392456054688\n",
      "Train Loss: 108.42896270751953\n",
      "Valid Loss: 707.2472534179688\n",
      "Train Loss: 113.26068115234375\n",
      "Valid Loss: 823.9755859375\n",
      "Train Loss: 106.04121398925781\n",
      "Valid Loss: 764.2129516601562\n",
      "Train Loss: 110.76202392578125\n",
      "Valid Loss: 851.5486450195312\n",
      "Train Loss: 111.84178161621094\n",
      "Valid Loss: 790.2095336914062\n",
      "Train Loss: 104.62057495117188\n",
      "Valid Loss: 982.342529296875\n",
      "Train Loss: 109.05003356933594\n",
      "Valid Loss: 774.2339477539062\n",
      "Train Loss: 109.54478454589844\n",
      "Valid Loss: 829.63525390625\n",
      "Train Loss: 115.12548065185547\n",
      "Valid Loss: 1116.060546875\n",
      "Train Loss: 106.31172943115234\n",
      "Valid Loss: 795.6911010742188\n",
      "Train Loss: 111.19886779785156\n",
      "Valid Loss: 721.4140625\n",
      "Train Loss: 108.72806549072266\n",
      "Valid Loss: 958.5435791015625\n",
      "Train Loss: 110.3642578125\n",
      "Valid Loss: 1111.8203125\n",
      "Train Loss: 111.9443588256836\n",
      "Valid Loss: 853.3318481445312\n",
      "Train Loss: 112.91742706298828\n",
      "Valid Loss: 823.631591796875\n",
      "Train Loss: 101.58463287353516\n",
      "Valid Loss: 1095.1329345703125\n",
      "Train Loss: 113.22164916992188\n",
      "Valid Loss: 971.6845703125\n",
      "Train Loss: 110.541748046875\n",
      "Valid Loss: 980.5038452148438\n",
      "Train Loss: 110.77727508544922\n",
      "Valid Loss: 891.4138793945312\n",
      "Train Loss: 106.37711334228516\n",
      "Valid Loss: 906.0452270507812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 111.22925567626953\n",
      "Valid Loss: 738.6792602539062\n",
      "Train Loss: 113.08966064453125\n",
      "Valid Loss: 863.8427124023438\n",
      "Train Loss: 111.64733123779297\n",
      "Valid Loss: 775.7091064453125\n",
      "Train Loss: 114.54255676269531\n",
      "Valid Loss: 996.1203002929688\n",
      "Train Loss: 105.15550231933594\n",
      "Valid Loss: 732.5252075195312\n",
      "Train Loss: 106.81468963623047\n",
      "Valid Loss: 791.4558715820312\n",
      "Train Loss: 107.93365478515625\n",
      "Valid Loss: 870.6603393554688\n",
      "Train Loss: 106.96981811523438\n",
      "Valid Loss: 817.143310546875\n",
      "Train Loss: 111.4213638305664\n",
      "Valid Loss: 747.0218505859375\n",
      "Train Loss: 108.58367156982422\n",
      "Valid Loss: 875.02294921875\n",
      "Train Loss: 115.84741973876953\n",
      "Valid Loss: 771.148681640625\n",
      "Train Loss: 107.5038833618164\n",
      "Valid Loss: 830.148681640625\n",
      "Train Loss: 113.87451171875\n",
      "Valid Loss: 925.7423706054688\n",
      "Train Loss: 108.75664520263672\n",
      "Valid Loss: 797.025634765625\n",
      "Train Loss: 110.99972534179688\n",
      "Valid Loss: 870.4384155273438\n",
      "Train Loss: 116.50851440429688\n",
      "Valid Loss: 726.2362670898438\n",
      "Train Loss: 112.69031524658203\n",
      "Valid Loss: 1119.222900390625\n",
      "Train Loss: 113.29055786132812\n",
      "Valid Loss: 1119.763427734375\n",
      "Train Loss: 105.48226165771484\n",
      "Valid Loss: 978.1826782226562\n",
      "Train Loss: 108.86004638671875\n",
      "Valid Loss: 975.0942993164062\n",
      "Train Loss: 113.84806060791016\n",
      "Valid Loss: 929.3753051757812\n",
      "Train Loss: 109.63420104980469\n",
      "Valid Loss: 1119.5369873046875\n",
      "Train Loss: 110.00569152832031\n",
      "Valid Loss: 909.5491333007812\n",
      "Train Loss: 105.45555114746094\n",
      "Valid Loss: 882.3204956054688\n",
      "Train Loss: 114.26564025878906\n",
      "Valid Loss: 830.0840454101562\n",
      "Train Loss: 110.82913970947266\n",
      "Valid Loss: 993.4515380859375\n",
      "Train Loss: 109.58109283447266\n",
      "Valid Loss: 825.0172729492188\n",
      "Train Loss: 102.62554931640625\n",
      "Valid Loss: 825.14306640625\n",
      "Train Loss: 108.03345489501953\n",
      "Valid Loss: 806.2711181640625\n",
      "Train Loss: 112.05107879638672\n",
      "Valid Loss: 765.3023071289062\n",
      "Train Loss: 115.77513885498047\n",
      "Valid Loss: 731.661865234375\n",
      "Train Loss: 105.63491821289062\n",
      "Valid Loss: 791.0513305664062\n",
      "Train Loss: 110.1724853515625\n",
      "Valid Loss: 1102.8828125\n",
      "Train Loss: 114.61762237548828\n",
      "Valid Loss: 930.3728637695312\n",
      "Train Loss: 113.167724609375\n",
      "Valid Loss: 870.9196166992188\n",
      "Train Loss: 100.94088745117188\n",
      "Valid Loss: 715.31103515625\n",
      "Train Loss: 107.77267456054688\n",
      "Valid Loss: 927.0680541992188\n",
      "Train Loss: 112.19081115722656\n",
      "Valid Loss: 778.123291015625\n",
      "Train Loss: 112.80038452148438\n",
      "Valid Loss: 706.9849853515625\n",
      "Train Loss: 109.58478546142578\n",
      "Valid Loss: 934.0595092773438\n",
      "Train Loss: 111.64869689941406\n",
      "Valid Loss: 915.1091918945312\n",
      "Train Loss: 109.78608703613281\n",
      "Valid Loss: 910.6659545898438\n",
      "Train Loss: 111.65913391113281\n",
      "Valid Loss: 851.4193725585938\n",
      "Train Loss: 111.99125671386719\n",
      "Valid Loss: 760.197998046875\n",
      "Train Loss: 115.28215026855469\n",
      "Valid Loss: 831.9439697265625\n",
      "Train Loss: 107.15046691894531\n",
      "Valid Loss: 894.6355590820312\n",
      "Train Loss: 113.6972885131836\n",
      "Valid Loss: 914.7335815429688\n",
      "Train Loss: 110.9198989868164\n",
      "Valid Loss: 833.1283569335938\n",
      "Train Loss: 115.52995300292969\n",
      "Valid Loss: 973.3801879882812\n",
      "Train Loss: 114.24668884277344\n",
      "Valid Loss: 932.2685546875\n",
      "Train Loss: 107.84513092041016\n",
      "Valid Loss: 763.2276611328125\n",
      "Train Loss: 108.73985290527344\n",
      "Valid Loss: 867.58984375\n",
      "Train Loss: 109.17677307128906\n",
      "Valid Loss: 735.0377197265625\n",
      "Train Loss: 115.76068115234375\n",
      "Valid Loss: 766.843505859375\n",
      "Train Loss: 112.7015380859375\n",
      "Valid Loss: 1120.173828125\n",
      "Train Loss: 110.7321548461914\n",
      "Valid Loss: 932.3421630859375\n",
      "Train Loss: 109.90045928955078\n",
      "Valid Loss: 802.2041015625\n",
      "Train Loss: 110.8985824584961\n",
      "Valid Loss: 937.8709106445312\n",
      "Train Loss: 108.13024139404297\n",
      "Valid Loss: 916.9064331054688\n",
      "Train Loss: 108.24488067626953\n",
      "Valid Loss: 868.3338012695312\n",
      "Train Loss: 108.1348876953125\n",
      "Valid Loss: 775.9071044921875\n",
      "Train Loss: 110.60245513916016\n",
      "Valid Loss: 829.273193359375\n",
      "Train Loss: 113.2949447631836\n",
      "Valid Loss: 1110.7227783203125\n",
      "Train Loss: 109.84452819824219\n",
      "Valid Loss: 780.4432373046875\n",
      "Train Loss: 112.5468978881836\n",
      "Valid Loss: 1113.1968994140625\n",
      "Train Loss: 106.50415802001953\n",
      "Valid Loss: 777.7661743164062\n",
      "Train Loss: 108.87631225585938\n",
      "Valid Loss: 736.4382934570312\n",
      "Train Loss: 113.67982482910156\n",
      "Valid Loss: 729.1414184570312\n",
      "Train Loss: 111.2034683227539\n",
      "Valid Loss: 966.4885864257812\n",
      "Train Loss: 113.65437316894531\n",
      "Valid Loss: 833.1358642578125\n",
      "Train Loss: 113.2818603515625\n",
      "Valid Loss: 867.7901000976562\n",
      "Train Loss: 101.18663787841797\n",
      "Valid Loss: 979.368896484375\n",
      "Train Loss: 114.3779296875\n",
      "Valid Loss: 771.392333984375\n",
      "Train Loss: 111.07060241699219\n",
      "Valid Loss: 937.831787109375\n",
      "Train Loss: 109.90220642089844\n",
      "Valid Loss: 866.3538208007812\n",
      "Train Loss: 113.61318969726562\n",
      "Valid Loss: 739.2479248046875\n",
      "Train Loss: 104.6837387084961\n",
      "Valid Loss: 1100.0472412109375\n",
      "Train Loss: 104.0695571899414\n",
      "Valid Loss: 800.4716186523438\n",
      "Train Loss: 111.37873840332031\n",
      "Valid Loss: 936.1995239257812\n",
      "Train Loss: 111.01948547363281\n",
      "Valid Loss: 992.1442260742188\n",
      "Train Loss: 109.93416595458984\n",
      "Valid Loss: 785.1384887695312\n",
      "Train Loss: 108.52549743652344\n",
      "Valid Loss: 722.5761108398438\n",
      "Train Loss: 110.59125518798828\n",
      "Valid Loss: 977.9459838867188\n",
      "Train Loss: 109.48896026611328\n",
      "Valid Loss: 978.9114379882812\n",
      "Train Loss: 114.96662902832031\n",
      "Valid Loss: 891.4276733398438\n",
      "Train Loss: 103.690673828125\n",
      "Valid Loss: 801.2490844726562\n",
      "Train Loss: 111.34165954589844\n",
      "Valid Loss: 735.8773803710938\n",
      "Train Loss: 110.31056213378906\n",
      "Valid Loss: 998.00439453125\n",
      "Train Loss: 108.79483795166016\n",
      "Valid Loss: 826.62646484375\n",
      "Train Loss: 115.65120697021484\n",
      "Valid Loss: 800.5730590820312\n",
      "Train Loss: 110.65350341796875\n",
      "Valid Loss: 1110.896728515625\n",
      "Train Loss: 113.45541381835938\n",
      "Valid Loss: 786.1497192382812\n",
      "Train Loss: 109.24288177490234\n",
      "Valid Loss: 981.8866577148438\n",
      "Train Loss: 112.29905700683594\n",
      "Valid Loss: 997.795166015625\n",
      "Train Loss: 112.00758361816406\n",
      "Valid Loss: 1120.633544921875\n",
      "Train Loss: 106.20960235595703\n",
      "Valid Loss: 737.6036376953125\n",
      "Train Loss: 113.8772201538086\n",
      "Valid Loss: 893.2438354492188\n",
      "Train Loss: 110.25440979003906\n",
      "Valid Loss: 755.7166748046875\n",
      "Train Loss: 110.47953033447266\n",
      "Valid Loss: 876.13720703125\n",
      "Train Loss: 108.76809692382812\n",
      "Valid Loss: 967.9267578125\n",
      "Train Loss: 106.81687927246094\n",
      "Valid Loss: 936.4175415039062\n",
      "Train Loss: 113.5711669921875\n",
      "Valid Loss: 1113.9593505859375\n",
      "Train Loss: 108.61756896972656\n",
      "Valid Loss: 782.3356323242188\n",
      "Train Loss: 113.25898742675781\n",
      "Valid Loss: 1116.888427734375\n",
      "Train Loss: 111.84989929199219\n",
      "Valid Loss: 776.5283203125\n",
      "Train Loss: 116.20707702636719\n",
      "Valid Loss: 875.781494140625\n",
      "Train Loss: 103.63129425048828\n",
      "Valid Loss: 909.5656127929688\n",
      "Train Loss: 106.50167083740234\n",
      "Valid Loss: 923.7078857421875\n",
      "Train Loss: 112.260498046875\n",
      "Valid Loss: 886.2420043945312\n",
      "Train Loss: 106.80815887451172\n",
      "Valid Loss: 910.9174194335938\n",
      "Train Loss: 115.96311950683594\n",
      "Valid Loss: 1104.0916748046875\n",
      "Train Loss: 114.26889038085938\n",
      "Valid Loss: 980.0768432617188\n",
      "Train Loss: 110.11091613769531\n",
      "Valid Loss: 782.839111328125\n",
      "Train Loss: 109.99829864501953\n",
      "Valid Loss: 747.4996337890625\n",
      "Train Loss: 112.90957641601562\n",
      "Valid Loss: 713.3466796875\n",
      "Train Loss: 107.20462799072266\n",
      "Valid Loss: 904.2869262695312\n",
      "Train Loss: 110.85233306884766\n",
      "Valid Loss: 715.9710083007812\n",
      "Train Loss: 112.71302795410156\n",
      "Valid Loss: 829.0501708984375\n",
      "Train Loss: 109.70268249511719\n",
      "Valid Loss: 1120.4820556640625\n",
      "Train Loss: 112.32769012451172\n",
      "Valid Loss: 872.3129272460938\n",
      "Train Loss: 107.9025650024414\n",
      "Valid Loss: 867.7009887695312\n",
      "Train Loss: 104.59574890136719\n",
      "Valid Loss: 772.1475219726562\n",
      "Train Loss: 114.18193817138672\n",
      "Valid Loss: 741.6990966796875\n",
      "Train Loss: 107.87400817871094\n",
      "Valid Loss: 740.753662109375\n",
      "Train Loss: 111.39689636230469\n",
      "Valid Loss: 887.7620239257812\n",
      "Train Loss: 114.50431823730469\n",
      "Valid Loss: 786.1185913085938\n",
      "Train Loss: 110.15986633300781\n",
      "Valid Loss: 911.0100708007812\n",
      "Train Loss: 109.51138305664062\n",
      "Valid Loss: 993.67529296875\n",
      "Train Loss: 114.33068084716797\n",
      "Valid Loss: 910.743896484375\n",
      "Train Loss: 104.16796875\n",
      "Valid Loss: 786.1065063476562\n",
      "Train Loss: 115.0430908203125\n",
      "Valid Loss: 778.407958984375\n",
      "Train Loss: 105.79057312011719\n",
      "Valid Loss: 797.95263671875\n",
      "Train Loss: 111.17700958251953\n",
      "Valid Loss: 798.9099731445312\n",
      "Train Loss: 111.81413269042969\n",
      "Valid Loss: 929.1156616210938\n",
      "Train Loss: 116.0113754272461\n",
      "Valid Loss: 923.8438110351562\n",
      "Train Loss: 102.22703552246094\n",
      "Valid Loss: 774.31982421875\n",
      "Train Loss: 114.26823425292969\n",
      "Valid Loss: 802.016845703125\n",
      "Train Loss: 110.03651428222656\n",
      "Valid Loss: 734.4320068359375\n",
      "Train Loss: 109.66443634033203\n",
      "Valid Loss: 799.2755737304688\n",
      "Train Loss: 113.81090545654297\n",
      "Valid Loss: 779.4417114257812\n",
      "Train Loss: 111.01129150390625\n",
      "Valid Loss: 755.8018188476562\n",
      "Train Loss: 108.7332763671875\n",
      "Valid Loss: 911.08837890625\n",
      "Train Loss: 106.44209289550781\n",
      "Valid Loss: 1116.187744140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 111.32588958740234\n",
      "Valid Loss: 798.2139282226562\n",
      "Train Loss: 110.87149810791016\n",
      "Valid Loss: 782.9210205078125\n",
      "Train Loss: 112.72086334228516\n",
      "Valid Loss: 997.3516235351562\n",
      "Train Loss: 107.16419982910156\n",
      "Valid Loss: 769.755126953125\n",
      "Train Loss: 108.31005096435547\n",
      "Valid Loss: 781.93994140625\n",
      "Train Loss: 110.4136962890625\n",
      "Valid Loss: 832.067138671875\n",
      "Train Loss: 113.20555877685547\n",
      "Valid Loss: 1107.60498046875\n",
      "Train Loss: 112.46891784667969\n",
      "Valid Loss: 935.2734985351562\n",
      "Train Loss: 114.17710876464844\n",
      "Valid Loss: 798.8885498046875\n",
      "Train Loss: 112.98951721191406\n",
      "Valid Loss: 834.3778686523438\n",
      "Train Loss: 107.28947448730469\n",
      "Valid Loss: 1122.1116943359375\n",
      "Train Loss: 108.08219909667969\n",
      "Valid Loss: 725.94775390625\n",
      "Train Loss: 110.81704711914062\n",
      "Valid Loss: 919.59814453125\n",
      "Train Loss: 109.20233154296875\n",
      "Valid Loss: 1102.938232421875\n",
      "Train Loss: 115.06752014160156\n",
      "Valid Loss: 888.7695922851562\n",
      "Train Loss: 115.39398956298828\n",
      "Valid Loss: 831.8958740234375\n",
      "Train Loss: 111.03118133544922\n",
      "Valid Loss: 773.833251953125\n",
      "Train Loss: 113.80554962158203\n",
      "Valid Loss: 1120.273681640625\n",
      "Train Loss: 112.50839233398438\n",
      "Valid Loss: 726.723388671875\n",
      "Train Loss: 104.90203857421875\n",
      "Valid Loss: 787.458740234375\n",
      "Train Loss: 112.95722961425781\n",
      "Valid Loss: 917.1439819335938\n",
      "Train Loss: 110.98922729492188\n",
      "Valid Loss: 776.943359375\n",
      "Train Loss: 109.5484619140625\n",
      "Valid Loss: 862.8064575195312\n",
      "Train Loss: 113.42596435546875\n",
      "Valid Loss: 915.6166381835938\n",
      "Train Loss: 115.16766357421875\n",
      "Valid Loss: 915.7685546875\n",
      "Train Loss: 110.29376983642578\n",
      "Valid Loss: 829.9746704101562\n",
      "Train Loss: 107.37255859375\n",
      "Valid Loss: 753.224365234375\n",
      "Train Loss: 106.79901123046875\n",
      "Valid Loss: 706.8474731445312\n",
      "Train Loss: 111.48715209960938\n",
      "Valid Loss: 998.17724609375\n",
      "Train Loss: 109.40159606933594\n",
      "Valid Loss: 769.9305419921875\n",
      "Train Loss: 112.6346664428711\n",
      "Valid Loss: 979.5188598632812\n",
      "Train Loss: 111.86296844482422\n",
      "Valid Loss: 801.1838989257812\n",
      "Train Loss: 108.22859954833984\n",
      "Valid Loss: 852.08154296875\n",
      "Train Loss: 109.57849884033203\n",
      "Valid Loss: 786.9890747070312\n",
      "Train Loss: 112.04075622558594\n",
      "Valid Loss: 991.460205078125\n",
      "Train Loss: 111.82627868652344\n",
      "Valid Loss: 830.0303344726562\n",
      "Train Loss: 113.86967468261719\n",
      "Valid Loss: 738.4134521484375\n",
      "Train Loss: 110.8189926147461\n",
      "Valid Loss: 871.77587890625\n",
      "Train Loss: 110.52749633789062\n",
      "Valid Loss: 729.3340454101562\n",
      "Train Loss: 110.94976806640625\n",
      "Valid Loss: 1119.095458984375\n",
      "Train Loss: 107.26089477539062\n",
      "Valid Loss: 917.05078125\n",
      "Train Loss: 113.83255004882812\n",
      "Valid Loss: 831.449462890625\n",
      "Train Loss: 111.01716613769531\n",
      "Valid Loss: 1120.71826171875\n",
      "Train Loss: 108.26117706298828\n",
      "Valid Loss: 868.720947265625\n",
      "Train Loss: 107.31716918945312\n",
      "Valid Loss: 823.53515625\n",
      "Train Loss: 103.94710540771484\n",
      "Valid Loss: 806.6572875976562\n",
      "Train Loss: 109.92919921875\n",
      "Valid Loss: 802.1260986328125\n",
      "Train Loss: 111.88627624511719\n",
      "Valid Loss: 972.583740234375\n",
      "Train Loss: 111.00004577636719\n",
      "Valid Loss: 1097.302001953125\n",
      "Train Loss: 111.6438217163086\n",
      "Valid Loss: 828.9857177734375\n",
      "Train Loss: 110.62451934814453\n",
      "Valid Loss: 892.1902465820312\n",
      "Train Loss: 113.77384948730469\n",
      "Valid Loss: 1122.2626953125\n",
      "Train Loss: 109.58760070800781\n",
      "Valid Loss: 736.22900390625\n",
      "Train Loss: 110.28824615478516\n",
      "Valid Loss: 939.9220581054688\n",
      "Train Loss: 109.88226318359375\n",
      "Valid Loss: 763.8637084960938\n",
      "Train Loss: 108.31468963623047\n",
      "Valid Loss: 803.187255859375\n",
      "Train Loss: 109.64085388183594\n",
      "Valid Loss: 801.7713623046875\n",
      "Train Loss: 111.73860931396484\n",
      "Valid Loss: 1121.896484375\n",
      "Train Loss: 115.4835205078125\n",
      "Valid Loss: 866.5966796875\n",
      "Train Loss: 110.16989135742188\n",
      "Valid Loss: 767.0208129882812\n",
      "Train Loss: 109.19544219970703\n",
      "Valid Loss: 933.4540405273438\n",
      "Train Loss: 111.53097534179688\n",
      "Valid Loss: 893.6836547851562\n",
      "Train Loss: 111.03257751464844\n",
      "Valid Loss: 1118.2286376953125\n",
      "Train Loss: 109.19712829589844\n",
      "Valid Loss: 842.71728515625\n",
      "Train Loss: 111.88137817382812\n",
      "Valid Loss: 768.1988525390625\n",
      "Train Loss: 109.00843811035156\n",
      "Valid Loss: 766.2598876953125\n",
      "Train Loss: 110.88423919677734\n",
      "Valid Loss: 785.808349609375\n",
      "Train Loss: 103.24421691894531\n",
      "Valid Loss: 736.238525390625\n",
      "Train Loss: 112.775146484375\n",
      "Valid Loss: 988.0994262695312\n",
      "Train Loss: 101.71973419189453\n",
      "Valid Loss: 1122.591796875\n",
      "Train Loss: 112.96412658691406\n",
      "Valid Loss: 811.17333984375\n",
      "Train Loss: 108.76083374023438\n",
      "Valid Loss: 933.1650390625\n",
      "Train Loss: 109.3970947265625\n",
      "Valid Loss: 1123.2220458984375\n",
      "Train Loss: 113.58842468261719\n",
      "Valid Loss: 742.5155029296875\n",
      "Train Loss: 112.97596740722656\n",
      "Valid Loss: 791.5853881835938\n",
      "Train Loss: 111.7091293334961\n",
      "Valid Loss: 867.64013671875\n",
      "Train Loss: 111.2149887084961\n",
      "Valid Loss: 911.4595336914062\n",
      "Train Loss: 108.80970001220703\n",
      "Valid Loss: 743.0228271484375\n",
      "Train Loss: 109.70475006103516\n",
      "Valid Loss: 895.4067993164062\n",
      "Train Loss: 112.11286926269531\n",
      "Valid Loss: 772.3711547851562\n",
      "Train Loss: 108.0908203125\n",
      "Valid Loss: 734.7371215820312\n",
      "Train Loss: 115.87326049804688\n",
      "Valid Loss: 748.2720336914062\n",
      "Train Loss: 109.09466552734375\n",
      "Valid Loss: 910.5156860351562\n",
      "Train Loss: 113.94960021972656\n",
      "Valid Loss: 737.127685546875\n",
      "Train Loss: 103.73290252685547\n",
      "Valid Loss: 1107.9974365234375\n",
      "Train Loss: 105.07466125488281\n",
      "Valid Loss: 753.2060546875\n",
      "Train Loss: 113.95521545410156\n",
      "Valid Loss: 855.982666015625\n",
      "Train Loss: 109.75433349609375\n",
      "Valid Loss: 828.1068725585938\n",
      "Train Loss: 110.5071029663086\n",
      "Valid Loss: 1122.564697265625\n",
      "Train Loss: 107.70504760742188\n",
      "Valid Loss: 763.9332275390625\n",
      "Train Loss: 103.8958740234375\n",
      "Valid Loss: 908.8656616210938\n",
      "Train Loss: 114.8960952758789\n",
      "Valid Loss: 779.2359008789062\n",
      "Train Loss: 109.87184143066406\n",
      "Valid Loss: 933.302978515625\n",
      "Train Loss: 114.31184387207031\n",
      "Valid Loss: 775.8895874023438\n",
      "Train Loss: 107.3793716430664\n",
      "Valid Loss: 873.10595703125\n",
      "Train Loss: 105.77509307861328\n",
      "Valid Loss: 873.7891845703125\n",
      "Train Loss: 106.53646087646484\n",
      "Valid Loss: 834.3641967773438\n",
      "Train Loss: 104.91051483154297\n",
      "Valid Loss: 888.7039184570312\n",
      "Train Loss: 113.32262420654297\n",
      "Valid Loss: 789.4071044921875\n",
      "Train Loss: 107.79039001464844\n",
      "Valid Loss: 741.5811767578125\n",
      "Train Loss: 113.12388610839844\n",
      "Valid Loss: 734.7911987304688\n",
      "Train Loss: 108.42850494384766\n",
      "Valid Loss: 736.127685546875\n",
      "Train Loss: 105.47477722167969\n",
      "Valid Loss: 895.5849609375\n",
      "Train Loss: 111.76199340820312\n",
      "Valid Loss: 867.4551391601562\n",
      "Train Loss: 107.84095764160156\n",
      "Valid Loss: 820.7967529296875\n",
      "Train Loss: 105.68325805664062\n",
      "Valid Loss: 797.89111328125\n",
      "Train Loss: 109.16567993164062\n",
      "Valid Loss: 983.6613159179688\n",
      "Train Loss: 111.025390625\n",
      "Valid Loss: 839.2836303710938\n",
      "Train Loss: 108.613037109375\n",
      "Valid Loss: 787.5296020507812\n",
      "Train Loss: 110.9137954711914\n",
      "Valid Loss: 936.3638305664062\n",
      "Train Loss: 117.13140869140625\n",
      "Valid Loss: 986.1487426757812\n",
      "Train Loss: 107.59635925292969\n",
      "Valid Loss: 927.5488891601562\n",
      "Train Loss: 111.87223052978516\n",
      "Valid Loss: 869.0283813476562\n",
      "Train Loss: 107.80614471435547\n",
      "Valid Loss: 789.4440307617188\n",
      "Train Loss: 110.99809265136719\n",
      "Valid Loss: 876.9617309570312\n",
      "Train Loss: 112.08181762695312\n",
      "Valid Loss: 999.0484008789062\n",
      "Train Loss: 109.07622528076172\n",
      "Valid Loss: 774.585693359375\n",
      "Train Loss: 115.58998107910156\n",
      "Valid Loss: 813.62548828125\n",
      "Train Loss: 106.5810775756836\n",
      "Valid Loss: 864.739501953125\n",
      "Train Loss: 115.0426254272461\n",
      "Valid Loss: 825.3618774414062\n",
      "Train Loss: 112.66989135742188\n",
      "Valid Loss: 779.26611328125\n",
      "Train Loss: 104.18919372558594\n",
      "Valid Loss: 798.80908203125\n",
      "Train Loss: 109.82196044921875\n",
      "Valid Loss: 780.9758911132812\n",
      "Train Loss: 104.66889190673828\n",
      "Valid Loss: 1001.1657104492188\n",
      "Train Loss: 110.16948699951172\n",
      "Valid Loss: 835.0421142578125\n",
      "Train Loss: 112.50947570800781\n",
      "Valid Loss: 724.590087890625\n",
      "Train Loss: 110.4901123046875\n",
      "Valid Loss: 729.4554443359375\n",
      "Train Loss: 111.02261352539062\n",
      "Valid Loss: 819.5338134765625\n",
      "Train Loss: 113.59613800048828\n",
      "Valid Loss: 803.101318359375\n",
      "Train Loss: 109.40436553955078\n",
      "Valid Loss: 794.032470703125\n",
      "Train Loss: 103.89369201660156\n",
      "Valid Loss: 840.4783325195312\n",
      "Train Loss: 111.26827239990234\n",
      "Valid Loss: 792.70654296875\n",
      "Train Loss: 106.66734313964844\n",
      "Valid Loss: 931.2275390625\n",
      "Train Loss: 112.73493957519531\n",
      "Valid Loss: 739.8096313476562\n",
      "Train Loss: 112.75785064697266\n",
      "Valid Loss: 947.8063354492188\n",
      "Train Loss: 104.89141845703125\n",
      "Valid Loss: 728.8355712890625\n",
      "Train Loss: 105.52298736572266\n",
      "Valid Loss: 840.5161743164062\n",
      "Train Loss: 108.94224548339844\n",
      "Valid Loss: 729.6255493164062\n",
      "Train Loss: 112.8592300415039\n",
      "Valid Loss: 756.495849609375\n",
      "Train Loss: 111.15559387207031\n",
      "Valid Loss: 895.8287963867188\n",
      "Train Loss: 111.96092987060547\n",
      "Valid Loss: 855.0548095703125\n",
      "Train Loss: 109.34053802490234\n",
      "Valid Loss: 982.7411499023438\n",
      "Train Loss: 106.20184326171875\n",
      "Valid Loss: 964.0762939453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 111.08951568603516\n",
      "Valid Loss: 1100.107421875\n",
      "Train Loss: 114.69695281982422\n",
      "Valid Loss: 744.1094360351562\n",
      "Train Loss: 116.25482177734375\n",
      "Valid Loss: 994.825927734375\n",
      "Train Loss: 111.68946075439453\n",
      "Valid Loss: 730.3470458984375\n",
      "Train Loss: 109.29320526123047\n",
      "Valid Loss: 998.4586791992188\n",
      "Train Loss: 116.57160949707031\n",
      "Valid Loss: 939.901123046875\n",
      "Train Loss: 107.59638214111328\n",
      "Valid Loss: 761.652099609375\n",
      "Train Loss: 110.42915344238281\n",
      "Valid Loss: 835.6305541992188\n",
      "Train Loss: 111.99349212646484\n",
      "Valid Loss: 776.5693969726562\n",
      "Train Loss: 114.291259765625\n",
      "Valid Loss: 792.171142578125\n",
      "Train Loss: 109.99340057373047\n",
      "Valid Loss: 777.686767578125\n",
      "Train Loss: 110.50584411621094\n",
      "Valid Loss: 843.9744262695312\n",
      "Train Loss: 105.75413513183594\n",
      "Valid Loss: 1122.4970703125\n",
      "Train Loss: 114.58624267578125\n",
      "Valid Loss: 999.3917846679688\n",
      "Train Loss: 116.084716796875\n",
      "Valid Loss: 925.5983276367188\n",
      "Train Loss: 105.66157531738281\n",
      "Valid Loss: 936.361572265625\n",
      "Train Loss: 107.05984497070312\n",
      "Valid Loss: 737.6382446289062\n",
      "Train Loss: 113.05369567871094\n",
      "Valid Loss: 781.4674682617188\n",
      "Train Loss: 104.22026824951172\n",
      "Valid Loss: 798.0069580078125\n",
      "Train Loss: 113.28256225585938\n",
      "Valid Loss: 815.0755004882812\n",
      "Train Loss: 116.32266235351562\n",
      "Valid Loss: 716.63525390625\n",
      "Train Loss: 107.60029602050781\n",
      "Valid Loss: 888.1851806640625\n",
      "Train Loss: 110.91233825683594\n",
      "Valid Loss: 735.245849609375\n",
      "Train Loss: 115.46137237548828\n",
      "Valid Loss: 998.3020629882812\n",
      "Train Loss: 115.20121765136719\n",
      "Valid Loss: 796.0689086914062\n",
      "Train Loss: 104.08123779296875\n",
      "Valid Loss: 919.19482421875\n",
      "Train Loss: 111.81981658935547\n",
      "Valid Loss: 741.274658203125\n",
      "Train Loss: 110.29873657226562\n",
      "Valid Loss: 898.750244140625\n",
      "Train Loss: 112.4681625366211\n",
      "Valid Loss: 914.5408325195312\n",
      "Train Loss: 111.76421356201172\n",
      "Valid Loss: 982.8886108398438\n",
      "Train Loss: 107.57185363769531\n",
      "Valid Loss: 732.0919189453125\n",
      "Train Loss: 113.20277404785156\n",
      "Valid Loss: 1124.3544921875\n",
      "Train Loss: 114.9905776977539\n",
      "Valid Loss: 793.8854370117188\n",
      "Train Loss: 112.85774993896484\n",
      "Valid Loss: 825.2772216796875\n",
      "Train Loss: 114.88428497314453\n",
      "Valid Loss: 918.0162353515625\n",
      "Train Loss: 110.34180450439453\n",
      "Valid Loss: 1123.232421875\n",
      "Train Loss: 107.52470397949219\n",
      "Valid Loss: 777.637939453125\n",
      "Train Loss: 110.95845031738281\n",
      "Valid Loss: 869.4675903320312\n",
      "Train Loss: 109.50554656982422\n",
      "Valid Loss: 769.4360961914062\n",
      "Train Loss: 109.26856994628906\n",
      "Valid Loss: 691.4208984375\n",
      "Train Loss: 114.40784454345703\n",
      "Valid Loss: 749.0892333984375\n",
      "Train Loss: 113.87996673583984\n",
      "Valid Loss: 854.7252197265625\n",
      "Train Loss: 112.6394271850586\n",
      "Valid Loss: 1117.5999755859375\n",
      "Train Loss: 112.4283447265625\n",
      "Valid Loss: 734.1444702148438\n",
      "Train Loss: 115.20436096191406\n",
      "Valid Loss: 808.0938110351562\n",
      "Train Loss: 109.58042907714844\n",
      "Valid Loss: 798.3998413085938\n",
      "Train Loss: 111.08946990966797\n",
      "Valid Loss: 813.2984619140625\n",
      "Train Loss: 110.61337280273438\n",
      "Valid Loss: 802.5919799804688\n",
      "Train Loss: 111.11511993408203\n",
      "Valid Loss: 1124.3853759765625\n",
      "Train Loss: 107.17987060546875\n",
      "Valid Loss: 893.1525268554688\n",
      "Train Loss: 109.40596771240234\n",
      "Valid Loss: 986.0142211914062\n",
      "Train Loss: 111.99000549316406\n",
      "Valid Loss: 768.2135009765625\n",
      "Train Loss: 109.79049682617188\n",
      "Valid Loss: 1124.6715087890625\n",
      "Train Loss: 111.51040649414062\n",
      "Valid Loss: 780.3751831054688\n",
      "Train Loss: 114.58260345458984\n",
      "Valid Loss: 857.46044921875\n",
      "Train Loss: 107.7752685546875\n",
      "Valid Loss: 783.2935791015625\n",
      "Train Loss: 115.19918823242188\n",
      "Valid Loss: 826.194580078125\n",
      "Train Loss: 110.06001281738281\n",
      "Valid Loss: 767.718994140625\n",
      "Train Loss: 113.27156829833984\n",
      "Valid Loss: 793.7559814453125\n",
      "Train Loss: 102.37068939208984\n",
      "Valid Loss: 979.8858032226562\n",
      "Train Loss: 110.35162353515625\n",
      "Valid Loss: 776.1068725585938\n",
      "Train Loss: 109.20856475830078\n",
      "Valid Loss: 781.0089111328125\n",
      "Train Loss: 111.20882415771484\n",
      "Valid Loss: 980.8085327148438\n",
      "Train Loss: 111.67613220214844\n",
      "Valid Loss: 794.519287109375\n",
      "Train Loss: 114.28617858886719\n",
      "Valid Loss: 868.4385375976562\n",
      "Train Loss: 112.80694580078125\n",
      "Valid Loss: 906.4614868164062\n",
      "Train Loss: 109.79302978515625\n",
      "Valid Loss: 713.87841796875\n",
      "Train Loss: 111.47731018066406\n",
      "Valid Loss: 1122.0462646484375\n",
      "Train Loss: 109.00718688964844\n",
      "Valid Loss: 778.6260986328125\n",
      "Train Loss: 114.76469421386719\n",
      "Valid Loss: 873.1182250976562\n",
      "Train Loss: 112.81597137451172\n",
      "Valid Loss: 826.9092407226562\n",
      "Train Loss: 111.39523315429688\n",
      "Valid Loss: 789.6058959960938\n",
      "Train Loss: 111.82268524169922\n",
      "Valid Loss: 801.8941040039062\n",
      "Train Loss: 108.93619537353516\n",
      "Valid Loss: 922.2966918945312\n",
      "Train Loss: 104.54757690429688\n",
      "Valid Loss: 967.4361572265625\n",
      "Train Loss: 106.98186492919922\n",
      "Valid Loss: 740.43896484375\n",
      "Train Loss: 107.38753509521484\n",
      "Valid Loss: 874.0352783203125\n",
      "Train Loss: 106.65087127685547\n",
      "Valid Loss: 826.4451904296875\n",
      "Train Loss: 114.1004409790039\n",
      "Valid Loss: 869.2084350585938\n",
      "Train Loss: 106.27920532226562\n",
      "Valid Loss: 856.0007934570312\n",
      "Train Loss: 112.08531188964844\n",
      "Valid Loss: 799.24365234375\n",
      "Train Loss: 111.60006713867188\n",
      "Valid Loss: 865.3582153320312\n",
      "Train Loss: 109.29331970214844\n",
      "Valid Loss: 749.5714721679688\n",
      "Train Loss: 109.95306396484375\n",
      "Valid Loss: 787.0203247070312\n",
      "Train Loss: 113.04840850830078\n",
      "Valid Loss: 855.5890502929688\n",
      "Train Loss: 114.2525863647461\n",
      "Valid Loss: 1120.390625\n",
      "Train Loss: 111.66295623779297\n",
      "Valid Loss: 981.675048828125\n",
      "Train Loss: 110.62327575683594\n",
      "Valid Loss: 933.807861328125\n",
      "Train Loss: 109.6503677368164\n",
      "Valid Loss: 1105.0135498046875\n",
      "Train Loss: 114.17591094970703\n",
      "Valid Loss: 865.5751342773438\n",
      "Train Loss: 112.82089233398438\n",
      "Valid Loss: 748.094482421875\n",
      "Train Loss: 101.61268615722656\n",
      "Valid Loss: 971.3729858398438\n",
      "Train Loss: 110.00975799560547\n",
      "Valid Loss: 919.1942749023438\n",
      "Train Loss: 106.0453109741211\n",
      "Valid Loss: 770.159423828125\n",
      "Train Loss: 113.10328674316406\n",
      "Valid Loss: 902.0582885742188\n",
      "Train Loss: 108.79547119140625\n",
      "Valid Loss: 761.834228515625\n",
      "Train Loss: 107.54753875732422\n",
      "Valid Loss: 754.7403564453125\n",
      "Train Loss: 106.89703369140625\n",
      "Valid Loss: 1122.6396484375\n",
      "Train Loss: 111.91388702392578\n",
      "Valid Loss: 788.9219970703125\n",
      "Train Loss: 106.50191497802734\n",
      "Valid Loss: 767.6853637695312\n",
      "Train Loss: 107.35796356201172\n",
      "Valid Loss: 790.6573486328125\n",
      "Train Loss: 112.94178771972656\n",
      "Valid Loss: 803.7021484375\n",
      "Train Loss: 114.6292495727539\n",
      "Valid Loss: 740.1370849609375\n",
      "Train Loss: 112.46266174316406\n",
      "Valid Loss: 983.59619140625\n",
      "Train Loss: 105.92308807373047\n",
      "Valid Loss: 993.50146484375\n",
      "Train Loss: 112.84040069580078\n",
      "Valid Loss: 1116.72998046875\n",
      "Train Loss: 110.69815063476562\n",
      "Valid Loss: 779.8850708007812\n",
      "Train Loss: 111.80835723876953\n",
      "Valid Loss: 905.4425659179688\n",
      "Train Loss: 110.84870910644531\n",
      "Valid Loss: 796.64306640625\n",
      "Train Loss: 113.22480010986328\n",
      "Valid Loss: 823.3480224609375\n",
      "Train Loss: 107.52108764648438\n",
      "Valid Loss: 814.4220581054688\n",
      "Train Loss: 114.66700744628906\n",
      "Valid Loss: 768.4060668945312\n",
      "Train Loss: 101.62319946289062\n",
      "Valid Loss: 1127.13134765625\n",
      "Train Loss: 113.05070495605469\n",
      "Valid Loss: 809.9873657226562\n",
      "Train Loss: 108.93486022949219\n",
      "Valid Loss: 807.3175048828125\n",
      "Train Loss: 110.15584564208984\n",
      "Valid Loss: 829.4646606445312\n",
      "Train Loss: 111.03760528564453\n",
      "Valid Loss: 929.5004272460938\n",
      "Train Loss: 113.47129821777344\n",
      "Valid Loss: 980.1560668945312\n",
      "Train Loss: 106.7568130493164\n",
      "Valid Loss: 919.6488037109375\n",
      "Train Loss: 105.89854431152344\n",
      "Valid Loss: 939.649169921875\n",
      "Train Loss: 103.34992218017578\n",
      "Valid Loss: 748.0720825195312\n",
      "Train Loss: 111.10907745361328\n",
      "Valid Loss: 785.3967895507812\n",
      "Train Loss: 102.60840606689453\n",
      "Valid Loss: 743.3877563476562\n",
      "Train Loss: 113.82559204101562\n",
      "Valid Loss: 762.9631958007812\n",
      "Train Loss: 109.68666076660156\n",
      "Valid Loss: 935.433837890625\n",
      "Train Loss: 110.15391540527344\n",
      "Valid Loss: 921.1831665039062\n",
      "Train Loss: 110.45128631591797\n",
      "Valid Loss: 1119.283447265625\n",
      "Train Loss: 111.00187683105469\n",
      "Valid Loss: 870.3971557617188\n",
      "Train Loss: 112.64588928222656\n",
      "Valid Loss: 798.1995239257812\n",
      "Train Loss: 109.4857177734375\n",
      "Valid Loss: 785.5192260742188\n",
      "Train Loss: 111.31302642822266\n",
      "Valid Loss: 857.7108764648438\n",
      "Train Loss: 111.43327331542969\n",
      "Valid Loss: 743.5591430664062\n",
      "Train Loss: 112.27413940429688\n",
      "Valid Loss: 835.980712890625\n",
      "Train Loss: 108.40141296386719\n",
      "Valid Loss: 902.8107299804688\n",
      "Train Loss: 109.57179260253906\n",
      "Valid Loss: 1124.789794921875\n",
      "Train Loss: 109.55917358398438\n",
      "Valid Loss: 972.9482421875\n",
      "Train Loss: 109.12740325927734\n",
      "Valid Loss: 753.6710205078125\n",
      "Train Loss: 114.73575592041016\n",
      "Valid Loss: 902.1537475585938\n",
      "Train Loss: 110.30747985839844\n",
      "Valid Loss: 1094.2965087890625\n",
      "Train Loss: 109.92635345458984\n",
      "Valid Loss: 916.91845703125\n",
      "Train Loss: 110.7309799194336\n",
      "Valid Loss: 768.5082397460938\n",
      "Min RMSE: 537.5588989257812\n"
     ]
    }
   ],
   "source": [
    "Train(1000, model, train_loader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb56e952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f81df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
