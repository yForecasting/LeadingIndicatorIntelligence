{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0369c9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwight/miniconda3/lib/python3.8/site-packages/tslearn/clustering/kmeans.py:16: UserWarning: Scikit-learn <0.24 will be deprecated in a future release of tslearn\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Native libraries\n",
    "import os\n",
    "import math\n",
    "# Essential Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fredapi import Fred\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Algorithms\n",
    "from statistics import mean, stdev, variance\n",
    "from minisom import MiniSom\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.cluster import KMeans, MeanShift, estimate_bandwidth\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "703bb0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"monthly_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7d14fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fred = Fred(api_key='3c6ba58d26525f17af95af4fabed24be')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c88e827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv2 = csv[50:-20]\n",
    "csv2 = csv2.drop(['SMU06419406562200001.1', 'SMU21000006056170001SA.1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01deecf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNRATE</th>\n",
       "      <th>CPIAUCSL</th>\n",
       "      <th>FEDFUNDS</th>\n",
       "      <th>CSUSHPINSA</th>\n",
       "      <th>M2SL</th>\n",
       "      <th>M1SL</th>\n",
       "      <th>PSAVERT</th>\n",
       "      <th>PAYEMS</th>\n",
       "      <th>INDPRO</th>\n",
       "      <th>TB3MS</th>\n",
       "      <th>...</th>\n",
       "      <th>ESTPIEAMP01GPM</th>\n",
       "      <th>BROSERPA158MFRBDAL</th>\n",
       "      <th>LARTTULA158MFRBDAL</th>\n",
       "      <th>LAUCN310910000000004</th>\n",
       "      <th>LAUCN271470000000004</th>\n",
       "      <th>LFWATTFECAM647N</th>\n",
       "      <th>LFWA55FECAM647S</th>\n",
       "      <th>LFWA55FEKRM647S</th>\n",
       "      <th>LFWA55MAAUM647N</th>\n",
       "      <th>LFWA55MAAUM647S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.4</td>\n",
       "      <td>191.700</td>\n",
       "      <td>1.93</td>\n",
       "      <td>157.497</td>\n",
       "      <td>6399.4</td>\n",
       "      <td>1374.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>132503.0</td>\n",
       "      <td>97.6137</td>\n",
       "      <td>2.07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.202429</td>\n",
       "      <td>-1.713722</td>\n",
       "      <td>2.689583</td>\n",
       "      <td>12.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>12971900.0</td>\n",
       "      <td>1725500.0</td>\n",
       "      <td>2.143286e+06</td>\n",
       "      <td>1063251.0</td>\n",
       "      <td>1.062949e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.4</td>\n",
       "      <td>192.400</td>\n",
       "      <td>2.50</td>\n",
       "      <td>161.924</td>\n",
       "      <td>6432.5</td>\n",
       "      <td>1371.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>133032.0</td>\n",
       "      <td>99.4639</td>\n",
       "      <td>2.54</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201410</td>\n",
       "      <td>1.561165</td>\n",
       "      <td>16.658304</td>\n",
       "      <td>25.0</td>\n",
       "      <td>983.0</td>\n",
       "      <td>13010400.0</td>\n",
       "      <td>1745200.0</td>\n",
       "      <td>2.138804e+06</td>\n",
       "      <td>1072885.0</td>\n",
       "      <td>1.073819e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.1</td>\n",
       "      <td>193.600</td>\n",
       "      <td>3.00</td>\n",
       "      <td>169.544</td>\n",
       "      <td>6473.1</td>\n",
       "      <td>1366.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>133690.0</td>\n",
       "      <td>99.6033</td>\n",
       "      <td>2.84</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100604</td>\n",
       "      <td>1.090706</td>\n",
       "      <td>-4.866917</td>\n",
       "      <td>11.0</td>\n",
       "      <td>747.0</td>\n",
       "      <td>13057400.0</td>\n",
       "      <td>1763600.0</td>\n",
       "      <td>2.166473e+06</td>\n",
       "      <td>1081863.0</td>\n",
       "      <td>1.081815e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>196.100</td>\n",
       "      <td>3.50</td>\n",
       "      <td>175.922</td>\n",
       "      <td>6569.8</td>\n",
       "      <td>1377.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>134498.0</td>\n",
       "      <td>99.9435</td>\n",
       "      <td>3.44</td>\n",
       "      <td>...</td>\n",
       "      <td>1.206030</td>\n",
       "      <td>9.230125</td>\n",
       "      <td>-3.218818</td>\n",
       "      <td>9.0</td>\n",
       "      <td>726.0</td>\n",
       "      <td>13110300.0</td>\n",
       "      <td>1782100.0</td>\n",
       "      <td>2.175409e+06</td>\n",
       "      <td>1090390.0</td>\n",
       "      <td>1.090723e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>198.100</td>\n",
       "      <td>4.00</td>\n",
       "      <td>179.675</td>\n",
       "      <td>6654.5</td>\n",
       "      <td>1376.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>134993.0</td>\n",
       "      <td>100.3216</td>\n",
       "      <td>3.88</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.638735</td>\n",
       "      <td>6.844361</td>\n",
       "      <td>8.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>13152700.0</td>\n",
       "      <td>1798800.0</td>\n",
       "      <td>2.175877e+06</td>\n",
       "      <td>1099186.0</td>\n",
       "      <td>1.098971e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>3.7</td>\n",
       "      <td>256.118</td>\n",
       "      <td>2.13</td>\n",
       "      <td>211.802</td>\n",
       "      <td>14938.8</td>\n",
       "      <td>3844.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>151108.0</td>\n",
       "      <td>109.8543</td>\n",
       "      <td>1.95</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.223881</td>\n",
       "      <td>2.935424</td>\n",
       "      <td>-1.687233</td>\n",
       "      <td>13.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>15569800.0</td>\n",
       "      <td>2620100.0</td>\n",
       "      <td>3.998115e+06</td>\n",
       "      <td>1437108.0</td>\n",
       "      <td>1.438242e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>3.6</td>\n",
       "      <td>257.989</td>\n",
       "      <td>1.55</td>\n",
       "      <td>212.222</td>\n",
       "      <td>15254.4</td>\n",
       "      <td>3955.6</td>\n",
       "      <td>7.5</td>\n",
       "      <td>151758.0</td>\n",
       "      <td>110.0388</td>\n",
       "      <td>1.54</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150376</td>\n",
       "      <td>5.135172</td>\n",
       "      <td>-4.344529</td>\n",
       "      <td>11.0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>15624700.0</td>\n",
       "      <td>2625300.0</td>\n",
       "      <td>4.015310e+06</td>\n",
       "      <td>1444387.0</td>\n",
       "      <td>1.443890e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>3.5</td>\n",
       "      <td>258.824</td>\n",
       "      <td>1.58</td>\n",
       "      <td>213.295</td>\n",
       "      <td>15473.4</td>\n",
       "      <td>4027.6</td>\n",
       "      <td>8.3</td>\n",
       "      <td>152523.0</td>\n",
       "      <td>109.2966</td>\n",
       "      <td>1.52</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.452489</td>\n",
       "      <td>4.145019</td>\n",
       "      <td>2.584733</td>\n",
       "      <td>13.0</td>\n",
       "      <td>752.0</td>\n",
       "      <td>15663700.0</td>\n",
       "      <td>2629400.0</td>\n",
       "      <td>4.020187e+06</td>\n",
       "      <td>1450121.0</td>\n",
       "      <td>1.449740e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>13.3</td>\n",
       "      <td>255.942</td>\n",
       "      <td>0.05</td>\n",
       "      <td>218.569</td>\n",
       "      <td>17893.0</td>\n",
       "      <td>16275.9</td>\n",
       "      <td>24.7</td>\n",
       "      <td>132994.0</td>\n",
       "      <td>92.0613</td>\n",
       "      <td>0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152555</td>\n",
       "      <td>58.708814</td>\n",
       "      <td>23.355997</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1953.0</td>\n",
       "      <td>15693800.0</td>\n",
       "      <td>2633500.0</td>\n",
       "      <td>4.032882e+06</td>\n",
       "      <td>1453775.0</td>\n",
       "      <td>1.453747e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>8.4</td>\n",
       "      <td>259.511</td>\n",
       "      <td>0.10</td>\n",
       "      <td>224.139</td>\n",
       "      <td>18381.8</td>\n",
       "      <td>16906.0</td>\n",
       "      <td>14.6</td>\n",
       "      <td>141149.0</td>\n",
       "      <td>102.8885</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076161</td>\n",
       "      <td>22.084257</td>\n",
       "      <td>2.479332</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1073.0</td>\n",
       "      <td>15731700.0</td>\n",
       "      <td>2635800.0</td>\n",
       "      <td>4.053895e+06</td>\n",
       "      <td>1455757.0</td>\n",
       "      <td>1.456060e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 87827 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    UNRATE  CPIAUCSL  FEDFUNDS  CSUSHPINSA     M2SL     M1SL  PSAVERT  \\\n",
       "0      5.4   191.700      1.93     157.497   6399.4   1374.2      4.0   \n",
       "1      5.4   192.400      2.50     161.924   6432.5   1371.1      3.2   \n",
       "2      5.1   193.600      3.00     169.544   6473.1   1366.0      3.5   \n",
       "3      4.9   196.100      3.50     175.922   6569.8   1377.8      2.6   \n",
       "4      5.0   198.100      4.00     179.675   6654.5   1376.1      3.5   \n",
       "..     ...       ...       ...         ...      ...      ...      ...   \n",
       "59     3.7   256.118      2.13     211.802  14938.8   3844.0      7.3   \n",
       "60     3.6   257.989      1.55     212.222  15254.4   3955.6      7.5   \n",
       "61     3.5   258.824      1.58     213.295  15473.4   4027.6      8.3   \n",
       "62    13.3   255.942      0.05     218.569  17893.0  16275.9     24.7   \n",
       "63     8.4   259.511      0.10     224.139  18381.8  16906.0     14.6   \n",
       "\n",
       "      PAYEMS    INDPRO  TB3MS  ...  ESTPIEAMP01GPM  BROSERPA158MFRBDAL  \\\n",
       "0   132503.0   97.6137   2.07  ...       -0.202429           -1.713722   \n",
       "1   133032.0   99.4639   2.54  ...       -0.201410            1.561165   \n",
       "2   133690.0   99.6033   2.84  ...       -0.100604            1.090706   \n",
       "3   134498.0   99.9435   3.44  ...        1.206030            9.230125   \n",
       "4   134993.0  100.3216   3.88  ...        0.000000            2.638735   \n",
       "..       ...       ...    ...  ...             ...                 ...   \n",
       "59  151108.0  109.8543   1.95  ...       -0.223881            2.935424   \n",
       "60  151758.0  110.0388   1.54  ...        0.150376            5.135172   \n",
       "61  152523.0  109.2966   1.52  ...       -0.452489            4.145019   \n",
       "62  132994.0   92.0613   0.13  ...       -0.152555           58.708814   \n",
       "63  141149.0  102.8885   0.10  ...       -0.076161           22.084257   \n",
       "\n",
       "    LARTTULA158MFRBDAL  LAUCN310910000000004  LAUCN271470000000004  \\\n",
       "0             2.689583                  12.0                 712.0   \n",
       "1            16.658304                  25.0                 983.0   \n",
       "2            -4.866917                  11.0                 747.0   \n",
       "3            -3.218818                   9.0                 726.0   \n",
       "4             6.844361                   8.0                 713.0   \n",
       "..                 ...                   ...                   ...   \n",
       "59           -1.687233                  13.0                 642.0   \n",
       "60           -4.344529                  11.0                 505.0   \n",
       "61            2.584733                  13.0                 752.0   \n",
       "62           23.355997                  10.0                1953.0   \n",
       "63            2.479332                   8.0                1073.0   \n",
       "\n",
       "    LFWATTFECAM647N  LFWA55FECAM647S  LFWA55FEKRM647S  LFWA55MAAUM647N  \\\n",
       "0        12971900.0        1725500.0     2.143286e+06        1063251.0   \n",
       "1        13010400.0        1745200.0     2.138804e+06        1072885.0   \n",
       "2        13057400.0        1763600.0     2.166473e+06        1081863.0   \n",
       "3        13110300.0        1782100.0     2.175409e+06        1090390.0   \n",
       "4        13152700.0        1798800.0     2.175877e+06        1099186.0   \n",
       "..              ...              ...              ...              ...   \n",
       "59       15569800.0        2620100.0     3.998115e+06        1437108.0   \n",
       "60       15624700.0        2625300.0     4.015310e+06        1444387.0   \n",
       "61       15663700.0        2629400.0     4.020187e+06        1450121.0   \n",
       "62       15693800.0        2633500.0     4.032882e+06        1453775.0   \n",
       "63       15731700.0        2635800.0     4.053895e+06        1455757.0   \n",
       "\n",
       "    LFWA55MAAUM647S  \n",
       "0      1.062949e+06  \n",
       "1      1.073819e+06  \n",
       "2      1.081815e+06  \n",
       "3      1.090723e+06  \n",
       "4      1.098971e+06  \n",
       "..              ...  \n",
       "59     1.438242e+06  \n",
       "60     1.443890e+06  \n",
       "61     1.449740e+06  \n",
       "62     1.453747e+06  \n",
       "63     1.456060e+06  \n",
       "\n",
       "[64 rows x 87827 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = csv2.dropna(axis=\"columns\")[8::3].reset_index().drop([\"index\", \"Unnamed: 0\"], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "aef69c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>5438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>6056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>5773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>6236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sales\n",
       "0    1308\n",
       "1    1054\n",
       "2    1258\n",
       "3    1362\n",
       "4    1225\n",
       "..    ...\n",
       "60   4797\n",
       "61   5438\n",
       "62   6056\n",
       "63   5773\n",
       "64   6236\n",
       "\n",
       "[65 rows x 1 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.read_csv(\"micron.csv\").drop(\"date\", axis=1)[::-1].reset_index().drop(\"index\", axis=1)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "286567d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = data.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f58a136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = full\n",
    "scaler = MinMaxScaler()\n",
    "full = MinMaxScaler().fit_transform(full)\n",
    "som_x = som_y = math.ceil(math.sqrt(math.sqrt(len(full))))\n",
    "full = np.transpose(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f40a2a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOM clustering executed in 0.0m 5.428215503692627s \n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "som_x = som_y = math.ceil(math.sqrt(math.sqrt(len(full))))\n",
    "# I didn't see its significance but to make the map square,\n",
    "# I calculated square root of map size which is \n",
    "# the square root of the number of series\n",
    "# for the row and column counts of som\n",
    "\n",
    "som = MiniSom(som_x, som_y,len(full[0]), sigma=0.3, learning_rate = 0.1)\n",
    "\n",
    "som.random_weights_init(full)\n",
    "som.train(full, 50000)\n",
    "time_elapsed = time.time() - since\n",
    "print(f\"SOM clustering executed in {time_elapsed // 60}m {time_elapsed % 60}s \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79a65763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA clustering executed in 0.0m 0.3369779586791992s \n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "mySeries_transformed = pca.fit_transform(full)\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print(f\"PCA clustering executed in {time_elapsed // 60}m {time_elapsed % 60}s \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91bde7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n",
      "Kmeans on PCA clustering executed in 0.0m 40.98774480819702s \n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "cluster_count = math.ceil(math.sqrt(len(full))) \n",
    "print(cluster_count)\n",
    "kmeans = KMeans(n_clusters=60,max_iter=5000)\n",
    "\n",
    "labels = kmeans.fit_predict(mySeries_transformed)\n",
    "time_elapsed = time.time() - since\n",
    "print(f\"Kmeans on PCA clustering executed in {time_elapsed // 60}m {time_elapsed % 60}s \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39f32117",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_map = []\n",
    "i = 0\n",
    "cluster_x = []\n",
    "cluster_dict = {}\n",
    "for idx in names:\n",
    "    winner_node = som.winner(full[i])\n",
    "    name = f\"Cluster {winner_node[0] * som_y + winner_node[1] + 1}\"\n",
    "    cluster_map.append((idx, name))\n",
    "\n",
    "    cluster_x.append(name)\n",
    "    i += 1\n",
    "\n",
    "cluster_x = Counter(cluster_x)\n",
    "clusters = pd.DataFrame(cluster_map, columns=[\"Name\", \"Cluster\"]).sort_values(by=\"Cluster\").set_index(\"Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b532b2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SMU15000007072200030SA',\n",
       " 'SMU53426444245200001SA',\n",
       " 'SMU45439004100000001',\n",
       " 'SMU15000007072000030SA',\n",
       " 'SMU53426604245200001SA',\n",
       " 'SMU06125403231100001',\n",
       " 'SMU21000006055000001',\n",
       " 'SMU48000008081300001SA',\n",
       " 'SMU48185800500000001',\n",
       " 'EXP5200',\n",
       " 'SMU09000006054150001SA',\n",
       " 'BISM938TRADN',\n",
       " 'CORP548NAN',\n",
       " 'CES6056140001',\n",
       " 'BISM938NRMN',\n",
       " 'SMS30000009092000001',\n",
       " 'SMU39104204244600001SA',\n",
       " 'TURCSINFT02STSAM',\n",
       " 'SMU25765249000000001SA',\n",
       " 'TOTLLW2024',\n",
       " 'SMU40364205553200001',\n",
       " 'SMU36350044244500001SA',\n",
       " 'SMU36287404100000001SA',\n",
       " 'LINC731TRADN',\n",
       " 'CEDA319TRADN',\n",
       " 'BROW148TRADN',\n",
       " 'BROW148TRAD',\n",
       " 'PITGND01JPM661N',\n",
       " 'CES4142000007',\n",
       " 'SMU36350044244510001SA',\n",
       " 'SMU24000004244510001SA',\n",
       " 'CP0550LTM086NEST',\n",
       " 'CP0730SIM086NEST',\n",
       " 'CPIAPPNS',\n",
       " 'SMU38139004200000001SA',\n",
       " 'SMU41000003231100006SA',\n",
       " 'SMU40000007072100001',\n",
       " 'SMU38220209092000001SA',\n",
       " 'SMU29411803200000030',\n",
       " 'ROCH336PBSVN',\n",
       " 'KALA026FIRE',\n",
       " 'TX80813000M175FRBDAL',\n",
       " 'LALFN',\n",
       " 'CORP548NRMNN',\n",
       " 'IPG3364S',\n",
       " 'SMU42979626561000001SA',\n",
       " 'BISM938PBSVN',\n",
       " 'SMU29411803200000030SA',\n",
       " 'CASP256LEIHN',\n",
       " 'CP0733ESM086NEST',\n",
       " 'EXPMANWV',\n",
       " 'CWUR0000SAA',\n",
       " 'PCU221122221122428',\n",
       " 'SMU17281006562000001SA',\n",
       " 'SMU06125404244800001SA',\n",
       " 'SMS01000006561000001',\n",
       " 'IPG3364N',\n",
       " 'SMU40364201000000001',\n",
       " 'CUURA400SAA',\n",
       " 'SMU12484244245200001SA',\n",
       " 'PCU221122221122438',\n",
       " 'SCNDLW5564',\n",
       " 'SMS29000006056000001',\n",
       " 'SMU54000001000000008',\n",
       " 'SMU54000001000000008SA',\n",
       " 'SMU22291804200000001SA',\n",
       " 'SMU34000006056160001SA',\n",
       " 'LAUCN220490000000005',\n",
       " 'IPG336411T3S',\n",
       " 'SMU15465207072000030SA',\n",
       " 'CORP548TRADN',\n",
       " 'SMU38220204200000001SA',\n",
       " 'VALIMPSAM052N',\n",
       " 'GREE737FIREN',\n",
       " 'SMU31307004200000001',\n",
       " 'SMU38000007072200001',\n",
       " 'CPIEAPPAREL',\n",
       " 'SANA648SRVON',\n",
       " 'ODES248SRVO',\n",
       " 'SMU40364201500000001SA',\n",
       " 'SMU36350046054000001',\n",
       " 'SMU45439004100000001SA',\n",
       " 'PANA412LEIH',\n",
       " 'IV14211',\n",
       " 'SMU38000007072100001SA',\n",
       " 'FARG038SRVON',\n",
       " 'FARG038SRVO',\n",
       " 'LAUCN191410000000005',\n",
       " 'SMU38000007072100001',\n",
       " 'BROTTULA175MFRBDAL',\n",
       " 'SANL006LF',\n",
       " 'SMU48325804245200001',\n",
       " 'SMU38000007072000001',\n",
       " 'WPS0141',\n",
       " 'CASP256LEIH',\n",
       " 'KANK117EDUHN',\n",
       " 'SMU40364201500000001',\n",
       " 'CORP548TRAD',\n",
       " 'ODES248SRVO.1',\n",
       " 'SMU04460606056140001SA',\n",
       " 'LFEAMNTTCLM647S',\n",
       " 'SMU24251800700000001',\n",
       " 'IPG336411T3N',\n",
       " 'IV1421',\n",
       " 'SMU18000009092161101SA',\n",
       " 'PCU486110486110311',\n",
       " 'SMU24251800700000001SA',\n",
       " 'SMU26198204200000007',\n",
       " 'SMU51472606561000001SA',\n",
       " 'NDLEIHN',\n",
       " 'SMU48000004245100001SA',\n",
       " 'SMU19197806056130001SA',\n",
       " 'HONO115LFN',\n",
       " 'EXPNONNY',\n",
       " 'SMU17281006562000001',\n",
       " 'SMU17195004100000001SA',\n",
       " 'SMS20000006056000001',\n",
       " 'SMU48213404245200001',\n",
       " 'SMU29000006056100001SA',\n",
       " 'SMU22000002023700001SA',\n",
       " 'CUUR0400SAA',\n",
       " 'LALF',\n",
       " 'CP0300USM086NEST',\n",
       " 'WPS014',\n",
       " 'SMU36350046054000001SA',\n",
       " 'LASC735FIREN',\n",
       " 'LASC735FIRE',\n",
       " 'SMU48470200700000001SA',\n",
       " 'SMU06112444245200001',\n",
       " 'SMU22291807072200001',\n",
       " 'CWUR0000SETG',\n",
       " 'SMU50000006561130001SA',\n",
       " 'CLMTRADN',\n",
       " 'SMU19163006056000001',\n",
       " 'CES4245200001',\n",
       " 'SMU22000004244600001',\n",
       " 'ODES248SRVON',\n",
       " 'SMU31307004200000001SA',\n",
       " 'KANK117EDUH',\n",
       " 'EXPMANIL',\n",
       " 'SMU15465207072000030',\n",
       " 'SMU12331244245200001SA',\n",
       " 'SMU47000003100000007',\n",
       " 'SMU51000008081300001SA',\n",
       " 'SMU47000003100000007SA',\n",
       " 'USACPHP0300IXOBM',\n",
       " 'HIHONO7LFN',\n",
       " 'SMU36350044244510001',\n",
       " 'SMU37395804245200001',\n",
       " 'SCADLW2534',\n",
       " 'SMU25716509092000001SA',\n",
       " 'USACPHP0300IXEBM',\n",
       " 'CORP548NRMN',\n",
       " 'SMS10000006561000001',\n",
       " 'SMU38220204200000001',\n",
       " 'SMU06125404100000001SA',\n",
       " 'SMU72103807000000001SA',\n",
       " 'BISM938NRMNN',\n",
       " 'SMU26332200800000001',\n",
       " 'LFEAMNTTCLM647N',\n",
       " 'SMU53000004245200001',\n",
       " 'SMU56162200800000001',\n",
       " 'SMU56162200800000001SA',\n",
       " 'SMU48185800500000001SA',\n",
       " 'SMU28000003000000007SA',\n",
       " 'SMU15000007072000030',\n",
       " 'SMU15000007072200030',\n",
       " 'SMU12331244245200001',\n",
       " 'SMU35107409092000001SA',\n",
       " 'CWUR0000SAA1',\n",
       " 'SMU23767506056000001',\n",
       " 'SMU06125404100000001',\n",
       " 'SMU06125404244800001',\n",
       " 'SMU48000004245100001',\n",
       " 'EXPMANWA',\n",
       " 'SMU09000006054150001',\n",
       " 'SMU48325804200000001',\n",
       " 'LNU02026629',\n",
       " 'SMU20000006056000001',\n",
       " 'SMU24000004244500001SA',\n",
       " 'CEDA319TRAD',\n",
       " 'SMU27334604200000007SA',\n",
       " 'SMU40364201000000001SA',\n",
       " 'SMU36935614245200001',\n",
       " 'WPS014102',\n",
       " 'TRESEGTRM194N',\n",
       " 'SMU40364205553200001SA',\n",
       " 'LAUCN480150000000005',\n",
       " 'SMU37000006056140001SA',\n",
       " 'SMU19269800800000001',\n",
       " 'CEU4142000007',\n",
       " 'LAUCN370610000000005',\n",
       " 'SMU51000008081300001',\n",
       " 'SMU36000004244600001',\n",
       " 'SMU72000006562000001',\n",
       " 'SMU22000002023700001',\n",
       " 'SMU02000004245200001']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = clusters.loc[\"sales\"].values\n",
    "value = clusters[\"Cluster\"].values\n",
    "index = list(np.where(value ==cluster)[0])\n",
    "names = list(clusters.index.values[index])\n",
    "names.remove(\"sales\")\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5f4f4d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = {}\n",
    "for name in data:\n",
    "    x = data[name]\n",
    "    \n",
    "    corrs[abs(x.corr(target[\"sales\"]))] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0397054f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "53a499be",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for num in sorted(corrs)[:-40:-1]:\n",
    "    names.append(corrs[num])\n",
    "for num in sorted(corrs)[:10:1]:\n",
    "    names.append(corrs[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4f0cd12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "62411cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = data.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "55a30586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CP0213PTM086NEST</th>\n",
       "      <th>LFEM64MAKRM647N</th>\n",
       "      <th>OWNFIRE</th>\n",
       "      <th>TOPE820PBSVN</th>\n",
       "      <th>SMU15000004322000001</th>\n",
       "      <th>SMU20928126000000001</th>\n",
       "      <th>CP0540EEM086NEST</th>\n",
       "      <th>SMU06310843133450001SA</th>\n",
       "      <th>SMU06310843133450001</th>\n",
       "      <th>SMU44772003000000030</th>\n",
       "      <th>...</th>\n",
       "      <th>HARR442URN</th>\n",
       "      <th>CGBD2564</th>\n",
       "      <th>LAUCN260190000000005</th>\n",
       "      <th>DURH537UR</th>\n",
       "      <th>NYXRPSNSA</th>\n",
       "      <th>PIEATI02CHM661N</th>\n",
       "      <th>ALAUTA1URN</th>\n",
       "      <th>ALBA513URN</th>\n",
       "      <th>JTS1000TSL</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61.03</td>\n",
       "      <td>12686900.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>63.5</td>\n",
       "      <td>69.62</td>\n",
       "      <td>38.949749</td>\n",
       "      <td>39.1</td>\n",
       "      <td>526.32</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>8168.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>17045.0</td>\n",
       "      <td>96.014029</td>\n",
       "      <td>4.4</td>\n",
       "      <td>6.1</td>\n",
       "      <td>4804.0</td>\n",
       "      <td>1308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.38</td>\n",
       "      <td>12417000.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>62.6</td>\n",
       "      <td>70.65</td>\n",
       "      <td>40.001376</td>\n",
       "      <td>40.1</td>\n",
       "      <td>516.85</td>\n",
       "      <td>...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>2.6</td>\n",
       "      <td>7954.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>13847.0</td>\n",
       "      <td>95.860035</td>\n",
       "      <td>4.7</td>\n",
       "      <td>6.4</td>\n",
       "      <td>4747.0</td>\n",
       "      <td>1054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60.48</td>\n",
       "      <td>12733700.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>65.9</td>\n",
       "      <td>70.45</td>\n",
       "      <td>40.052584</td>\n",
       "      <td>39.8</td>\n",
       "      <td>525.15</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>8518.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>14327.0</td>\n",
       "      <td>96.314645</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.9</td>\n",
       "      <td>4808.0</td>\n",
       "      <td>1258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59.93</td>\n",
       "      <td>12576300.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>67.9</td>\n",
       "      <td>70.67</td>\n",
       "      <td>40.071595</td>\n",
       "      <td>40.2</td>\n",
       "      <td>523.72</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>8919.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>20418.0</td>\n",
       "      <td>96.765370</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.9</td>\n",
       "      <td>4893.0</td>\n",
       "      <td>1362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59.94</td>\n",
       "      <td>12716000.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>68.4</td>\n",
       "      <td>70.38</td>\n",
       "      <td>39.980556</td>\n",
       "      <td>40.1</td>\n",
       "      <td>531.95</td>\n",
       "      <td>...</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8417.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>19443.0</td>\n",
       "      <td>97.366404</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.8</td>\n",
       "      <td>4686.0</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>98.61</td>\n",
       "      <td>14101000.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>12.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>90.3</td>\n",
       "      <td>98.89</td>\n",
       "      <td>24.643202</td>\n",
       "      <td>24.9</td>\n",
       "      <td>722.71</td>\n",
       "      <td>...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>2.4</td>\n",
       "      <td>8905.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>20104.0</td>\n",
       "      <td>100.422968</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5352.0</td>\n",
       "      <td>5144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>100.08</td>\n",
       "      <td>14131600.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>12.9</td>\n",
       "      <td>3.7</td>\n",
       "      <td>89.7</td>\n",
       "      <td>98.50</td>\n",
       "      <td>24.879765</td>\n",
       "      <td>24.8</td>\n",
       "      <td>763.60</td>\n",
       "      <td>...</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>8308.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>14871.0</td>\n",
       "      <td>100.320272</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>5300.0</td>\n",
       "      <td>4797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>97.91</td>\n",
       "      <td>13939400.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>12.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>87.5</td>\n",
       "      <td>97.09</td>\n",
       "      <td>25.031696</td>\n",
       "      <td>25.0</td>\n",
       "      <td>714.56</td>\n",
       "      <td>...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>8096.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>11785.0</td>\n",
       "      <td>99.940866</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.1</td>\n",
       "      <td>5366.0</td>\n",
       "      <td>5438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>94.02</td>\n",
       "      <td>13842500.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>11.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>83.5</td>\n",
       "      <td>97.86</td>\n",
       "      <td>25.096434</td>\n",
       "      <td>24.9</td>\n",
       "      <td>720.29</td>\n",
       "      <td>...</td>\n",
       "      <td>12.1</td>\n",
       "      <td>8.3</td>\n",
       "      <td>6941.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>11493.0</td>\n",
       "      <td>99.051668</td>\n",
       "      <td>6.4</td>\n",
       "      <td>8.7</td>\n",
       "      <td>4160.0</td>\n",
       "      <td>6056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>97.01</td>\n",
       "      <td>13876400.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>12.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>87.3</td>\n",
       "      <td>94.97</td>\n",
       "      <td>25.206486</td>\n",
       "      <td>25.5</td>\n",
       "      <td>741.82</td>\n",
       "      <td>...</td>\n",
       "      <td>8.6</td>\n",
       "      <td>6.3</td>\n",
       "      <td>8470.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>18996.0</td>\n",
       "      <td>99.499106</td>\n",
       "      <td>5.8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4504.0</td>\n",
       "      <td>5773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    CP0213PTM086NEST  LFEM64MAKRM647N  OWNFIRE  TOPE820PBSVN  \\\n",
       "0              61.03       12686900.0      2.3           8.0   \n",
       "1              61.38       12417000.0      2.3           8.1   \n",
       "2              60.48       12733700.0      2.3           8.1   \n",
       "3              59.93       12576300.0      2.3           8.4   \n",
       "4              59.94       12716000.0      2.3           8.3   \n",
       "..               ...              ...      ...           ...   \n",
       "59             98.61       14101000.0      3.5          12.9   \n",
       "60            100.08       14131600.0      3.5          12.9   \n",
       "61             97.91       13939400.0      3.5          12.4   \n",
       "62             94.02       13842500.0      3.3          11.4   \n",
       "63             97.01       13876400.0      3.4          12.4   \n",
       "\n",
       "    SMU15000004322000001  SMU20928126000000001  CP0540EEM086NEST  \\\n",
       "0                    2.8                  63.5             69.62   \n",
       "1                    2.8                  62.6             70.65   \n",
       "2                    2.9                  65.9             70.45   \n",
       "3                    2.9                  67.9             70.67   \n",
       "4                    2.9                  68.4             70.38   \n",
       "..                   ...                   ...               ...   \n",
       "59                   3.8                  90.3             98.89   \n",
       "60                   3.7                  89.7             98.50   \n",
       "61                   3.7                  87.5             97.09   \n",
       "62                   3.7                  83.5             97.86   \n",
       "63                   3.7                  87.3             94.97   \n",
       "\n",
       "    SMU06310843133450001SA  SMU06310843133450001  SMU44772003000000030  ...  \\\n",
       "0                38.949749                  39.1                526.32  ...   \n",
       "1                40.001376                  40.1                516.85  ...   \n",
       "2                40.052584                  39.8                525.15  ...   \n",
       "3                40.071595                  40.2                523.72  ...   \n",
       "4                39.980556                  40.1                531.95  ...   \n",
       "..                     ...                   ...                   ...  ...   \n",
       "59               24.643202                  24.9                722.71  ...   \n",
       "60               24.879765                  24.8                763.60  ...   \n",
       "61               25.031696                  25.0                714.56  ...   \n",
       "62               25.096434                  24.9                720.29  ...   \n",
       "63               25.206486                  25.5                741.82  ...   \n",
       "\n",
       "    HARR442URN  CGBD2564  LAUCN260190000000005  DURH537UR  NYXRPSNSA  \\\n",
       "0          4.0       2.7                8168.0        4.2    17045.0   \n",
       "1          4.6       2.6                7954.0        4.4    13847.0   \n",
       "2          4.0       2.6                8518.0        4.4    14327.0   \n",
       "3          4.0       2.4                8919.0        4.3    20418.0   \n",
       "4          3.9       2.5                8417.0        4.1    19443.0   \n",
       "..         ...       ...                   ...        ...        ...   \n",
       "59         4.2       2.4                8905.0        3.4    20104.0   \n",
       "60         3.6       1.8                8308.0        3.2    14871.0   \n",
       "61         4.2       2.1                8096.0        3.2    11785.0   \n",
       "62        12.1       8.3                6941.0       10.8    11493.0   \n",
       "63         8.6       6.3                8470.0        5.7    18996.0   \n",
       "\n",
       "    PIEATI02CHM661N  ALAUTA1URN  ALBA513URN  JTS1000TSL  sales  \n",
       "0         96.014029         4.4         6.1      4804.0   1308  \n",
       "1         95.860035         4.7         6.4      4747.0   1054  \n",
       "2         96.314645         3.3         5.9      4808.0   1258  \n",
       "3         96.765370         3.8         5.9      4893.0   1362  \n",
       "4         97.366404         3.4         5.8      4686.0   1225  \n",
       "..              ...         ...         ...         ...    ...  \n",
       "59       100.422968         2.8         4.5      5352.0   5144  \n",
       "60       100.320272         2.2         3.7      5300.0   4797  \n",
       "61        99.940866         2.6         4.1      5366.0   5438  \n",
       "62        99.051668         6.4         8.7      4160.0   6056  \n",
       "63        99.499106         5.8         8.0      4504.0   5773  \n",
       "\n",
       "[64 rows x 50 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bc580370",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = full.iloc[:,:-1],full.iloc[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6a7620c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9f1c00dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6c43a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3,learning_rate = 0.2,\n",
    "                max_depth = 10, alpha = 5,n_estimators = 300, tree_method=\"gpu_hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "48089956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:20:02] WARNING: ../src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(alpha=5, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.2, max_delta_step=0, max_depth=10,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=300, n_jobs=8, num_parallel_tree=1,\n",
       "             objective='reg:linear', random_state=0, reg_alpha=5, reg_lambda=1,\n",
       "             scale_pos_weight=1, subsample=1, tree_method='gpu_hist',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_reg.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0f3b69ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xg_reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0591da6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 858.222774\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "584a998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {}\n",
    "for name in names:\n",
    "    d = fred.get_series(name, observation_start=\"2/1/2021\", observation_end = \"2/1/2021\")\n",
    "    values[name] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "58c156de",
   "metadata": {},
   "outputs": [],
   "source": [
    "values[\"sales\"] = 6236\n",
    "test = pd.DataFrame(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c1d5b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.from_numpy(test.to_numpy())\n",
    "test_data2 = torch.from_numpy(full[:-3:-1].to_numpy())\n",
    "test_data = torch.cat([test_data2, test_data]).unsqueeze(0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4752e815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7341.5322]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model.load_state_dict(torch.load(\"model.pt\"))\n",
    "model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2918052a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2555.3232], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_reg.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d5189e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "     \n",
    "\n",
    "        self.output_block = nn.Sequential(nn.Conv1d(3, 16, 2, 1, 0), nn.ReLU(), \n",
    "                                          nn.Conv1d(16, 32, 2, 1, 0), nn.ReLU(), \n",
    "                                          nn.Conv1d(32, 32, 2, 1, 0), nn.ReLU(), \n",
    "                                          nn.Conv1d(32, 16, 2, 1, 0), nn.ReLU())\n",
    "        self.fc = nn.Sequential(nn.LazyLinear(512), nn.ReLU(), nn.Linear(512, 256), nn.ReLU(), nn.Linear(256, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        net = self.output_block(x)\n",
    "        net = net.view(net.shape[0], -1)\n",
    "        net = self.fc(net)\n",
    "        \n",
    "        return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a396928d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>5438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>6056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>5773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>6236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sales\n",
       "3    1362\n",
       "4    1225\n",
       "5    1312\n",
       "6    1373\n",
       "7    1530\n",
       "..    ...\n",
       "60   4797\n",
       "61   5438\n",
       "62   6056\n",
       "63   5773\n",
       "64   6236\n",
       "\n",
       "[62 rows x 1 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "16e1a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, df, target):\n",
    "        data = df[1:].reset_index(drop=True)\n",
    "        sales = target[:-1].reset_index(drop=True)\n",
    "        self.data = data.join(sales)\n",
    "        self.target = target[3:].to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)-3\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data.iloc[idx:idx+3].to_numpy()\n",
    "        target = self.target[idx]\n",
    "        return torch.tensor(data).float(), torch.tensor(target).float()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "32c5f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = int(len(data) * 0.8)\n",
    "train, test = data[:idx], data[idx:]\n",
    "traint, testt = target[:idx], target[idx:]\n",
    "trainset = dataset(train, traint)\n",
    "testset = dataset(test, testt)\n",
    "train_loader = DataLoader(trainset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5d1bd6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwight/miniconda3/lib/python3.8/site-packages/torch/nn/modules/lazy.py:175: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=25, factor=0.5, min_lr=0.000001)\n",
    "def Train(epochs, model, train_loader):\n",
    "    valid_loss_min = np.Inf\n",
    "    min_rmse = np.Inf\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        valid_loss = 0.0\n",
    "        rmse = 0.0\n",
    "        model.train()\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "       \n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            output = model(images)\n",
    "        \n",
    "            loss = torch.sqrt(criterion(output,labels))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "        model.eval()\n",
    "        for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "       \n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            output = model(images)\n",
    "        \n",
    "            loss = torch.sqrt(criterion(output,labels))\n",
    "          \n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "                \n",
    "        scheduler.step(valid_loss)\n",
    "        print(f\"Train Loss: {train_loss.item()}\")\n",
    "        print(f\"Valid Loss: {valid_loss.item()}\")\n",
    "        if valid_loss.item() < valid_loss_min:\n",
    "            valid_loss_min = valid_loss.item()\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "    print(f\"Min RMSE: {valid_loss_min}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "970d5267",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 14404.99609375\n",
      "Valid Loss: 4604.17529296875\n",
      "Train Loss: 1931.1536865234375\n",
      "Valid Loss: 2623.03515625\n",
      "Train Loss: 1213.804443359375\n",
      "Valid Loss: 4203.15966796875\n",
      "Train Loss: 1327.091064453125\n",
      "Valid Loss: 3934.09423828125\n",
      "Train Loss: 1189.7322998046875\n",
      "Valid Loss: 3725.74951171875\n",
      "Train Loss: 1111.98681640625\n",
      "Valid Loss: 3329.759033203125\n",
      "Train Loss: 1074.5225830078125\n",
      "Valid Loss: 4327.03271484375\n",
      "Train Loss: 1113.3994140625\n",
      "Valid Loss: 4136.115234375\n",
      "Train Loss: 1087.845458984375\n",
      "Valid Loss: 2974.157470703125\n",
      "Train Loss: 1104.0758056640625\n",
      "Valid Loss: 3163.33154296875\n",
      "Train Loss: 1194.8238525390625\n",
      "Valid Loss: 4032.378662109375\n",
      "Train Loss: 1252.1951904296875\n",
      "Valid Loss: 3607.66162109375\n",
      "Train Loss: 1076.4114990234375\n",
      "Valid Loss: 4027.7314453125\n",
      "Train Loss: 1123.351318359375\n",
      "Valid Loss: 4257.75341796875\n",
      "Train Loss: 1089.855224609375\n",
      "Valid Loss: 2853.810546875\n",
      "Train Loss: 1039.6171875\n",
      "Valid Loss: 3525.20068359375\n",
      "Train Loss: 1085.1416015625\n",
      "Valid Loss: 2953.9921875\n",
      "Train Loss: 1081.553955078125\n",
      "Valid Loss: 3471.840087890625\n",
      "Train Loss: 962.5948486328125\n",
      "Valid Loss: 2884.87939453125\n",
      "Train Loss: 951.81103515625\n",
      "Valid Loss: 1113.4376220703125\n",
      "Train Loss: 1089.2906494140625\n",
      "Valid Loss: 2921.45849609375\n",
      "Train Loss: 763.8007202148438\n",
      "Valid Loss: 2143.8095703125\n",
      "Train Loss: 567.456298828125\n",
      "Valid Loss: 1465.2852783203125\n",
      "Train Loss: 572.44921875\n",
      "Valid Loss: 1525.2398681640625\n",
      "Train Loss: 565.4434204101562\n",
      "Valid Loss: 1673.955078125\n",
      "Train Loss: 558.9560546875\n",
      "Valid Loss: 1072.409912109375\n",
      "Train Loss: 600.5670166015625\n",
      "Valid Loss: 1285.2205810546875\n",
      "Train Loss: 449.0997314453125\n",
      "Valid Loss: 942.1262817382812\n",
      "Train Loss: 534.27783203125\n",
      "Valid Loss: 855.7393188476562\n",
      "Train Loss: 565.25439453125\n",
      "Valid Loss: 1263.4075927734375\n",
      "Train Loss: 401.37744140625\n",
      "Valid Loss: 1418.224853515625\n",
      "Train Loss: 454.4137268066406\n",
      "Valid Loss: 1940.3875732421875\n",
      "Train Loss: 706.3564453125\n",
      "Valid Loss: 846.206298828125\n",
      "Train Loss: 470.16583251953125\n",
      "Valid Loss: 664.8031005859375\n",
      "Train Loss: 427.8264465332031\n",
      "Valid Loss: 1208.612060546875\n",
      "Train Loss: 407.3790283203125\n",
      "Valid Loss: 1116.1395263671875\n",
      "Train Loss: 359.964599609375\n",
      "Valid Loss: 1228.0206298828125\n",
      "Train Loss: 411.55291748046875\n",
      "Valid Loss: 1252.5782470703125\n",
      "Train Loss: 360.99993896484375\n",
      "Valid Loss: 1302.4962158203125\n",
      "Train Loss: 341.67315673828125\n",
      "Valid Loss: 1027.4793701171875\n",
      "Train Loss: 306.05694580078125\n",
      "Valid Loss: 826.7344360351562\n",
      "Train Loss: 397.6947021484375\n",
      "Valid Loss: 958.3116455078125\n",
      "Train Loss: 343.15234375\n",
      "Valid Loss: 881.7615356445312\n",
      "Train Loss: 296.5650634765625\n",
      "Valid Loss: 775.896728515625\n",
      "Train Loss: 354.52294921875\n",
      "Valid Loss: 760.71044921875\n",
      "Train Loss: 366.850341796875\n",
      "Valid Loss: 907.85205078125\n",
      "Train Loss: 400.44525146484375\n",
      "Valid Loss: 738.9838256835938\n",
      "Train Loss: 274.7671813964844\n",
      "Valid Loss: 777.4782104492188\n",
      "Train Loss: 271.76318359375\n",
      "Valid Loss: 1013.6599731445312\n",
      "Train Loss: 292.4965515136719\n",
      "Valid Loss: 797.1614379882812\n",
      "Train Loss: 274.8191223144531\n",
      "Valid Loss: 782.2837524414062\n",
      "Train Loss: 311.5383605957031\n",
      "Valid Loss: 889.8604125976562\n",
      "Train Loss: 301.144775390625\n",
      "Valid Loss: 1298.4718017578125\n",
      "Train Loss: 454.1156311035156\n",
      "Valid Loss: 618.4739990234375\n",
      "Train Loss: 395.000732421875\n",
      "Valid Loss: 977.404052734375\n",
      "Train Loss: 381.53662109375\n",
      "Valid Loss: 1529.707763671875\n",
      "Train Loss: 324.8725891113281\n",
      "Valid Loss: 547.82568359375\n",
      "Train Loss: 243.90452575683594\n",
      "Valid Loss: 529.8704833984375\n",
      "Train Loss: 221.7603302001953\n",
      "Valid Loss: 636.3123168945312\n",
      "Train Loss: 255.4777069091797\n",
      "Valid Loss: 579.9937133789062\n",
      "Train Loss: 278.7558288574219\n",
      "Valid Loss: 539.6141357421875\n",
      "Train Loss: 346.640869140625\n",
      "Valid Loss: 526.4465942382812\n",
      "Train Loss: 380.9465637207031\n",
      "Valid Loss: 750.4305419921875\n",
      "Train Loss: 410.12890625\n",
      "Valid Loss: 1284.2166748046875\n",
      "Train Loss: 425.8446350097656\n",
      "Valid Loss: 583.3329467773438\n",
      "Train Loss: 384.2947082519531\n",
      "Valid Loss: 777.9183349609375\n",
      "Train Loss: 347.19244384765625\n",
      "Valid Loss: 982.4415283203125\n",
      "Train Loss: 299.2403564453125\n",
      "Valid Loss: 626.0987548828125\n",
      "Train Loss: 276.58563232421875\n",
      "Valid Loss: 903.535888671875\n",
      "Train Loss: 347.58038330078125\n",
      "Valid Loss: 749.6286010742188\n",
      "Train Loss: 344.3677062988281\n",
      "Valid Loss: 562.37353515625\n",
      "Train Loss: 392.72369384765625\n",
      "Valid Loss: 722.6043090820312\n",
      "Train Loss: 302.7926025390625\n",
      "Valid Loss: 1446.3031005859375\n",
      "Train Loss: 490.7474060058594\n",
      "Valid Loss: 647.86669921875\n",
      "Train Loss: 355.767578125\n",
      "Valid Loss: 565.2454223632812\n",
      "Train Loss: 219.43167114257812\n",
      "Valid Loss: 764.8258666992188\n",
      "Train Loss: 274.45489501953125\n",
      "Valid Loss: 1018.9456176757812\n",
      "Train Loss: 227.9122772216797\n",
      "Valid Loss: 800.4444580078125\n",
      "Train Loss: 255.07301330566406\n",
      "Valid Loss: 777.6397094726562\n",
      "Train Loss: 216.59503173828125\n",
      "Valid Loss: 496.72430419921875\n",
      "Train Loss: 275.0697326660156\n",
      "Valid Loss: 518.7025146484375\n",
      "Train Loss: 261.9462890625\n",
      "Valid Loss: 728.48193359375\n",
      "Train Loss: 232.1138458251953\n",
      "Valid Loss: 546.3510131835938\n",
      "Train Loss: 219.89569091796875\n",
      "Valid Loss: 715.175537109375\n",
      "Train Loss: 197.98281860351562\n",
      "Valid Loss: 605.50927734375\n",
      "Train Loss: 188.46994018554688\n",
      "Valid Loss: 880.3472290039062\n",
      "Train Loss: 202.2570343017578\n",
      "Valid Loss: 519.5886840820312\n",
      "Train Loss: 179.16357421875\n",
      "Valid Loss: 761.88330078125\n",
      "Train Loss: 225.86575317382812\n",
      "Valid Loss: 557.1363525390625\n",
      "Train Loss: 228.49417114257812\n",
      "Valid Loss: 602.552978515625\n",
      "Train Loss: 285.44390869140625\n",
      "Valid Loss: 790.0852661132812\n",
      "Train Loss: 291.4398193359375\n",
      "Valid Loss: 697.4993286132812\n",
      "Train Loss: 238.62115478515625\n",
      "Valid Loss: 700.3389282226562\n",
      "Train Loss: 216.4473876953125\n",
      "Valid Loss: 704.74267578125\n",
      "Train Loss: 198.4469451904297\n",
      "Valid Loss: 671.4212036132812\n",
      "Train Loss: 192.16915893554688\n",
      "Valid Loss: 538.970703125\n",
      "Train Loss: 218.22080993652344\n",
      "Valid Loss: 721.75146484375\n",
      "Train Loss: 225.37863159179688\n",
      "Valid Loss: 532.8844604492188\n",
      "Train Loss: 228.71380615234375\n",
      "Valid Loss: 539.8760375976562\n",
      "Train Loss: 257.3968811035156\n",
      "Valid Loss: 886.8206787109375\n",
      "Train Loss: 356.21142578125\n",
      "Valid Loss: 1112.8685302734375\n",
      "Train Loss: 325.7561950683594\n",
      "Valid Loss: 974.8469848632812\n",
      "Train Loss: 382.77081298828125\n",
      "Valid Loss: 466.5041809082031\n",
      "Train Loss: 309.2948303222656\n",
      "Valid Loss: 474.0333557128906\n",
      "Train Loss: 224.9783477783203\n",
      "Valid Loss: 791.8767700195312\n",
      "Train Loss: 193.3523712158203\n",
      "Valid Loss: 594.0856323242188\n",
      "Train Loss: 188.67962646484375\n",
      "Valid Loss: 663.2849731445312\n",
      "Train Loss: 208.27511596679688\n",
      "Valid Loss: 692.8844604492188\n",
      "Train Loss: 159.69284057617188\n",
      "Valid Loss: 684.7188110351562\n",
      "Train Loss: 204.3175048828125\n",
      "Valid Loss: 833.987548828125\n",
      "Train Loss: 224.4989471435547\n",
      "Valid Loss: 688.4297485351562\n",
      "Train Loss: 169.69061279296875\n",
      "Valid Loss: 466.79010009765625\n",
      "Train Loss: 198.72543334960938\n",
      "Valid Loss: 416.1861572265625\n",
      "Train Loss: 167.06178283691406\n",
      "Valid Loss: 808.0062866210938\n",
      "Train Loss: 212.65414428710938\n",
      "Valid Loss: 545.3985595703125\n",
      "Train Loss: 204.22201538085938\n",
      "Valid Loss: 583.4222412109375\n",
      "Train Loss: 203.66864013671875\n",
      "Valid Loss: 650.0513305664062\n",
      "Train Loss: 181.6124725341797\n",
      "Valid Loss: 487.7135314941406\n",
      "Train Loss: 178.4668426513672\n",
      "Valid Loss: 807.4046020507812\n",
      "Train Loss: 196.20547485351562\n",
      "Valid Loss: 524.9339599609375\n",
      "Train Loss: 163.91928100585938\n",
      "Valid Loss: 438.8683166503906\n",
      "Train Loss: 200.40284729003906\n",
      "Valid Loss: 478.7235412597656\n",
      "Train Loss: 165.62313842773438\n",
      "Valid Loss: 561.7477416992188\n",
      "Train Loss: 141.78604125976562\n",
      "Valid Loss: 418.85198974609375\n",
      "Train Loss: 147.88412475585938\n",
      "Valid Loss: 471.24786376953125\n",
      "Train Loss: 173.33840942382812\n",
      "Valid Loss: 476.5140686035156\n",
      "Train Loss: 179.8236083984375\n",
      "Valid Loss: 790.5471801757812\n",
      "Train Loss: 240.72674560546875\n",
      "Valid Loss: 813.641357421875\n",
      "Train Loss: 198.2362060546875\n",
      "Valid Loss: 802.4457397460938\n",
      "Train Loss: 277.5198974609375\n",
      "Valid Loss: 717.09423828125\n",
      "Train Loss: 294.5080261230469\n",
      "Valid Loss: 650.541015625\n",
      "Train Loss: 304.26214599609375\n",
      "Valid Loss: 728.7838745117188\n",
      "Train Loss: 297.48297119140625\n",
      "Valid Loss: 1131.1531982421875\n",
      "Train Loss: 338.45770263671875\n",
      "Valid Loss: 467.8173828125\n",
      "Train Loss: 274.2314758300781\n",
      "Valid Loss: 666.5407104492188\n",
      "Train Loss: 214.91726684570312\n",
      "Valid Loss: 504.6141357421875\n",
      "Train Loss: 163.08627319335938\n",
      "Valid Loss: 624.9144897460938\n",
      "Train Loss: 206.22906494140625\n",
      "Valid Loss: 623.2352294921875\n",
      "Train Loss: 168.07217407226562\n",
      "Valid Loss: 563.9282836914062\n",
      "Train Loss: 156.27487182617188\n",
      "Valid Loss: 477.67523193359375\n",
      "Train Loss: 143.8513946533203\n",
      "Valid Loss: 508.61181640625\n",
      "Train Loss: 149.46107482910156\n",
      "Valid Loss: 617.1310424804688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 175.91445922851562\n",
      "Valid Loss: 441.12188720703125\n",
      "Train Loss: 124.5052490234375\n",
      "Valid Loss: 539.479736328125\n",
      "Train Loss: 151.33226013183594\n",
      "Valid Loss: 546.3299560546875\n",
      "Train Loss: 133.8482666015625\n",
      "Valid Loss: 512.1544189453125\n",
      "Train Loss: 149.49330139160156\n",
      "Valid Loss: 499.51953125\n",
      "Train Loss: 121.60563659667969\n",
      "Valid Loss: 561.2553100585938\n",
      "Train Loss: 130.08078002929688\n",
      "Valid Loss: 729.8281860351562\n",
      "Train Loss: 140.4468536376953\n",
      "Valid Loss: 539.4808349609375\n",
      "Train Loss: 176.1941375732422\n",
      "Valid Loss: 493.353271484375\n",
      "Train Loss: 202.536376953125\n",
      "Valid Loss: 849.2951049804688\n",
      "Train Loss: 167.90541076660156\n",
      "Valid Loss: 530.0210571289062\n",
      "Train Loss: 139.14732360839844\n",
      "Valid Loss: 749.4083251953125\n",
      "Train Loss: 145.4086456298828\n",
      "Valid Loss: 780.8353881835938\n",
      "Train Loss: 166.39984130859375\n",
      "Valid Loss: 523.5712280273438\n",
      "Train Loss: 150.1130828857422\n",
      "Valid Loss: 446.1455078125\n",
      "Train Loss: 111.23774719238281\n",
      "Valid Loss: 510.6570129394531\n",
      "Train Loss: 134.48744201660156\n",
      "Valid Loss: 657.9414672851562\n",
      "Train Loss: 146.6201934814453\n",
      "Valid Loss: 490.12945556640625\n",
      "Train Loss: 136.0121612548828\n",
      "Valid Loss: 462.0960693359375\n",
      "Train Loss: 151.76148986816406\n",
      "Valid Loss: 549.5185546875\n",
      "Train Loss: 151.55006408691406\n",
      "Valid Loss: 702.4996337890625\n",
      "Train Loss: 134.2093963623047\n",
      "Valid Loss: 457.36822509765625\n",
      "Train Loss: 160.506103515625\n",
      "Valid Loss: 479.7835693359375\n",
      "Train Loss: 149.7432403564453\n",
      "Valid Loss: 426.3053283691406\n",
      "Train Loss: 138.05157470703125\n",
      "Valid Loss: 494.5667724609375\n",
      "Train Loss: 105.04010009765625\n",
      "Valid Loss: 500.06280517578125\n",
      "Train Loss: 109.32552337646484\n",
      "Valid Loss: 491.8959045410156\n",
      "Train Loss: 109.39864349365234\n",
      "Valid Loss: 511.292724609375\n",
      "Train Loss: 108.59440612792969\n",
      "Valid Loss: 570.6769409179688\n",
      "Train Loss: 119.85919189453125\n",
      "Valid Loss: 684.5275268554688\n",
      "Train Loss: 112.8760986328125\n",
      "Valid Loss: 563.4827270507812\n",
      "Train Loss: 101.50334167480469\n",
      "Valid Loss: 582.7233276367188\n",
      "Train Loss: 98.93050384521484\n",
      "Valid Loss: 507.46612548828125\n",
      "Train Loss: 104.1952896118164\n",
      "Valid Loss: 551.4000854492188\n",
      "Train Loss: 100.12620544433594\n",
      "Valid Loss: 431.642333984375\n",
      "Train Loss: 96.69194793701172\n",
      "Valid Loss: 718.132568359375\n",
      "Train Loss: 105.22891235351562\n",
      "Valid Loss: 697.0064697265625\n",
      "Train Loss: 96.16996002197266\n",
      "Valid Loss: 506.98858642578125\n",
      "Train Loss: 103.9965591430664\n",
      "Valid Loss: 553.0260620117188\n",
      "Train Loss: 93.20706176757812\n",
      "Valid Loss: 755.3389892578125\n",
      "Train Loss: 104.23744201660156\n",
      "Valid Loss: 558.7891235351562\n",
      "Train Loss: 94.74041748046875\n",
      "Valid Loss: 457.4356689453125\n",
      "Train Loss: 94.28387451171875\n",
      "Valid Loss: 435.9007568359375\n",
      "Train Loss: 100.46324920654297\n",
      "Valid Loss: 439.527587890625\n",
      "Train Loss: 96.47023010253906\n",
      "Valid Loss: 566.0859985351562\n",
      "Train Loss: 119.47047424316406\n",
      "Valid Loss: 547.8208618164062\n",
      "Train Loss: 117.82398986816406\n",
      "Valid Loss: 512.6484375\n",
      "Train Loss: 106.17125701904297\n",
      "Valid Loss: 715.3548583984375\n",
      "Train Loss: 118.03143310546875\n",
      "Valid Loss: 472.9173889160156\n",
      "Train Loss: 91.51481628417969\n",
      "Valid Loss: 599.1435546875\n",
      "Train Loss: 101.84803009033203\n",
      "Valid Loss: 695.81640625\n",
      "Train Loss: 111.4513168334961\n",
      "Valid Loss: 545.5689086914062\n",
      "Train Loss: 95.72748565673828\n",
      "Valid Loss: 546.4205932617188\n",
      "Train Loss: 92.5513916015625\n",
      "Valid Loss: 554.1572875976562\n",
      "Train Loss: 104.22115325927734\n",
      "Valid Loss: 553.521484375\n",
      "Train Loss: 95.11629486083984\n",
      "Valid Loss: 559.3661499023438\n",
      "Train Loss: 91.2643051147461\n",
      "Valid Loss: 461.875244140625\n",
      "Train Loss: 97.91307830810547\n",
      "Valid Loss: 554.2489624023438\n",
      "Train Loss: 88.88037109375\n",
      "Valid Loss: 592.7116088867188\n",
      "Train Loss: 92.15523529052734\n",
      "Valid Loss: 422.7795715332031\n",
      "Train Loss: 85.8772964477539\n",
      "Valid Loss: 738.9528198242188\n",
      "Train Loss: 87.09971618652344\n",
      "Valid Loss: 723.453125\n",
      "Train Loss: 90.53118896484375\n",
      "Valid Loss: 721.7003784179688\n",
      "Train Loss: 90.63072967529297\n",
      "Valid Loss: 584.8991088867188\n",
      "Train Loss: 93.79308319091797\n",
      "Valid Loss: 530.8900756835938\n",
      "Train Loss: 85.18782043457031\n",
      "Valid Loss: 523.5755004882812\n",
      "Train Loss: 100.53008270263672\n",
      "Valid Loss: 492.903564453125\n",
      "Train Loss: 92.61363220214844\n",
      "Valid Loss: 516.0799560546875\n",
      "Train Loss: 87.58546447753906\n",
      "Valid Loss: 534.9082641601562\n",
      "Train Loss: 83.27498626708984\n",
      "Valid Loss: 568.8627319335938\n",
      "Train Loss: 81.9468994140625\n",
      "Valid Loss: 546.0546264648438\n",
      "Train Loss: 86.1605453491211\n",
      "Valid Loss: 464.50030517578125\n",
      "Train Loss: 83.54625701904297\n",
      "Valid Loss: 585.7660522460938\n",
      "Train Loss: 88.60824584960938\n",
      "Valid Loss: 486.2267150878906\n",
      "Train Loss: 85.78349304199219\n",
      "Valid Loss: 559.7612915039062\n",
      "Train Loss: 78.2777328491211\n",
      "Valid Loss: 567.7457885742188\n",
      "Train Loss: 79.48101806640625\n",
      "Valid Loss: 506.4813537597656\n",
      "Train Loss: 82.52101135253906\n",
      "Valid Loss: 724.4382934570312\n",
      "Train Loss: 77.59978485107422\n",
      "Valid Loss: 577.4239501953125\n",
      "Train Loss: 77.49989318847656\n",
      "Valid Loss: 570.8772583007812\n",
      "Train Loss: 81.54456329345703\n",
      "Valid Loss: 563.152587890625\n",
      "Train Loss: 78.77911376953125\n",
      "Valid Loss: 601.4634399414062\n",
      "Train Loss: 78.46141052246094\n",
      "Valid Loss: 551.6362915039062\n",
      "Train Loss: 76.3203353881836\n",
      "Valid Loss: 558.255126953125\n",
      "Train Loss: 78.66806030273438\n",
      "Valid Loss: 425.35406494140625\n",
      "Train Loss: 81.67298126220703\n",
      "Valid Loss: 553.086669921875\n",
      "Train Loss: 77.06034088134766\n",
      "Valid Loss: 712.7859497070312\n",
      "Train Loss: 78.43598937988281\n",
      "Valid Loss: 739.517822265625\n",
      "Train Loss: 76.95159149169922\n",
      "Valid Loss: 727.613037109375\n",
      "Train Loss: 78.5519790649414\n",
      "Valid Loss: 731.7136840820312\n",
      "Train Loss: 82.1967544555664\n",
      "Valid Loss: 727.6053466796875\n",
      "Train Loss: 76.3625717163086\n",
      "Valid Loss: 534.4676513671875\n",
      "Train Loss: 79.04547119140625\n",
      "Valid Loss: 730.329345703125\n",
      "Train Loss: 75.75950622558594\n",
      "Valid Loss: 535.7052612304688\n",
      "Train Loss: 77.93766021728516\n",
      "Valid Loss: 727.3551635742188\n",
      "Train Loss: 77.41929626464844\n",
      "Valid Loss: 563.2836303710938\n",
      "Train Loss: 86.66381072998047\n",
      "Valid Loss: 557.310546875\n",
      "Train Loss: 80.86309814453125\n",
      "Valid Loss: 556.22998046875\n",
      "Train Loss: 77.3035659790039\n",
      "Valid Loss: 522.8067016601562\n",
      "Train Loss: 78.26515197753906\n",
      "Valid Loss: 453.90167236328125\n",
      "Train Loss: 76.09123229980469\n",
      "Valid Loss: 567.830810546875\n",
      "Train Loss: 77.32728576660156\n",
      "Valid Loss: 572.052490234375\n",
      "Train Loss: 73.96481323242188\n",
      "Valid Loss: 453.50531005859375\n",
      "Train Loss: 74.43760681152344\n",
      "Valid Loss: 717.1387329101562\n",
      "Train Loss: 74.35679626464844\n",
      "Valid Loss: 525.046142578125\n",
      "Train Loss: 75.57794952392578\n",
      "Valid Loss: 724.2982788085938\n",
      "Train Loss: 74.63736724853516\n",
      "Valid Loss: 564.4364013671875\n",
      "Train Loss: 73.65083312988281\n",
      "Valid Loss: 538.0521240234375\n",
      "Train Loss: 76.23304748535156\n",
      "Valid Loss: 566.1107788085938\n",
      "Train Loss: 75.19445037841797\n",
      "Valid Loss: 732.4458618164062\n",
      "Train Loss: 76.47599792480469\n",
      "Valid Loss: 542.61572265625\n",
      "Train Loss: 74.42124938964844\n",
      "Valid Loss: 531.3565063476562\n",
      "Train Loss: 75.77238464355469\n",
      "Valid Loss: 724.3963012695312\n",
      "Train Loss: 75.40274047851562\n",
      "Valid Loss: 430.77752685546875\n",
      "Train Loss: 74.77742767333984\n",
      "Valid Loss: 726.4066162109375\n",
      "Train Loss: 73.06611633300781\n",
      "Valid Loss: 595.6716918945312\n",
      "Train Loss: 74.8181381225586\n",
      "Valid Loss: 599.52099609375\n",
      "Train Loss: 70.1612548828125\n",
      "Valid Loss: 729.247802734375\n",
      "Train Loss: 74.07097625732422\n",
      "Valid Loss: 541.4559326171875\n",
      "Train Loss: 73.84735107421875\n",
      "Valid Loss: 566.15283203125\n",
      "Train Loss: 74.31550598144531\n",
      "Valid Loss: 563.5342407226562\n",
      "Train Loss: 72.27103424072266\n",
      "Valid Loss: 737.6375122070312\n",
      "Train Loss: 71.90638732910156\n",
      "Valid Loss: 731.0589599609375\n",
      "Train Loss: 73.88545227050781\n",
      "Valid Loss: 561.0715942382812\n",
      "Train Loss: 72.60826873779297\n",
      "Valid Loss: 723.17333984375\n",
      "Train Loss: 74.31205749511719\n",
      "Valid Loss: 572.5573120117188\n",
      "Train Loss: 74.44155883789062\n",
      "Valid Loss: 605.4671630859375\n",
      "Train Loss: 70.73486328125\n",
      "Valid Loss: 567.8654174804688\n",
      "Train Loss: 73.83564758300781\n",
      "Valid Loss: 592.1372680664062\n",
      "Train Loss: 72.00914764404297\n",
      "Valid Loss: 563.700927734375\n",
      "Train Loss: 71.97406768798828\n",
      "Valid Loss: 560.0885009765625\n",
      "Train Loss: 72.30888366699219\n",
      "Valid Loss: 544.4642944335938\n",
      "Train Loss: 73.40780639648438\n",
      "Valid Loss: 534.0675659179688\n",
      "Train Loss: 70.95890808105469\n",
      "Valid Loss: 726.4026489257812\n",
      "Train Loss: 67.6260986328125\n",
      "Valid Loss: 566.9024047851562\n",
      "Train Loss: 73.40447235107422\n",
      "Valid Loss: 565.4046020507812\n",
      "Train Loss: 72.18089294433594\n",
      "Valid Loss: 722.17578125\n",
      "Train Loss: 71.14775085449219\n",
      "Valid Loss: 540.2284545898438\n",
      "Train Loss: 73.01362609863281\n",
      "Valid Loss: 732.2686767578125\n",
      "Train Loss: 71.27398681640625\n",
      "Valid Loss: 517.8927612304688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 73.36345672607422\n",
      "Valid Loss: 573.6124877929688\n",
      "Train Loss: 73.79987335205078\n",
      "Valid Loss: 433.02880859375\n",
      "Train Loss: 72.6086654663086\n",
      "Valid Loss: 563.3661499023438\n",
      "Train Loss: 73.22526550292969\n",
      "Valid Loss: 567.019287109375\n",
      "Train Loss: 73.26133728027344\n",
      "Valid Loss: 568.4072265625\n",
      "Train Loss: 67.89846801757812\n",
      "Valid Loss: 733.582763671875\n",
      "Train Loss: 71.18912506103516\n",
      "Valid Loss: 595.802734375\n",
      "Train Loss: 71.85567474365234\n",
      "Valid Loss: 539.2406616210938\n",
      "Train Loss: 71.84952545166016\n",
      "Valid Loss: 568.4303588867188\n",
      "Train Loss: 68.61940002441406\n",
      "Valid Loss: 571.8065795898438\n",
      "Train Loss: 67.47990417480469\n",
      "Valid Loss: 521.357421875\n",
      "Train Loss: 69.89691162109375\n",
      "Valid Loss: 564.41943359375\n",
      "Train Loss: 73.46366882324219\n",
      "Valid Loss: 571.046142578125\n",
      "Train Loss: 71.91847229003906\n",
      "Valid Loss: 544.1968383789062\n",
      "Train Loss: 71.29473876953125\n",
      "Valid Loss: 597.3981323242188\n",
      "Train Loss: 71.07061767578125\n",
      "Valid Loss: 452.64947509765625\n",
      "Train Loss: 73.38777160644531\n",
      "Valid Loss: 721.7510375976562\n",
      "Train Loss: 70.5788345336914\n",
      "Valid Loss: 730.6820068359375\n",
      "Train Loss: 70.50371551513672\n",
      "Valid Loss: 568.122802734375\n",
      "Train Loss: 71.47648620605469\n",
      "Valid Loss: 542.7379760742188\n",
      "Train Loss: 71.94767761230469\n",
      "Valid Loss: 564.9970703125\n",
      "Train Loss: 69.9071044921875\n",
      "Valid Loss: 425.3799133300781\n",
      "Train Loss: 71.03456115722656\n",
      "Valid Loss: 566.79541015625\n",
      "Train Loss: 72.33908081054688\n",
      "Valid Loss: 427.44732666015625\n",
      "Train Loss: 69.40696716308594\n",
      "Valid Loss: 727.119384765625\n",
      "Train Loss: 70.35095977783203\n",
      "Valid Loss: 599.5323486328125\n",
      "Train Loss: 72.81794738769531\n",
      "Valid Loss: 520.0217895507812\n",
      "Train Loss: 72.52677154541016\n",
      "Valid Loss: 735.510498046875\n",
      "Train Loss: 68.7064208984375\n",
      "Valid Loss: 514.0984497070312\n",
      "Train Loss: 70.05621337890625\n",
      "Valid Loss: 731.638671875\n",
      "Train Loss: 70.3914794921875\n",
      "Valid Loss: 564.3827514648438\n",
      "Train Loss: 67.1663818359375\n",
      "Valid Loss: 544.32861328125\n",
      "Train Loss: 72.29813385009766\n",
      "Valid Loss: 522.9309692382812\n",
      "Train Loss: 70.63955688476562\n",
      "Valid Loss: 600.2820434570312\n",
      "Train Loss: 69.59202575683594\n",
      "Valid Loss: 538.0347290039062\n",
      "Train Loss: 72.0054931640625\n",
      "Valid Loss: 600.615478515625\n",
      "Train Loss: 71.64352416992188\n",
      "Valid Loss: 731.2250366210938\n",
      "Train Loss: 70.43041229248047\n",
      "Valid Loss: 732.4306030273438\n",
      "Train Loss: 71.11494445800781\n",
      "Valid Loss: 726.825439453125\n",
      "Train Loss: 71.69152069091797\n",
      "Valid Loss: 511.06396484375\n",
      "Train Loss: 70.54679107666016\n",
      "Valid Loss: 564.8740234375\n",
      "Train Loss: 68.80543518066406\n",
      "Valid Loss: 544.4939575195312\n",
      "Train Loss: 70.5036392211914\n",
      "Valid Loss: 544.4041137695312\n",
      "Train Loss: 69.51300811767578\n",
      "Valid Loss: 564.3560180664062\n",
      "Train Loss: 71.22142028808594\n",
      "Valid Loss: 427.87481689453125\n",
      "Train Loss: 70.90311431884766\n",
      "Valid Loss: 455.65643310546875\n",
      "Train Loss: 70.6404037475586\n",
      "Valid Loss: 723.6719970703125\n",
      "Train Loss: 71.15887451171875\n",
      "Valid Loss: 540.2885131835938\n",
      "Train Loss: 71.25237274169922\n",
      "Valid Loss: 567.668212890625\n",
      "Train Loss: 69.84699249267578\n",
      "Valid Loss: 566.6232299804688\n",
      "Train Loss: 68.43024444580078\n",
      "Valid Loss: 567.7378540039062\n",
      "Train Loss: 67.53614807128906\n",
      "Valid Loss: 523.5119018554688\n",
      "Train Loss: 70.01451110839844\n",
      "Valid Loss: 738.057861328125\n",
      "Train Loss: 68.00347137451172\n",
      "Valid Loss: 543.2435913085938\n",
      "Train Loss: 66.26485443115234\n",
      "Valid Loss: 721.0665283203125\n",
      "Train Loss: 67.69782257080078\n",
      "Valid Loss: 717.26025390625\n",
      "Train Loss: 70.26454162597656\n",
      "Valid Loss: 569.4462280273438\n",
      "Train Loss: 70.31732177734375\n",
      "Valid Loss: 732.779541015625\n",
      "Train Loss: 70.8733901977539\n",
      "Valid Loss: 456.76446533203125\n",
      "Train Loss: 68.94783782958984\n",
      "Valid Loss: 539.1594848632812\n",
      "Train Loss: 69.4168930053711\n",
      "Valid Loss: 725.5768432617188\n",
      "Train Loss: 72.06256103515625\n",
      "Valid Loss: 568.3132934570312\n",
      "Train Loss: 69.64823150634766\n",
      "Valid Loss: 533.8834228515625\n",
      "Train Loss: 69.25334167480469\n",
      "Valid Loss: 575.8869018554688\n",
      "Train Loss: 69.36519622802734\n",
      "Valid Loss: 564.48486328125\n",
      "Train Loss: 71.29174041748047\n",
      "Valid Loss: 600.41455078125\n",
      "Train Loss: 69.39048767089844\n",
      "Valid Loss: 565.3212280273438\n",
      "Train Loss: 71.47893524169922\n",
      "Valid Loss: 455.1588134765625\n",
      "Train Loss: 69.81069946289062\n",
      "Valid Loss: 567.9285278320312\n",
      "Train Loss: 70.96339416503906\n",
      "Valid Loss: 455.1487121582031\n",
      "Train Loss: 69.65699768066406\n",
      "Valid Loss: 538.7587280273438\n",
      "Train Loss: 71.81078338623047\n",
      "Valid Loss: 519.4900512695312\n",
      "Train Loss: 70.91110229492188\n",
      "Valid Loss: 558.2413940429688\n",
      "Train Loss: 69.90975189208984\n",
      "Valid Loss: 429.8235168457031\n",
      "Train Loss: 70.34554290771484\n",
      "Valid Loss: 536.1201171875\n",
      "Train Loss: 66.79168701171875\n",
      "Valid Loss: 512.015869140625\n",
      "Train Loss: 67.90571594238281\n",
      "Valid Loss: 565.2429809570312\n",
      "Train Loss: 70.53392028808594\n",
      "Valid Loss: 531.3786010742188\n",
      "Train Loss: 70.8905029296875\n",
      "Valid Loss: 455.5770568847656\n",
      "Train Loss: 71.5473403930664\n",
      "Valid Loss: 524.7661743164062\n",
      "Train Loss: 70.07484436035156\n",
      "Valid Loss: 454.75341796875\n",
      "Train Loss: 70.0599136352539\n",
      "Valid Loss: 568.6043090820312\n",
      "Train Loss: 69.63868713378906\n",
      "Valid Loss: 577.1123046875\n",
      "Train Loss: 69.72227478027344\n",
      "Valid Loss: 566.346435546875\n",
      "Train Loss: 70.87723541259766\n",
      "Valid Loss: 543.7471313476562\n",
      "Train Loss: 70.19941711425781\n",
      "Valid Loss: 543.4326171875\n",
      "Train Loss: 69.9704818725586\n",
      "Valid Loss: 566.498046875\n",
      "Train Loss: 68.66846466064453\n",
      "Valid Loss: 518.4719848632812\n",
      "Train Loss: 67.59327697753906\n",
      "Valid Loss: 718.2645874023438\n",
      "Train Loss: 70.43999481201172\n",
      "Valid Loss: 600.2445678710938\n",
      "Train Loss: 68.18286895751953\n",
      "Valid Loss: 569.5634155273438\n",
      "Train Loss: 71.95329284667969\n",
      "Valid Loss: 456.66973876953125\n",
      "Train Loss: 70.85548400878906\n",
      "Valid Loss: 600.0364379882812\n",
      "Train Loss: 69.82324981689453\n",
      "Valid Loss: 544.3431396484375\n",
      "Train Loss: 70.05113983154297\n",
      "Valid Loss: 568.1917114257812\n",
      "Train Loss: 71.12905883789062\n",
      "Valid Loss: 568.7871704101562\n",
      "Train Loss: 70.7831039428711\n",
      "Valid Loss: 569.5750122070312\n",
      "Train Loss: 71.99779510498047\n",
      "Valid Loss: 544.86328125\n",
      "Train Loss: 70.2360610961914\n",
      "Valid Loss: 540.2591552734375\n",
      "Train Loss: 70.55673217773438\n",
      "Valid Loss: 599.8403930664062\n",
      "Train Loss: 71.44055938720703\n",
      "Valid Loss: 731.6385498046875\n",
      "Train Loss: 70.23119354248047\n",
      "Valid Loss: 540.9061889648438\n",
      "Train Loss: 69.28193664550781\n",
      "Valid Loss: 731.7848510742188\n",
      "Train Loss: 69.03680419921875\n",
      "Valid Loss: 544.3029174804688\n",
      "Train Loss: 71.11194610595703\n",
      "Valid Loss: 557.8701171875\n",
      "Train Loss: 69.77957153320312\n",
      "Valid Loss: 540.7495727539062\n",
      "Train Loss: 70.28511047363281\n",
      "Valid Loss: 566.9701538085938\n",
      "Train Loss: 66.53143310546875\n",
      "Valid Loss: 566.8883666992188\n",
      "Train Loss: 69.62986755371094\n",
      "Valid Loss: 531.6045532226562\n",
      "Train Loss: 70.80226135253906\n",
      "Valid Loss: 726.4991455078125\n",
      "Train Loss: 71.04276275634766\n",
      "Valid Loss: 454.7401428222656\n",
      "Train Loss: 68.6022720336914\n",
      "Valid Loss: 731.5938720703125\n",
      "Train Loss: 71.08938598632812\n",
      "Valid Loss: 734.6859130859375\n",
      "Train Loss: 68.1124267578125\n",
      "Valid Loss: 727.2230224609375\n",
      "Train Loss: 71.35214233398438\n",
      "Valid Loss: 567.4296875\n",
      "Train Loss: 70.85121154785156\n",
      "Valid Loss: 601.283203125\n",
      "Train Loss: 67.8479995727539\n",
      "Valid Loss: 567.5691528320312\n",
      "Train Loss: 72.04252624511719\n",
      "Valid Loss: 561.6631469726562\n",
      "Train Loss: 71.6508560180664\n",
      "Valid Loss: 511.6617736816406\n",
      "Train Loss: 70.36226654052734\n",
      "Valid Loss: 570.08203125\n",
      "Train Loss: 70.87029266357422\n",
      "Valid Loss: 600.7764282226562\n",
      "Train Loss: 71.71697998046875\n",
      "Valid Loss: 574.73046875\n",
      "Train Loss: 70.87113952636719\n",
      "Valid Loss: 542.6404418945312\n",
      "Train Loss: 70.73719787597656\n",
      "Valid Loss: 568.4597778320312\n",
      "Train Loss: 69.76958465576172\n",
      "Valid Loss: 566.7185668945312\n",
      "Train Loss: 65.77445983886719\n",
      "Valid Loss: 568.6913452148438\n",
      "Train Loss: 70.14823150634766\n",
      "Valid Loss: 566.523193359375\n",
      "Train Loss: 70.0016860961914\n",
      "Valid Loss: 574.84521484375\n",
      "Train Loss: 71.40502166748047\n",
      "Valid Loss: 524.9286499023438\n",
      "Train Loss: 71.7197494506836\n",
      "Valid Loss: 517.7876586914062\n",
      "Train Loss: 68.67076873779297\n",
      "Valid Loss: 736.0377807617188\n",
      "Train Loss: 68.3772964477539\n",
      "Valid Loss: 569.2932739257812\n",
      "Train Loss: 71.425537109375\n",
      "Valid Loss: 557.3613891601562\n",
      "Train Loss: 70.61933135986328\n",
      "Valid Loss: 575.4931640625\n",
      "Train Loss: 68.09962463378906\n",
      "Valid Loss: 728.3142700195312\n",
      "Train Loss: 71.13204193115234\n",
      "Valid Loss: 731.760009765625\n",
      "Train Loss: 70.04039001464844\n",
      "Valid Loss: 574.225830078125\n",
      "Train Loss: 68.17386627197266\n",
      "Valid Loss: 537.6087036132812\n",
      "Train Loss: 70.27102661132812\n",
      "Valid Loss: 724.59130859375\n",
      "Train Loss: 69.68928527832031\n",
      "Valid Loss: 544.44873046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 69.14789581298828\n",
      "Valid Loss: 434.905517578125\n",
      "Train Loss: 68.43154907226562\n",
      "Valid Loss: 517.4584350585938\n",
      "Train Loss: 69.97235107421875\n",
      "Valid Loss: 569.9212036132812\n",
      "Train Loss: 70.14908599853516\n",
      "Valid Loss: 568.2794799804688\n",
      "Train Loss: 66.25083923339844\n",
      "Valid Loss: 557.3464965820312\n",
      "Train Loss: 70.3773193359375\n",
      "Valid Loss: 455.83154296875\n",
      "Train Loss: 71.52501678466797\n",
      "Valid Loss: 569.4948120117188\n",
      "Train Loss: 70.58111572265625\n",
      "Valid Loss: 566.4956665039062\n",
      "Train Loss: 71.39920806884766\n",
      "Valid Loss: 511.88525390625\n",
      "Train Loss: 68.88127899169922\n",
      "Valid Loss: 737.3117065429688\n",
      "Train Loss: 70.20658111572266\n",
      "Valid Loss: 574.5415649414062\n",
      "Train Loss: 71.65664672851562\n",
      "Valid Loss: 557.716552734375\n",
      "Train Loss: 70.53634643554688\n",
      "Valid Loss: 565.3314819335938\n",
      "Train Loss: 71.50068664550781\n",
      "Valid Loss: 533.237548828125\n",
      "Train Loss: 69.83454132080078\n",
      "Valid Loss: 544.156982421875\n",
      "Train Loss: 71.62779235839844\n",
      "Valid Loss: 427.2405090332031\n",
      "Train Loss: 70.5545883178711\n",
      "Valid Loss: 456.10211181640625\n",
      "Train Loss: 70.75631713867188\n",
      "Valid Loss: 570.2288818359375\n",
      "Train Loss: 67.87049102783203\n",
      "Valid Loss: 578.0617065429688\n",
      "Train Loss: 67.7248764038086\n",
      "Valid Loss: 568.7127075195312\n",
      "Train Loss: 69.04182434082031\n",
      "Valid Loss: 570.0186157226562\n",
      "Train Loss: 71.48076629638672\n",
      "Valid Loss: 539.0161743164062\n",
      "Train Loss: 68.58031463623047\n",
      "Valid Loss: 736.46435546875\n",
      "Train Loss: 68.52112579345703\n",
      "Valid Loss: 524.9210815429688\n",
      "Train Loss: 68.07688903808594\n",
      "Valid Loss: 455.6369934082031\n",
      "Train Loss: 71.87704467773438\n",
      "Valid Loss: 569.4266967773438\n",
      "Train Loss: 69.1415786743164\n",
      "Valid Loss: 726.8101806640625\n",
      "Train Loss: 70.50054168701172\n",
      "Valid Loss: 558.492919921875\n",
      "Train Loss: 68.36626434326172\n",
      "Valid Loss: 728.6439208984375\n",
      "Train Loss: 69.8799057006836\n",
      "Valid Loss: 567.22119140625\n",
      "Train Loss: 71.9444808959961\n",
      "Valid Loss: 717.7169799804688\n",
      "Train Loss: 70.87853240966797\n",
      "Valid Loss: 549.2896728515625\n",
      "Train Loss: 70.51512145996094\n",
      "Valid Loss: 511.386962890625\n",
      "Train Loss: 69.18104553222656\n",
      "Valid Loss: 531.3831176757812\n",
      "Train Loss: 70.79348754882812\n",
      "Valid Loss: 724.1749267578125\n",
      "Train Loss: 70.01070404052734\n",
      "Valid Loss: 722.7094116210938\n",
      "Train Loss: 69.54383087158203\n",
      "Valid Loss: 544.169921875\n",
      "Train Loss: 70.22654724121094\n",
      "Valid Loss: 569.7681884765625\n",
      "Train Loss: 70.18209075927734\n",
      "Valid Loss: 520.0096435546875\n",
      "Train Loss: 70.09882354736328\n",
      "Valid Loss: 600.2052612304688\n",
      "Train Loss: 70.73467254638672\n",
      "Valid Loss: 543.7896118164062\n",
      "Train Loss: 70.53178405761719\n",
      "Valid Loss: 729.527099609375\n",
      "Train Loss: 70.5570297241211\n",
      "Valid Loss: 598.95703125\n",
      "Train Loss: 70.15768432617188\n",
      "Valid Loss: 600.5393676757812\n",
      "Train Loss: 68.86396026611328\n",
      "Valid Loss: 434.6908264160156\n",
      "Train Loss: 71.14682006835938\n",
      "Valid Loss: 543.7742309570312\n",
      "Train Loss: 66.33332824707031\n",
      "Valid Loss: 543.8911743164062\n",
      "Train Loss: 68.90369415283203\n",
      "Valid Loss: 569.44970703125\n",
      "Train Loss: 70.71708679199219\n",
      "Valid Loss: 531.6788940429688\n",
      "Train Loss: 68.49609375\n",
      "Valid Loss: 543.7824096679688\n",
      "Train Loss: 69.17229461669922\n",
      "Valid Loss: 731.3968505859375\n",
      "Train Loss: 71.0728759765625\n",
      "Valid Loss: 535.43896484375\n",
      "Train Loss: 70.32010650634766\n",
      "Valid Loss: 543.7577514648438\n",
      "Train Loss: 70.22299194335938\n",
      "Valid Loss: 731.240478515625\n",
      "Train Loss: 70.6063461303711\n",
      "Valid Loss: 558.2760620117188\n",
      "Train Loss: 69.25914764404297\n",
      "Valid Loss: 567.7438354492188\n",
      "Train Loss: 71.51832580566406\n",
      "Valid Loss: 718.5557861328125\n",
      "Train Loss: 68.77758026123047\n",
      "Valid Loss: 548.1876831054688\n",
      "Train Loss: 69.81647491455078\n",
      "Valid Loss: 434.9785461425781\n",
      "Train Loss: 70.05810546875\n",
      "Valid Loss: 550.761474609375\n",
      "Train Loss: 70.38059997558594\n",
      "Valid Loss: 544.2814331054688\n",
      "Train Loss: 69.0998306274414\n",
      "Valid Loss: 725.3643798828125\n",
      "Train Loss: 70.22486114501953\n",
      "Valid Loss: 733.8001708984375\n",
      "Train Loss: 69.01270294189453\n",
      "Valid Loss: 549.3892211914062\n",
      "Train Loss: 70.84465026855469\n",
      "Valid Loss: 732.05810546875\n",
      "Train Loss: 69.93192291259766\n",
      "Valid Loss: 544.2622680664062\n",
      "Train Loss: 67.82343292236328\n",
      "Valid Loss: 435.0753479003906\n",
      "Train Loss: 70.4928207397461\n",
      "Valid Loss: 557.3305053710938\n",
      "Train Loss: 69.64456939697266\n",
      "Valid Loss: 568.8567504882812\n",
      "Train Loss: 68.5350341796875\n",
      "Valid Loss: 600.1019897460938\n",
      "Train Loss: 70.39598083496094\n",
      "Valid Loss: 568.1273803710938\n",
      "Train Loss: 70.1835708618164\n",
      "Valid Loss: 530.7621459960938\n",
      "Train Loss: 68.14927673339844\n",
      "Valid Loss: 543.7704467773438\n",
      "Train Loss: 69.85956573486328\n",
      "Valid Loss: 548.1155395507812\n",
      "Train Loss: 70.59111785888672\n",
      "Valid Loss: 724.1005859375\n",
      "Train Loss: 67.72615814208984\n",
      "Valid Loss: 599.70751953125\n",
      "Train Loss: 71.51166534423828\n",
      "Valid Loss: 600.552490234375\n",
      "Train Loss: 69.23673248291016\n",
      "Valid Loss: 561.9894409179688\n",
      "Train Loss: 70.27003479003906\n",
      "Valid Loss: 600.1046752929688\n",
      "Train Loss: 70.4051513671875\n",
      "Valid Loss: 534.8345947265625\n",
      "Train Loss: 71.02972412109375\n",
      "Valid Loss: 570.0639038085938\n",
      "Train Loss: 69.27371215820312\n",
      "Valid Loss: 727.334716796875\n",
      "Train Loss: 70.32666778564453\n",
      "Valid Loss: 558.562744140625\n",
      "Train Loss: 69.01948547363281\n",
      "Valid Loss: 520.0513305664062\n",
      "Train Loss: 69.40750122070312\n",
      "Valid Loss: 544.283447265625\n",
      "Train Loss: 70.43885803222656\n",
      "Valid Loss: 511.8561096191406\n",
      "Train Loss: 69.9481201171875\n",
      "Valid Loss: 455.4995422363281\n",
      "Train Loss: 68.74034881591797\n",
      "Valid Loss: 544.2979736328125\n",
      "Train Loss: 71.05074310302734\n",
      "Valid Loss: 511.48724365234375\n",
      "Train Loss: 71.46057891845703\n",
      "Valid Loss: 566.6107788085938\n",
      "Train Loss: 68.85818481445312\n",
      "Valid Loss: 455.63134765625\n",
      "Train Loss: 70.5350112915039\n",
      "Valid Loss: 455.5209045410156\n",
      "Train Loss: 67.92862701416016\n",
      "Valid Loss: 533.6768188476562\n",
      "Train Loss: 68.24290466308594\n",
      "Valid Loss: 543.8622436523438\n",
      "Train Loss: 69.97250366210938\n",
      "Valid Loss: 455.36871337890625\n",
      "Train Loss: 69.642578125\n",
      "Valid Loss: 543.7760009765625\n",
      "Train Loss: 71.36751556396484\n",
      "Valid Loss: 569.564208984375\n",
      "Train Loss: 70.07257843017578\n",
      "Valid Loss: 519.6060791015625\n",
      "Train Loss: 70.88225555419922\n",
      "Valid Loss: 567.7194213867188\n",
      "Train Loss: 70.47066497802734\n",
      "Valid Loss: 727.4022827148438\n",
      "Train Loss: 68.87730407714844\n",
      "Valid Loss: 718.7935791015625\n",
      "Train Loss: 71.39971160888672\n",
      "Valid Loss: 566.545166015625\n",
      "Train Loss: 70.95732116699219\n",
      "Valid Loss: 430.12176513671875\n",
      "Train Loss: 70.55921936035156\n",
      "Valid Loss: 718.802490234375\n",
      "Train Loss: 67.19357299804688\n",
      "Valid Loss: 546.5245971679688\n",
      "Train Loss: 69.55818939208984\n",
      "Valid Loss: 538.8278198242188\n",
      "Train Loss: 71.29512786865234\n",
      "Valid Loss: 721.548095703125\n",
      "Train Loss: 71.3627700805664\n",
      "Valid Loss: 455.6676025390625\n",
      "Train Loss: 68.12674713134766\n",
      "Valid Loss: 544.2564697265625\n",
      "Train Loss: 71.04212188720703\n",
      "Valid Loss: 731.48291015625\n",
      "Train Loss: 69.84910583496094\n",
      "Valid Loss: 540.7443237304688\n",
      "Train Loss: 71.63004302978516\n",
      "Valid Loss: 599.63623046875\n",
      "Train Loss: 70.39541625976562\n",
      "Valid Loss: 600.288818359375\n",
      "Train Loss: 67.9984359741211\n",
      "Valid Loss: 728.6263427734375\n",
      "Train Loss: 69.2744140625\n",
      "Valid Loss: 544.044189453125\n",
      "Train Loss: 69.0733642578125\n",
      "Valid Loss: 435.2412109375\n",
      "Train Loss: 70.48461151123047\n",
      "Valid Loss: 723.982666015625\n",
      "Train Loss: 69.6378402709961\n",
      "Valid Loss: 569.83837890625\n",
      "Train Loss: 71.64018249511719\n",
      "Valid Loss: 599.9561767578125\n",
      "Train Loss: 69.87464904785156\n",
      "Valid Loss: 542.1860961914062\n",
      "Train Loss: 67.3675308227539\n",
      "Valid Loss: 566.5119018554688\n",
      "Train Loss: 67.142333984375\n",
      "Valid Loss: 532.2503051757812\n",
      "Train Loss: 70.09687805175781\n",
      "Valid Loss: 599.6893920898438\n",
      "Train Loss: 71.38703155517578\n",
      "Valid Loss: 566.39404296875\n",
      "Train Loss: 70.40593719482422\n",
      "Valid Loss: 718.3167114257812\n",
      "Train Loss: 70.78610229492188\n",
      "Valid Loss: 735.626953125\n",
      "Train Loss: 70.7553482055664\n",
      "Valid Loss: 534.467041015625\n",
      "Train Loss: 68.88007354736328\n",
      "Valid Loss: 566.3703002929688\n",
      "Train Loss: 70.79805755615234\n",
      "Valid Loss: 575.3295288085938\n",
      "Train Loss: 71.07953643798828\n",
      "Valid Loss: 567.682861328125\n",
      "Train Loss: 70.6164321899414\n",
      "Valid Loss: 544.1904296875\n",
      "Train Loss: 70.68740844726562\n",
      "Valid Loss: 530.8706665039062\n",
      "Train Loss: 70.32211303710938\n",
      "Valid Loss: 600.1967163085938\n",
      "Train Loss: 70.08937072753906\n",
      "Valid Loss: 566.3438110351562\n",
      "Train Loss: 71.80936431884766\n",
      "Valid Loss: 728.0255126953125\n",
      "Train Loss: 70.37525177001953\n",
      "Valid Loss: 737.5203857421875\n",
      "Train Loss: 70.59026336669922\n",
      "Valid Loss: 544.4834594726562\n",
      "Train Loss: 69.10629272460938\n",
      "Valid Loss: 599.9954223632812\n",
      "Train Loss: 71.77910614013672\n",
      "Valid Loss: 731.9513549804688\n",
      "Train Loss: 69.6818618774414\n",
      "Valid Loss: 568.3224487304688\n",
      "Train Loss: 71.34381866455078\n",
      "Valid Loss: 544.0331420898438\n",
      "Train Loss: 71.2945556640625\n",
      "Valid Loss: 600.338623046875\n",
      "Train Loss: 69.85811614990234\n",
      "Valid Loss: 599.5885620117188\n",
      "Train Loss: 70.62571716308594\n",
      "Valid Loss: 455.5419006347656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 67.49808502197266\n",
      "Valid Loss: 455.52789306640625\n",
      "Train Loss: 67.4533920288086\n",
      "Valid Loss: 568.1082763671875\n",
      "Train Loss: 70.69285583496094\n",
      "Valid Loss: 575.1002807617188\n",
      "Train Loss: 69.1910400390625\n",
      "Valid Loss: 432.73968505859375\n",
      "Train Loss: 70.44413757324219\n",
      "Valid Loss: 544.1473999023438\n",
      "Train Loss: 69.11952209472656\n",
      "Valid Loss: 519.8569946289062\n",
      "Train Loss: 69.4056396484375\n",
      "Valid Loss: 533.7139282226562\n",
      "Train Loss: 70.09272003173828\n",
      "Valid Loss: 511.24053955078125\n",
      "Train Loss: 70.83734893798828\n",
      "Valid Loss: 734.58935546875\n",
      "Train Loss: 70.85358428955078\n",
      "Valid Loss: 549.3880615234375\n",
      "Train Loss: 70.39043426513672\n",
      "Valid Loss: 544.1240844726562\n",
      "Train Loss: 69.43566131591797\n",
      "Valid Loss: 736.8877563476562\n",
      "Train Loss: 69.74165344238281\n",
      "Valid Loss: 724.9314575195312\n",
      "Train Loss: 68.2876968383789\n",
      "Valid Loss: 432.212646484375\n",
      "Train Loss: 70.93346405029297\n",
      "Valid Loss: 718.020263671875\n",
      "Train Loss: 70.81230163574219\n",
      "Valid Loss: 600.263916015625\n",
      "Train Loss: 68.8253173828125\n",
      "Valid Loss: 568.22314453125\n",
      "Train Loss: 70.43183898925781\n",
      "Valid Loss: 732.17236328125\n",
      "Train Loss: 69.00308990478516\n",
      "Valid Loss: 546.6767578125\n",
      "Train Loss: 70.25770568847656\n",
      "Valid Loss: 727.2432861328125\n",
      "Train Loss: 71.60872650146484\n",
      "Valid Loss: 569.9091186523438\n",
      "Train Loss: 70.03166198730469\n",
      "Valid Loss: 599.8271484375\n",
      "Train Loss: 69.43976593017578\n",
      "Valid Loss: 600.1234130859375\n",
      "Train Loss: 69.27086639404297\n",
      "Valid Loss: 727.4761962890625\n",
      "Train Loss: 70.10433959960938\n",
      "Valid Loss: 534.0701293945312\n",
      "Train Loss: 70.2254867553711\n",
      "Valid Loss: 737.8458251953125\n",
      "Train Loss: 68.68525695800781\n",
      "Valid Loss: 730.4382934570312\n",
      "Train Loss: 69.79750061035156\n",
      "Valid Loss: 600.0955200195312\n",
      "Train Loss: 70.01884460449219\n",
      "Valid Loss: 600.2529296875\n",
      "Train Loss: 69.70905303955078\n",
      "Valid Loss: 567.3362426757812\n",
      "Train Loss: 69.46776580810547\n",
      "Valid Loss: 541.6184692382812\n",
      "Train Loss: 68.97349548339844\n",
      "Valid Loss: 599.25\n",
      "Train Loss: 71.52687072753906\n",
      "Valid Loss: 543.9072875976562\n",
      "Train Loss: 69.86693572998047\n",
      "Valid Loss: 575.21435546875\n",
      "Train Loss: 69.28767395019531\n",
      "Valid Loss: 566.2531127929688\n",
      "Train Loss: 70.59070587158203\n",
      "Valid Loss: 569.9609375\n",
      "Train Loss: 70.96627807617188\n",
      "Valid Loss: 557.3414306640625\n",
      "Train Loss: 69.46737670898438\n",
      "Valid Loss: 540.4769897460938\n",
      "Train Loss: 68.58802795410156\n",
      "Valid Loss: 569.9436645507812\n",
      "Train Loss: 69.9974365234375\n",
      "Valid Loss: 536.4898071289062\n",
      "Train Loss: 68.19188690185547\n",
      "Valid Loss: 599.8404541015625\n",
      "Train Loss: 71.00042724609375\n",
      "Valid Loss: 737.74462890625\n",
      "Train Loss: 71.28695678710938\n",
      "Valid Loss: 565.6203002929688\n",
      "Train Loss: 71.21310424804688\n",
      "Valid Loss: 600.0084228515625\n",
      "Train Loss: 70.32913208007812\n",
      "Valid Loss: 544.2874145507812\n",
      "Train Loss: 70.37332153320312\n",
      "Valid Loss: 544.2050170898438\n",
      "Train Loss: 67.48773193359375\n",
      "Valid Loss: 600.0936889648438\n",
      "Train Loss: 70.17115020751953\n",
      "Valid Loss: 557.702880859375\n",
      "Train Loss: 70.71216583251953\n",
      "Valid Loss: 575.0621948242188\n",
      "Train Loss: 67.27271270751953\n",
      "Valid Loss: 566.4043579101562\n",
      "Train Loss: 69.33303833007812\n",
      "Valid Loss: 568.2186889648438\n",
      "Train Loss: 67.6416244506836\n",
      "Valid Loss: 737.5396118164062\n",
      "Train Loss: 69.92914581298828\n",
      "Valid Loss: 721.4949951171875\n",
      "Train Loss: 70.06414794921875\n",
      "Valid Loss: 569.8455200195312\n",
      "Train Loss: 71.17548370361328\n",
      "Valid Loss: 728.2913818359375\n",
      "Train Loss: 70.31914520263672\n",
      "Valid Loss: 574.40283203125\n",
      "Train Loss: 70.1052017211914\n",
      "Valid Loss: 731.656982421875\n",
      "Train Loss: 70.04219055175781\n",
      "Valid Loss: 599.7555541992188\n",
      "Train Loss: 71.03109741210938\n",
      "Valid Loss: 567.9850463867188\n",
      "Train Loss: 70.9602279663086\n",
      "Valid Loss: 599.8051147460938\n",
      "Train Loss: 68.67680358886719\n",
      "Valid Loss: 544.2124633789062\n",
      "Train Loss: 66.64115905761719\n",
      "Valid Loss: 432.4077453613281\n",
      "Train Loss: 69.17922973632812\n",
      "Valid Loss: 727.9500122070312\n",
      "Train Loss: 69.05603790283203\n",
      "Valid Loss: 557.177734375\n",
      "Train Loss: 70.41659545898438\n",
      "Valid Loss: 599.7144165039062\n",
      "Train Loss: 71.55451965332031\n",
      "Valid Loss: 532.4395751953125\n",
      "Train Loss: 70.91410827636719\n",
      "Valid Loss: 569.4677124023438\n",
      "Train Loss: 71.01274108886719\n",
      "Valid Loss: 545.5125122070312\n",
      "Train Loss: 69.51493835449219\n",
      "Valid Loss: 455.3675842285156\n",
      "Train Loss: 66.7029037475586\n",
      "Valid Loss: 728.2447509765625\n",
      "Train Loss: 71.2513427734375\n",
      "Valid Loss: 540.7857055664062\n",
      "Train Loss: 70.8302993774414\n",
      "Valid Loss: 599.9602661132812\n",
      "Train Loss: 67.08656311035156\n",
      "Valid Loss: 544.1856079101562\n",
      "Train Loss: 71.09730529785156\n",
      "Valid Loss: 732.277099609375\n",
      "Train Loss: 70.88958740234375\n",
      "Valid Loss: 544.1342163085938\n",
      "Train Loss: 70.37728881835938\n",
      "Valid Loss: 544.0690307617188\n",
      "Train Loss: 66.64796447753906\n",
      "Valid Loss: 545.2816772460938\n",
      "Train Loss: 69.84622192382812\n",
      "Valid Loss: 566.4475708007812\n",
      "Train Loss: 69.38190460205078\n",
      "Valid Loss: 599.9103393554688\n",
      "Train Loss: 70.06033325195312\n",
      "Valid Loss: 568.004150390625\n",
      "Train Loss: 71.39620971679688\n",
      "Valid Loss: 530.681396484375\n",
      "Train Loss: 71.29057312011719\n",
      "Valid Loss: 455.7391662597656\n",
      "Train Loss: 69.41437530517578\n",
      "Valid Loss: 724.5386962890625\n",
      "Train Loss: 66.7806167602539\n",
      "Valid Loss: 600.3840942382812\n",
      "Train Loss: 69.95209503173828\n",
      "Valid Loss: 728.046875\n",
      "Train Loss: 69.99260711669922\n",
      "Valid Loss: 737.0545654296875\n",
      "Train Loss: 70.88037872314453\n",
      "Valid Loss: 737.5084228515625\n",
      "Train Loss: 71.01720428466797\n",
      "Valid Loss: 540.1138305664062\n",
      "Train Loss: 70.00821685791016\n",
      "Valid Loss: 434.94677734375\n",
      "Train Loss: 68.74319458007812\n",
      "Valid Loss: 435.4072570800781\n",
      "Train Loss: 69.27017211914062\n",
      "Valid Loss: 600.338623046875\n",
      "Train Loss: 71.0491943359375\n",
      "Valid Loss: 543.751220703125\n",
      "Train Loss: 70.36149597167969\n",
      "Valid Loss: 599.6504516601562\n",
      "Train Loss: 69.18991088867188\n",
      "Valid Loss: 455.5365295410156\n",
      "Train Loss: 70.40904235839844\n",
      "Valid Loss: 567.7859497070312\n",
      "Train Loss: 71.9197769165039\n",
      "Valid Loss: 599.492919921875\n",
      "Train Loss: 70.50721740722656\n",
      "Valid Loss: 727.3751831054688\n",
      "Train Loss: 71.26226806640625\n",
      "Valid Loss: 543.5206909179688\n",
      "Train Loss: 70.94529724121094\n",
      "Valid Loss: 599.45458984375\n",
      "Train Loss: 70.25598907470703\n",
      "Valid Loss: 567.9938354492188\n",
      "Train Loss: 70.20162963867188\n",
      "Valid Loss: 531.8778686523438\n",
      "Train Loss: 69.11859130859375\n",
      "Valid Loss: 537.833740234375\n",
      "Train Loss: 70.40168762207031\n",
      "Valid Loss: 517.6552734375\n",
      "Train Loss: 70.2534408569336\n",
      "Valid Loss: 599.630859375\n",
      "Train Loss: 68.87142181396484\n",
      "Valid Loss: 566.844970703125\n",
      "Train Loss: 69.5983657836914\n",
      "Valid Loss: 534.9353637695312\n",
      "Train Loss: 71.16239929199219\n",
      "Valid Loss: 530.5908813476562\n",
      "Train Loss: 65.86985778808594\n",
      "Valid Loss: 728.2230834960938\n",
      "Train Loss: 69.19601440429688\n",
      "Valid Loss: 432.185302734375\n",
      "Train Loss: 69.44501495361328\n",
      "Valid Loss: 728.41552734375\n",
      "Train Loss: 70.1413803100586\n",
      "Valid Loss: 543.0526123046875\n",
      "Train Loss: 70.82009887695312\n",
      "Valid Loss: 543.8883056640625\n",
      "Train Loss: 71.08942413330078\n",
      "Valid Loss: 732.10009765625\n",
      "Train Loss: 71.04309844970703\n",
      "Valid Loss: 531.8545532226562\n",
      "Train Loss: 70.38427734375\n",
      "Valid Loss: 731.6900024414062\n",
      "Train Loss: 68.48857879638672\n",
      "Valid Loss: 534.8921508789062\n",
      "Train Loss: 70.14151000976562\n",
      "Valid Loss: 732.0850830078125\n",
      "Train Loss: 70.41091918945312\n",
      "Valid Loss: 543.9242553710938\n",
      "Train Loss: 71.26961517333984\n",
      "Valid Loss: 725.2024536132812\n",
      "Train Loss: 71.15410614013672\n",
      "Valid Loss: 455.60382080078125\n",
      "Train Loss: 70.08991241455078\n",
      "Valid Loss: 426.8876953125\n",
      "Train Loss: 71.48992919921875\n",
      "Valid Loss: 717.5020141601562\n",
      "Train Loss: 70.82685089111328\n",
      "Valid Loss: 569.62939453125\n",
      "Train Loss: 71.09783172607422\n",
      "Valid Loss: 575.3659057617188\n",
      "Train Loss: 66.91056060791016\n",
      "Valid Loss: 599.3258666992188\n",
      "Train Loss: 69.59056091308594\n",
      "Valid Loss: 567.7048950195312\n",
      "Train Loss: 69.09272766113281\n",
      "Valid Loss: 431.9223937988281\n",
      "Train Loss: 69.27452087402344\n",
      "Valid Loss: 567.5258178710938\n",
      "Train Loss: 69.86785125732422\n",
      "Valid Loss: 566.3168334960938\n",
      "Train Loss: 70.6129150390625\n",
      "Valid Loss: 600.4447631835938\n",
      "Train Loss: 68.69000244140625\n",
      "Valid Loss: 516.8020629882812\n",
      "Train Loss: 69.33963775634766\n",
      "Valid Loss: 731.8119506835938\n",
      "Train Loss: 68.49418640136719\n",
      "Valid Loss: 574.743896484375\n",
      "Train Loss: 70.33808898925781\n",
      "Valid Loss: 567.692138671875\n",
      "Train Loss: 69.60508728027344\n",
      "Valid Loss: 734.6663818359375\n",
      "Train Loss: 69.87108612060547\n",
      "Valid Loss: 569.5091552734375\n",
      "Train Loss: 70.8932113647461\n",
      "Valid Loss: 519.584716796875\n",
      "Train Loss: 69.56106567382812\n",
      "Valid Loss: 725.3724975585938\n",
      "Train Loss: 70.33157348632812\n",
      "Valid Loss: 729.320556640625\n",
      "Train Loss: 69.46977233886719\n",
      "Valid Loss: 540.9240112304688\n",
      "Train Loss: 69.86409759521484\n",
      "Valid Loss: 566.4230346679688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 68.32642364501953\n",
      "Valid Loss: 599.6017456054688\n",
      "Train Loss: 71.4017562866211\n",
      "Valid Loss: 543.7356567382812\n",
      "Train Loss: 70.52593994140625\n",
      "Valid Loss: 732.484130859375\n",
      "Train Loss: 70.71292877197266\n",
      "Valid Loss: 567.8152465820312\n",
      "Train Loss: 71.8032455444336\n",
      "Valid Loss: 544.0198364257812\n",
      "Train Loss: 67.22298431396484\n",
      "Valid Loss: 599.4813842773438\n",
      "Train Loss: 68.26220703125\n",
      "Valid Loss: 544.181640625\n",
      "Train Loss: 70.59828186035156\n",
      "Valid Loss: 565.5779418945312\n",
      "Train Loss: 69.15977478027344\n",
      "Valid Loss: 600.0770263671875\n",
      "Train Loss: 69.15381622314453\n",
      "Valid Loss: 718.341796875\n",
      "Train Loss: 70.916015625\n",
      "Valid Loss: 569.7828979492188\n",
      "Train Loss: 68.92594146728516\n",
      "Valid Loss: 455.9560546875\n",
      "Train Loss: 69.13719177246094\n",
      "Valid Loss: 568.3777465820312\n",
      "Train Loss: 68.3379898071289\n",
      "Valid Loss: 599.6882934570312\n",
      "Train Loss: 69.41172790527344\n",
      "Valid Loss: 600.056640625\n",
      "Train Loss: 69.88018035888672\n",
      "Valid Loss: 725.142333984375\n",
      "Train Loss: 70.31559753417969\n",
      "Valid Loss: 737.3331298828125\n",
      "Train Loss: 71.16160583496094\n",
      "Valid Loss: 511.73760986328125\n",
      "Train Loss: 70.0685043334961\n",
      "Valid Loss: 534.8894653320312\n",
      "Train Loss: 70.35786437988281\n",
      "Valid Loss: 534.6231079101562\n",
      "Train Loss: 70.38156127929688\n",
      "Valid Loss: 570.3389282226562\n",
      "Train Loss: 71.34523010253906\n",
      "Valid Loss: 456.1705627441406\n",
      "Train Loss: 71.06116485595703\n",
      "Valid Loss: 732.7119140625\n",
      "Train Loss: 69.79159545898438\n",
      "Valid Loss: 455.90118408203125\n",
      "Train Loss: 69.72502136230469\n",
      "Valid Loss: 732.33349609375\n",
      "Train Loss: 70.73143005371094\n",
      "Valid Loss: 567.5969848632812\n",
      "Train Loss: 69.3759765625\n",
      "Valid Loss: 511.34796142578125\n",
      "Train Loss: 67.8082504272461\n",
      "Valid Loss: 557.893798828125\n",
      "Train Loss: 67.453125\n",
      "Valid Loss: 732.0946044921875\n",
      "Train Loss: 71.19384765625\n",
      "Valid Loss: 565.8666381835938\n",
      "Train Loss: 68.96058654785156\n",
      "Valid Loss: 569.0778198242188\n",
      "Train Loss: 70.44863891601562\n",
      "Valid Loss: 557.4755859375\n",
      "Train Loss: 68.69842529296875\n",
      "Valid Loss: 567.2578735351562\n",
      "Train Loss: 69.32798767089844\n",
      "Valid Loss: 731.3412475585938\n",
      "Train Loss: 70.29444122314453\n",
      "Valid Loss: 735.992431640625\n",
      "Train Loss: 71.31439971923828\n",
      "Valid Loss: 726.9577026367188\n",
      "Train Loss: 71.19430541992188\n",
      "Valid Loss: 599.7727661132812\n",
      "Train Loss: 69.16134643554688\n",
      "Valid Loss: 544.0786743164062\n",
      "Train Loss: 70.97845458984375\n",
      "Valid Loss: 544.0270385742188\n",
      "Train Loss: 69.18558502197266\n",
      "Valid Loss: 732.6583251953125\n",
      "Train Loss: 67.91598510742188\n",
      "Valid Loss: 600.1699829101562\n",
      "Train Loss: 69.25007629394531\n",
      "Valid Loss: 558.3008422851562\n",
      "Train Loss: 66.5575942993164\n",
      "Valid Loss: 570.1503295898438\n",
      "Train Loss: 71.2079086303711\n",
      "Valid Loss: 736.9827880859375\n",
      "Train Loss: 69.54843139648438\n",
      "Valid Loss: 732.0698852539062\n",
      "Train Loss: 70.29325103759766\n",
      "Valid Loss: 600.0418090820312\n",
      "Train Loss: 70.29840850830078\n",
      "Valid Loss: 565.5875244140625\n",
      "Train Loss: 68.6224365234375\n",
      "Valid Loss: 599.9243774414062\n",
      "Train Loss: 69.72987365722656\n",
      "Valid Loss: 540.7258911132812\n",
      "Train Loss: 69.38568115234375\n",
      "Valid Loss: 732.0445556640625\n",
      "Train Loss: 69.528076171875\n",
      "Valid Loss: 543.6259155273438\n",
      "Train Loss: 70.29094696044922\n",
      "Valid Loss: 455.79986572265625\n",
      "Train Loss: 70.16621398925781\n",
      "Valid Loss: 544.0623779296875\n",
      "Train Loss: 70.19514465332031\n",
      "Valid Loss: 566.9389038085938\n",
      "Train Loss: 68.27987670898438\n",
      "Valid Loss: 566.9577026367188\n",
      "Train Loss: 69.23943328857422\n",
      "Valid Loss: 599.4755249023438\n",
      "Train Loss: 70.094970703125\n",
      "Valid Loss: 566.22412109375\n",
      "Train Loss: 71.36488342285156\n",
      "Valid Loss: 430.14459228515625\n",
      "Train Loss: 69.0177993774414\n",
      "Valid Loss: 567.6531372070312\n",
      "Train Loss: 67.21186828613281\n",
      "Valid Loss: 569.194580078125\n",
      "Train Loss: 69.68038940429688\n",
      "Valid Loss: 569.4956665039062\n",
      "Train Loss: 69.58401489257812\n",
      "Valid Loss: 736.5431518554688\n",
      "Train Loss: 70.29812622070312\n",
      "Valid Loss: 569.654541015625\n",
      "Train Loss: 71.28005981445312\n",
      "Valid Loss: 566.7208862304688\n",
      "Train Loss: 70.94581604003906\n",
      "Valid Loss: 511.01513671875\n",
      "Train Loss: 69.97624206542969\n",
      "Valid Loss: 568.8789672851562\n",
      "Train Loss: 68.92546844482422\n",
      "Valid Loss: 567.233154296875\n",
      "Train Loss: 67.5472640991211\n",
      "Valid Loss: 540.977294921875\n",
      "Train Loss: 70.18441009521484\n",
      "Valid Loss: 731.2319946289062\n",
      "Train Loss: 66.9188232421875\n",
      "Valid Loss: 567.3137817382812\n",
      "Train Loss: 68.93849182128906\n",
      "Valid Loss: 566.3653564453125\n",
      "Train Loss: 71.47343444824219\n",
      "Valid Loss: 731.6734619140625\n",
      "Train Loss: 71.46704864501953\n",
      "Valid Loss: 557.8123168945312\n",
      "Train Loss: 71.2506332397461\n",
      "Valid Loss: 737.691650390625\n",
      "Train Loss: 70.61553192138672\n",
      "Valid Loss: 545.8268432617188\n",
      "Train Loss: 69.9642105102539\n",
      "Valid Loss: 566.9922485351562\n",
      "Train Loss: 68.43424224853516\n",
      "Valid Loss: 599.355224609375\n",
      "Train Loss: 67.96019744873047\n",
      "Valid Loss: 541.7382202148438\n",
      "Train Loss: 68.3515853881836\n",
      "Valid Loss: 566.8358154296875\n",
      "Train Loss: 69.26194763183594\n",
      "Valid Loss: 548.1867065429688\n",
      "Train Loss: 68.22024536132812\n",
      "Valid Loss: 599.2825317382812\n",
      "Train Loss: 68.7721939086914\n",
      "Valid Loss: 600.5908813476562\n",
      "Train Loss: 69.38890838623047\n",
      "Valid Loss: 543.8121948242188\n",
      "Train Loss: 68.56869506835938\n",
      "Valid Loss: 717.137451171875\n",
      "Train Loss: 71.35237121582031\n",
      "Valid Loss: 427.1114501953125\n",
      "Train Loss: 67.11717987060547\n",
      "Valid Loss: 731.344970703125\n",
      "Train Loss: 69.05650329589844\n",
      "Valid Loss: 599.7562866210938\n",
      "Train Loss: 70.24957275390625\n",
      "Valid Loss: 543.65234375\n",
      "Train Loss: 69.78941345214844\n",
      "Valid Loss: 724.838623046875\n",
      "Train Loss: 69.99103546142578\n",
      "Valid Loss: 531.8458862304688\n",
      "Train Loss: 68.46206665039062\n",
      "Valid Loss: 570.0093383789062\n",
      "Train Loss: 69.89068603515625\n",
      "Valid Loss: 599.8030395507812\n",
      "Train Loss: 69.46958923339844\n",
      "Valid Loss: 728.3812866210938\n",
      "Train Loss: 69.00140380859375\n",
      "Valid Loss: 545.9556274414062\n",
      "Train Loss: 70.0042724609375\n",
      "Valid Loss: 557.8027954101562\n",
      "Train Loss: 68.905029296875\n",
      "Valid Loss: 567.317626953125\n",
      "Train Loss: 69.10712432861328\n",
      "Valid Loss: 731.426513671875\n",
      "Train Loss: 68.69515228271484\n",
      "Valid Loss: 718.213134765625\n",
      "Train Loss: 70.66514587402344\n",
      "Valid Loss: 733.5217895507812\n",
      "Train Loss: 69.93675994873047\n",
      "Valid Loss: 535.1922607421875\n",
      "Train Loss: 68.5952377319336\n",
      "Valid Loss: 731.6698608398438\n",
      "Train Loss: 70.63111877441406\n",
      "Valid Loss: 569.9484252929688\n",
      "Train Loss: 67.242919921875\n",
      "Valid Loss: 543.9271850585938\n",
      "Train Loss: 70.00049591064453\n",
      "Valid Loss: 721.5662231445312\n",
      "Train Loss: 67.99950408935547\n",
      "Valid Loss: 568.051025390625\n",
      "Train Loss: 68.54059600830078\n",
      "Valid Loss: 544.1581420898438\n",
      "Train Loss: 70.29650115966797\n",
      "Valid Loss: 567.8993530273438\n",
      "Train Loss: 70.46591186523438\n",
      "Valid Loss: 540.9893188476562\n",
      "Train Loss: 68.0584945678711\n",
      "Valid Loss: 737.5808715820312\n",
      "Train Loss: 69.93525695800781\n",
      "Valid Loss: 427.23748779296875\n",
      "Train Loss: 70.49967193603516\n",
      "Valid Loss: 543.2566528320312\n",
      "Train Loss: 70.07920837402344\n",
      "Valid Loss: 728.4645385742188\n",
      "Train Loss: 71.01689147949219\n",
      "Valid Loss: 569.3713989257812\n",
      "Train Loss: 69.85913848876953\n",
      "Valid Loss: 557.7586059570312\n",
      "Train Loss: 70.0389404296875\n",
      "Valid Loss: 511.1772766113281\n",
      "Train Loss: 67.2186050415039\n",
      "Valid Loss: 543.6897583007812\n",
      "Train Loss: 70.90084075927734\n",
      "Valid Loss: 732.3424682617188\n",
      "Train Loss: 70.4820327758789\n",
      "Valid Loss: 731.7978515625\n",
      "Train Loss: 70.62647247314453\n",
      "Valid Loss: 723.921875\n",
      "Train Loss: 70.84090423583984\n",
      "Valid Loss: 566.8474731445312\n",
      "Train Loss: 69.72992706298828\n",
      "Valid Loss: 567.4834594726562\n",
      "Train Loss: 68.19144439697266\n",
      "Valid Loss: 548.9277954101562\n",
      "Train Loss: 71.04187774658203\n",
      "Valid Loss: 569.9760131835938\n",
      "Train Loss: 68.68962097167969\n",
      "Valid Loss: 727.6634521484375\n",
      "Train Loss: 68.95376586914062\n",
      "Valid Loss: 432.189453125\n",
      "Train Loss: 70.54155731201172\n",
      "Valid Loss: 736.6695556640625\n",
      "Train Loss: 69.06616973876953\n",
      "Valid Loss: 566.6524047851562\n",
      "Train Loss: 67.34281921386719\n",
      "Valid Loss: 567.1566772460938\n",
      "Train Loss: 70.4454345703125\n",
      "Valid Loss: 736.8674926757812\n",
      "Train Loss: 70.42137908935547\n",
      "Valid Loss: 728.48974609375\n",
      "Train Loss: 70.74577331542969\n",
      "Valid Loss: 600.174560546875\n",
      "Train Loss: 66.43965148925781\n",
      "Valid Loss: 569.5505981445312\n",
      "Train Loss: 68.62867736816406\n",
      "Valid Loss: 543.1033325195312\n",
      "Train Loss: 71.47632598876953\n",
      "Valid Loss: 732.709228515625\n",
      "Train Loss: 70.11822509765625\n",
      "Valid Loss: 569.8185424804688\n",
      "Train Loss: 71.22809600830078\n",
      "Valid Loss: 723.9340209960938\n",
      "Train Loss: 67.1771011352539\n",
      "Valid Loss: 566.853515625\n",
      "Train Loss: 70.53244018554688\n",
      "Valid Loss: 567.5686645507812\n",
      "Train Loss: 69.22434997558594\n",
      "Valid Loss: 569.7299194335938\n",
      "Train Loss: 69.13752746582031\n",
      "Valid Loss: 569.4286499023438\n",
      "Train Loss: 68.3028793334961\n",
      "Valid Loss: 567.5185546875\n",
      "Train Loss: 69.56077575683594\n",
      "Valid Loss: 567.03271484375\n",
      "Train Loss: 69.86602020263672\n",
      "Valid Loss: 427.2705078125\n",
      "Train Loss: 70.64930725097656\n",
      "Valid Loss: 731.2574462890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 69.31764221191406\n",
      "Valid Loss: 718.625\n",
      "Train Loss: 70.34473419189453\n",
      "Valid Loss: 534.0103149414062\n",
      "Train Loss: 68.56564331054688\n",
      "Valid Loss: 455.22515869140625\n",
      "Train Loss: 71.22007751464844\n",
      "Valid Loss: 736.667724609375\n",
      "Train Loss: 69.74980926513672\n",
      "Valid Loss: 569.199462890625\n",
      "Train Loss: 70.56784057617188\n",
      "Valid Loss: 455.437744140625\n",
      "Train Loss: 68.85319519042969\n",
      "Valid Loss: 426.9064025878906\n",
      "Train Loss: 69.51908111572266\n",
      "Valid Loss: 569.6824340820312\n",
      "Train Loss: 70.21980285644531\n",
      "Valid Loss: 427.0104064941406\n",
      "Train Loss: 71.04505157470703\n",
      "Valid Loss: 455.43939208984375\n",
      "Train Loss: 66.48887634277344\n",
      "Valid Loss: 728.5719604492188\n",
      "Train Loss: 70.73027038574219\n",
      "Valid Loss: 533.916259765625\n",
      "Train Loss: 70.02530670166016\n",
      "Valid Loss: 565.9945068359375\n",
      "Train Loss: 70.11174774169922\n",
      "Valid Loss: 600.1270141601562\n",
      "Train Loss: 70.74583435058594\n",
      "Valid Loss: 599.720703125\n",
      "Train Loss: 70.74337768554688\n",
      "Valid Loss: 548.1836547851562\n",
      "Train Loss: 70.92524719238281\n",
      "Valid Loss: 566.5480346679688\n",
      "Train Loss: 70.82408142089844\n",
      "Valid Loss: 569.599609375\n",
      "Train Loss: 69.87599182128906\n",
      "Valid Loss: 566.9872436523438\n",
      "Train Loss: 70.18386840820312\n",
      "Valid Loss: 731.765625\n",
      "Train Loss: 71.10182189941406\n",
      "Valid Loss: 725.1407470703125\n",
      "Train Loss: 69.9474105834961\n",
      "Valid Loss: 599.7868041992188\n",
      "Train Loss: 68.4897689819336\n",
      "Valid Loss: 430.31561279296875\n",
      "Train Loss: 69.99517059326172\n",
      "Valid Loss: 455.6725158691406\n",
      "Train Loss: 67.9651870727539\n",
      "Valid Loss: 737.8406982421875\n",
      "Train Loss: 70.29916381835938\n",
      "Valid Loss: 717.4576416015625\n",
      "Train Loss: 69.8483657836914\n",
      "Valid Loss: 535.4851684570312\n",
      "Train Loss: 69.75567626953125\n",
      "Valid Loss: 599.6925659179688\n",
      "Train Loss: 68.61079406738281\n",
      "Valid Loss: 600.57763671875\n",
      "Train Loss: 69.68201446533203\n",
      "Valid Loss: 600.2986450195312\n",
      "Train Loss: 70.78945922851562\n",
      "Valid Loss: 569.435546875\n",
      "Train Loss: 70.6191635131836\n",
      "Valid Loss: 732.6700439453125\n",
      "Train Loss: 70.02010345458984\n",
      "Valid Loss: 732.6004028320312\n",
      "Train Loss: 70.27483367919922\n",
      "Valid Loss: 434.88720703125\n",
      "Train Loss: 70.70122528076172\n",
      "Valid Loss: 600.8456420898438\n",
      "Train Loss: 70.6406478881836\n",
      "Valid Loss: 569.9265747070312\n",
      "Train Loss: 68.10758209228516\n",
      "Valid Loss: 567.58447265625\n",
      "Train Loss: 69.80144500732422\n",
      "Valid Loss: 431.82574462890625\n",
      "Train Loss: 68.08602142333984\n",
      "Valid Loss: 736.587646484375\n",
      "Train Loss: 69.66405487060547\n",
      "Valid Loss: 523.2761840820312\n",
      "Train Loss: 69.71589660644531\n",
      "Valid Loss: 567.3374633789062\n",
      "Train Loss: 66.97307586669922\n",
      "Valid Loss: 731.5997924804688\n",
      "Train Loss: 66.46577453613281\n",
      "Valid Loss: 569.3135375976562\n",
      "Train Loss: 68.48273468017578\n",
      "Valid Loss: 734.80517578125\n",
      "Train Loss: 70.85385131835938\n",
      "Valid Loss: 558.6509399414062\n",
      "Train Loss: 70.79216003417969\n",
      "Valid Loss: 574.1172485351562\n",
      "Train Loss: 68.43666076660156\n",
      "Valid Loss: 541.8541870117188\n",
      "Train Loss: 70.22813415527344\n",
      "Valid Loss: 524.2686157226562\n",
      "Train Loss: 69.01077270507812\n",
      "Valid Loss: 727.8037109375\n",
      "Train Loss: 70.23635864257812\n",
      "Valid Loss: 737.9141235351562\n",
      "Train Loss: 70.05876159667969\n",
      "Valid Loss: 566.3291625976562\n",
      "Train Loss: 69.47754669189453\n",
      "Valid Loss: 727.5069580078125\n",
      "Train Loss: 70.31810760498047\n",
      "Valid Loss: 455.6297607421875\n",
      "Train Loss: 71.13475799560547\n",
      "Valid Loss: 503.9055480957031\n",
      "Train Loss: 70.99301147460938\n",
      "Valid Loss: 533.8003540039062\n",
      "Train Loss: 69.7986831665039\n",
      "Valid Loss: 600.388671875\n",
      "Train Loss: 70.15687561035156\n",
      "Valid Loss: 599.8062133789062\n",
      "Train Loss: 70.4462890625\n",
      "Valid Loss: 718.2059326171875\n",
      "Train Loss: 69.45398712158203\n",
      "Valid Loss: 567.41015625\n",
      "Train Loss: 70.55207824707031\n",
      "Valid Loss: 569.7161254882812\n",
      "Train Loss: 69.0676498413086\n",
      "Valid Loss: 599.5746459960938\n",
      "Train Loss: 69.38343048095703\n",
      "Valid Loss: 569.7529296875\n",
      "Train Loss: 69.67501068115234\n",
      "Valid Loss: 455.72601318359375\n",
      "Train Loss: 70.14944458007812\n",
      "Valid Loss: 455.80426025390625\n",
      "Train Loss: 69.31510162353516\n",
      "Valid Loss: 600.2962036132812\n",
      "Train Loss: 70.61112976074219\n",
      "Valid Loss: 567.4202270507812\n",
      "Train Loss: 68.5302734375\n",
      "Valid Loss: 543.6427612304688\n",
      "Train Loss: 70.18008422851562\n",
      "Valid Loss: 718.8668212890625\n",
      "Train Loss: 68.41365814208984\n",
      "Valid Loss: 432.1382141113281\n",
      "Train Loss: 71.03765106201172\n",
      "Valid Loss: 569.8484497070312\n",
      "Train Loss: 69.34901428222656\n",
      "Valid Loss: 599.7819213867188\n",
      "Train Loss: 71.083984375\n",
      "Valid Loss: 543.582763671875\n",
      "Train Loss: 70.3455810546875\n",
      "Valid Loss: 548.4989624023438\n",
      "Train Loss: 69.97374725341797\n",
      "Valid Loss: 600.5635375976562\n",
      "Train Loss: 67.96011352539062\n",
      "Valid Loss: 567.04296875\n",
      "Train Loss: 69.0742416381836\n",
      "Valid Loss: 540.5033569335938\n",
      "Train Loss: 71.3247299194336\n",
      "Valid Loss: 731.302490234375\n",
      "Train Loss: 71.40705871582031\n",
      "Valid Loss: 570.0980834960938\n",
      "Train Loss: 69.93405151367188\n",
      "Valid Loss: 455.3692626953125\n",
      "Train Loss: 70.38041687011719\n",
      "Valid Loss: 732.26904296875\n",
      "Train Loss: 69.98660278320312\n",
      "Valid Loss: 567.4566040039062\n",
      "Train Loss: 69.34132385253906\n",
      "Valid Loss: 736.6015014648438\n",
      "Train Loss: 69.62057495117188\n",
      "Valid Loss: 575.387939453125\n",
      "Train Loss: 69.47066497802734\n",
      "Valid Loss: 731.447021484375\n",
      "Train Loss: 70.99048614501953\n",
      "Valid Loss: 599.584228515625\n",
      "Train Loss: 70.61662292480469\n",
      "Valid Loss: 567.5563354492188\n",
      "Train Loss: 70.80270385742188\n",
      "Valid Loss: 733.03515625\n",
      "Train Loss: 69.5419692993164\n",
      "Valid Loss: 565.841064453125\n",
      "Train Loss: 67.47374725341797\n",
      "Valid Loss: 523.5861206054688\n",
      "Train Loss: 68.27493286132812\n",
      "Valid Loss: 535.4276733398438\n",
      "Train Loss: 67.90319061279297\n",
      "Valid Loss: 523.9119262695312\n",
      "Train Loss: 70.15902709960938\n",
      "Valid Loss: 574.5693359375\n",
      "Train Loss: 68.41349029541016\n",
      "Valid Loss: 725.49609375\n",
      "Train Loss: 68.85926055908203\n",
      "Valid Loss: 548.1497192382812\n",
      "Train Loss: 71.20724487304688\n",
      "Valid Loss: 725.3869018554688\n",
      "Train Loss: 67.6528091430664\n",
      "Valid Loss: 566.9563598632812\n",
      "Train Loss: 70.08008575439453\n",
      "Valid Loss: 569.726806640625\n",
      "Train Loss: 70.79475402832031\n",
      "Valid Loss: 455.22198486328125\n",
      "Train Loss: 67.09040069580078\n",
      "Valid Loss: 434.82183837890625\n",
      "Train Loss: 68.21453857421875\n",
      "Valid Loss: 732.549560546875\n",
      "Train Loss: 68.28330993652344\n",
      "Valid Loss: 732.7777099609375\n",
      "Train Loss: 69.83536529541016\n",
      "Valid Loss: 510.75653076171875\n",
      "Train Loss: 70.84037017822266\n",
      "Valid Loss: 541.2238159179688\n",
      "Train Loss: 70.17170715332031\n",
      "Valid Loss: 569.8187255859375\n",
      "Train Loss: 70.64187622070312\n",
      "Valid Loss: 510.88043212890625\n",
      "Train Loss: 68.6707534790039\n",
      "Valid Loss: 430.1455383300781\n",
      "Train Loss: 69.72355651855469\n",
      "Valid Loss: 731.5863647460938\n",
      "Train Loss: 69.00535583496094\n",
      "Valid Loss: 542.9384765625\n",
      "Train Loss: 69.43788146972656\n",
      "Valid Loss: 542.9819946289062\n",
      "Train Loss: 69.76261138916016\n",
      "Valid Loss: 738.1414184570312\n",
      "Train Loss: 69.1009521484375\n",
      "Valid Loss: 728.9691772460938\n",
      "Train Loss: 69.68208312988281\n",
      "Valid Loss: 530.7294311523438\n",
      "Train Loss: 70.00481414794922\n",
      "Valid Loss: 599.0234375\n",
      "Train Loss: 67.66432189941406\n",
      "Valid Loss: 567.7431640625\n",
      "Train Loss: 71.02393341064453\n",
      "Valid Loss: 567.583740234375\n",
      "Train Loss: 70.46115112304688\n",
      "Valid Loss: 432.24127197265625\n",
      "Train Loss: 69.9580078125\n",
      "Valid Loss: 575.316650390625\n",
      "Train Loss: 67.57009887695312\n",
      "Valid Loss: 731.1493530273438\n",
      "Train Loss: 71.32743072509766\n",
      "Valid Loss: 569.5318603515625\n",
      "Train Loss: 68.93289184570312\n",
      "Valid Loss: 736.016845703125\n",
      "Train Loss: 70.13075256347656\n",
      "Valid Loss: 517.2401123046875\n",
      "Train Loss: 70.07633972167969\n",
      "Valid Loss: 510.62322998046875\n",
      "Train Loss: 70.36231994628906\n",
      "Valid Loss: 735.2109375\n",
      "Train Loss: 69.73780822753906\n",
      "Valid Loss: 600.1347045898438\n",
      "Train Loss: 69.75127410888672\n",
      "Valid Loss: 455.473876953125\n",
      "Train Loss: 70.79303741455078\n",
      "Valid Loss: 543.2981567382812\n",
      "Train Loss: 70.04637145996094\n",
      "Valid Loss: 731.08642578125\n",
      "Train Loss: 70.87300872802734\n",
      "Valid Loss: 525.4666137695312\n",
      "Train Loss: 67.51998138427734\n",
      "Valid Loss: 736.3718872070312\n",
      "Train Loss: 69.37642669677734\n",
      "Valid Loss: 536.0958251953125\n",
      "Train Loss: 69.78401184082031\n",
      "Valid Loss: 435.0242919921875\n",
      "Train Loss: 70.01011657714844\n",
      "Valid Loss: 543.2150268554688\n",
      "Train Loss: 67.70494079589844\n",
      "Valid Loss: 599.92919921875\n",
      "Train Loss: 69.60651397705078\n",
      "Valid Loss: 731.8311767578125\n",
      "Train Loss: 68.44355773925781\n",
      "Valid Loss: 728.46875\n",
      "Train Loss: 70.79037475585938\n",
      "Valid Loss: 543.3579711914062\n",
      "Train Loss: 70.52156829833984\n",
      "Valid Loss: 435.0899963378906\n",
      "Train Loss: 67.53109741210938\n",
      "Valid Loss: 732.1097412109375\n",
      "Train Loss: 70.36659240722656\n",
      "Valid Loss: 531.8449096679688\n",
      "Train Loss: 68.7074966430664\n",
      "Valid Loss: 725.42578125\n",
      "Train Loss: 71.3377914428711\n",
      "Valid Loss: 567.6353149414062\n",
      "Train Loss: 69.33344268798828\n",
      "Valid Loss: 727.2255249023438\n",
      "Train Loss: 68.26543426513672\n",
      "Valid Loss: 566.95263671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 71.1082534790039\n",
      "Valid Loss: 732.4857177734375\n",
      "Train Loss: 69.59678649902344\n",
      "Valid Loss: 599.6680297851562\n",
      "Train Loss: 69.45436096191406\n",
      "Valid Loss: 730.3143920898438\n",
      "Train Loss: 69.72811126708984\n",
      "Valid Loss: 574.667236328125\n",
      "Train Loss: 69.89010620117188\n",
      "Valid Loss: 543.1509399414062\n",
      "Train Loss: 69.1679458618164\n",
      "Valid Loss: 567.3561401367188\n",
      "Min RMSE: 416.1861572265625\n"
     ]
    }
   ],
   "source": [
    "Train(1000, model, train_loader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb56e952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f81df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
