{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0369c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Native libraries\n",
    "import os\n",
    "import math\n",
    "# Essential Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fredapi import Fred\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LassoLarsCV, LassoLars, lasso_path, Lasso, LassoCV, LinearRegression\n",
    "# Algorithms\n",
    "from statistics import mean, stdev, variance\n",
    "from minisom import MiniSom\n",
    "from tslearn.barycenters import dtw_barycenter_averaging\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.cluster import KMeans, MeanShift, estimate_bandwidth\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.base.datetools import dates_from_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "703bb0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"monthly_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7d14fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fred = Fred(api_key='3c6ba58d26525f17af95af4fabed24be')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c88e827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv2 = csv[50:-20]\n",
    "csv2 = csv2.drop(['SMU06419406562200001.1', 'SMU21000006056170001SA.1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01deecf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNRATE</th>\n",
       "      <th>CPIAUCSL</th>\n",
       "      <th>FEDFUNDS</th>\n",
       "      <th>CSUSHPINSA</th>\n",
       "      <th>M2SL</th>\n",
       "      <th>M1SL</th>\n",
       "      <th>PSAVERT</th>\n",
       "      <th>PAYEMS</th>\n",
       "      <th>INDPRO</th>\n",
       "      <th>TB3MS</th>\n",
       "      <th>...</th>\n",
       "      <th>ESTPIEAMP01GPM</th>\n",
       "      <th>BROSERPA158MFRBDAL</th>\n",
       "      <th>LARTTULA158MFRBDAL</th>\n",
       "      <th>LAUCN310910000000004</th>\n",
       "      <th>LAUCN271470000000004</th>\n",
       "      <th>LFWATTFECAM647N</th>\n",
       "      <th>LFWA55FECAM647S</th>\n",
       "      <th>LFWA55FEKRM647S</th>\n",
       "      <th>LFWA55MAAUM647N</th>\n",
       "      <th>LFWA55MAAUM647S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.4</td>\n",
       "      <td>191.700</td>\n",
       "      <td>1.93</td>\n",
       "      <td>157.497</td>\n",
       "      <td>6399.4</td>\n",
       "      <td>1374.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>132503.0</td>\n",
       "      <td>97.6137</td>\n",
       "      <td>2.07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.202429</td>\n",
       "      <td>-1.713722</td>\n",
       "      <td>2.689583</td>\n",
       "      <td>12.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>12971900.0</td>\n",
       "      <td>1725500.0</td>\n",
       "      <td>2.143286e+06</td>\n",
       "      <td>1063251.0</td>\n",
       "      <td>1.062949e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.4</td>\n",
       "      <td>192.400</td>\n",
       "      <td>2.50</td>\n",
       "      <td>161.924</td>\n",
       "      <td>6432.5</td>\n",
       "      <td>1371.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>133032.0</td>\n",
       "      <td>99.4639</td>\n",
       "      <td>2.54</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201410</td>\n",
       "      <td>1.561165</td>\n",
       "      <td>16.658304</td>\n",
       "      <td>25.0</td>\n",
       "      <td>983.0</td>\n",
       "      <td>13010400.0</td>\n",
       "      <td>1745200.0</td>\n",
       "      <td>2.138804e+06</td>\n",
       "      <td>1072885.0</td>\n",
       "      <td>1.073819e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.1</td>\n",
       "      <td>193.600</td>\n",
       "      <td>3.00</td>\n",
       "      <td>169.544</td>\n",
       "      <td>6473.1</td>\n",
       "      <td>1366.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>133690.0</td>\n",
       "      <td>99.6033</td>\n",
       "      <td>2.84</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100604</td>\n",
       "      <td>1.090706</td>\n",
       "      <td>-4.866917</td>\n",
       "      <td>11.0</td>\n",
       "      <td>747.0</td>\n",
       "      <td>13057400.0</td>\n",
       "      <td>1763600.0</td>\n",
       "      <td>2.166473e+06</td>\n",
       "      <td>1081863.0</td>\n",
       "      <td>1.081815e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>196.100</td>\n",
       "      <td>3.50</td>\n",
       "      <td>175.922</td>\n",
       "      <td>6569.8</td>\n",
       "      <td>1377.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>134498.0</td>\n",
       "      <td>99.9435</td>\n",
       "      <td>3.44</td>\n",
       "      <td>...</td>\n",
       "      <td>1.206030</td>\n",
       "      <td>9.230125</td>\n",
       "      <td>-3.218818</td>\n",
       "      <td>9.0</td>\n",
       "      <td>726.0</td>\n",
       "      <td>13110300.0</td>\n",
       "      <td>1782100.0</td>\n",
       "      <td>2.175409e+06</td>\n",
       "      <td>1090390.0</td>\n",
       "      <td>1.090723e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>198.100</td>\n",
       "      <td>4.00</td>\n",
       "      <td>179.675</td>\n",
       "      <td>6654.5</td>\n",
       "      <td>1376.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>134993.0</td>\n",
       "      <td>100.3216</td>\n",
       "      <td>3.88</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.638735</td>\n",
       "      <td>6.844361</td>\n",
       "      <td>8.0</td>\n",
       "      <td>713.0</td>\n",
       "      <td>13152700.0</td>\n",
       "      <td>1798800.0</td>\n",
       "      <td>2.175877e+06</td>\n",
       "      <td>1099186.0</td>\n",
       "      <td>1.098971e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>3.7</td>\n",
       "      <td>256.118</td>\n",
       "      <td>2.13</td>\n",
       "      <td>211.802</td>\n",
       "      <td>14938.8</td>\n",
       "      <td>3844.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>151108.0</td>\n",
       "      <td>109.8543</td>\n",
       "      <td>1.95</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.223881</td>\n",
       "      <td>2.935424</td>\n",
       "      <td>-1.687233</td>\n",
       "      <td>13.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>15569800.0</td>\n",
       "      <td>2620100.0</td>\n",
       "      <td>3.998115e+06</td>\n",
       "      <td>1437108.0</td>\n",
       "      <td>1.438242e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>3.6</td>\n",
       "      <td>257.989</td>\n",
       "      <td>1.55</td>\n",
       "      <td>212.222</td>\n",
       "      <td>15254.4</td>\n",
       "      <td>3955.6</td>\n",
       "      <td>7.5</td>\n",
       "      <td>151758.0</td>\n",
       "      <td>110.0388</td>\n",
       "      <td>1.54</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150376</td>\n",
       "      <td>5.135172</td>\n",
       "      <td>-4.344529</td>\n",
       "      <td>11.0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>15624700.0</td>\n",
       "      <td>2625300.0</td>\n",
       "      <td>4.015310e+06</td>\n",
       "      <td>1444387.0</td>\n",
       "      <td>1.443890e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>3.5</td>\n",
       "      <td>258.824</td>\n",
       "      <td>1.58</td>\n",
       "      <td>213.295</td>\n",
       "      <td>15473.4</td>\n",
       "      <td>4027.6</td>\n",
       "      <td>8.3</td>\n",
       "      <td>152523.0</td>\n",
       "      <td>109.2966</td>\n",
       "      <td>1.52</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.452489</td>\n",
       "      <td>4.145019</td>\n",
       "      <td>2.584733</td>\n",
       "      <td>13.0</td>\n",
       "      <td>752.0</td>\n",
       "      <td>15663700.0</td>\n",
       "      <td>2629400.0</td>\n",
       "      <td>4.020187e+06</td>\n",
       "      <td>1450121.0</td>\n",
       "      <td>1.449740e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>13.3</td>\n",
       "      <td>255.942</td>\n",
       "      <td>0.05</td>\n",
       "      <td>218.569</td>\n",
       "      <td>17893.0</td>\n",
       "      <td>16275.9</td>\n",
       "      <td>24.7</td>\n",
       "      <td>132994.0</td>\n",
       "      <td>92.0613</td>\n",
       "      <td>0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152555</td>\n",
       "      <td>58.708814</td>\n",
       "      <td>23.355997</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1953.0</td>\n",
       "      <td>15693800.0</td>\n",
       "      <td>2633500.0</td>\n",
       "      <td>4.032882e+06</td>\n",
       "      <td>1453775.0</td>\n",
       "      <td>1.453747e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>8.4</td>\n",
       "      <td>259.511</td>\n",
       "      <td>0.10</td>\n",
       "      <td>224.139</td>\n",
       "      <td>18381.8</td>\n",
       "      <td>16906.0</td>\n",
       "      <td>14.6</td>\n",
       "      <td>141149.0</td>\n",
       "      <td>102.8885</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076161</td>\n",
       "      <td>22.084257</td>\n",
       "      <td>2.479332</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1073.0</td>\n",
       "      <td>15731700.0</td>\n",
       "      <td>2635800.0</td>\n",
       "      <td>4.053895e+06</td>\n",
       "      <td>1455757.0</td>\n",
       "      <td>1.456060e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 87827 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    UNRATE  CPIAUCSL  FEDFUNDS  CSUSHPINSA     M2SL     M1SL  PSAVERT  \\\n",
       "0      5.4   191.700      1.93     157.497   6399.4   1374.2      4.0   \n",
       "1      5.4   192.400      2.50     161.924   6432.5   1371.1      3.2   \n",
       "2      5.1   193.600      3.00     169.544   6473.1   1366.0      3.5   \n",
       "3      4.9   196.100      3.50     175.922   6569.8   1377.8      2.6   \n",
       "4      5.0   198.100      4.00     179.675   6654.5   1376.1      3.5   \n",
       "..     ...       ...       ...         ...      ...      ...      ...   \n",
       "59     3.7   256.118      2.13     211.802  14938.8   3844.0      7.3   \n",
       "60     3.6   257.989      1.55     212.222  15254.4   3955.6      7.5   \n",
       "61     3.5   258.824      1.58     213.295  15473.4   4027.6      8.3   \n",
       "62    13.3   255.942      0.05     218.569  17893.0  16275.9     24.7   \n",
       "63     8.4   259.511      0.10     224.139  18381.8  16906.0     14.6   \n",
       "\n",
       "      PAYEMS    INDPRO  TB3MS  ...  ESTPIEAMP01GPM  BROSERPA158MFRBDAL  \\\n",
       "0   132503.0   97.6137   2.07  ...       -0.202429           -1.713722   \n",
       "1   133032.0   99.4639   2.54  ...       -0.201410            1.561165   \n",
       "2   133690.0   99.6033   2.84  ...       -0.100604            1.090706   \n",
       "3   134498.0   99.9435   3.44  ...        1.206030            9.230125   \n",
       "4   134993.0  100.3216   3.88  ...        0.000000            2.638735   \n",
       "..       ...       ...    ...  ...             ...                 ...   \n",
       "59  151108.0  109.8543   1.95  ...       -0.223881            2.935424   \n",
       "60  151758.0  110.0388   1.54  ...        0.150376            5.135172   \n",
       "61  152523.0  109.2966   1.52  ...       -0.452489            4.145019   \n",
       "62  132994.0   92.0613   0.13  ...       -0.152555           58.708814   \n",
       "63  141149.0  102.8885   0.10  ...       -0.076161           22.084257   \n",
       "\n",
       "    LARTTULA158MFRBDAL  LAUCN310910000000004  LAUCN271470000000004  \\\n",
       "0             2.689583                  12.0                 712.0   \n",
       "1            16.658304                  25.0                 983.0   \n",
       "2            -4.866917                  11.0                 747.0   \n",
       "3            -3.218818                   9.0                 726.0   \n",
       "4             6.844361                   8.0                 713.0   \n",
       "..                 ...                   ...                   ...   \n",
       "59           -1.687233                  13.0                 642.0   \n",
       "60           -4.344529                  11.0                 505.0   \n",
       "61            2.584733                  13.0                 752.0   \n",
       "62           23.355997                  10.0                1953.0   \n",
       "63            2.479332                   8.0                1073.0   \n",
       "\n",
       "    LFWATTFECAM647N  LFWA55FECAM647S  LFWA55FEKRM647S  LFWA55MAAUM647N  \\\n",
       "0        12971900.0        1725500.0     2.143286e+06        1063251.0   \n",
       "1        13010400.0        1745200.0     2.138804e+06        1072885.0   \n",
       "2        13057400.0        1763600.0     2.166473e+06        1081863.0   \n",
       "3        13110300.0        1782100.0     2.175409e+06        1090390.0   \n",
       "4        13152700.0        1798800.0     2.175877e+06        1099186.0   \n",
       "..              ...              ...              ...              ...   \n",
       "59       15569800.0        2620100.0     3.998115e+06        1437108.0   \n",
       "60       15624700.0        2625300.0     4.015310e+06        1444387.0   \n",
       "61       15663700.0        2629400.0     4.020187e+06        1450121.0   \n",
       "62       15693800.0        2633500.0     4.032882e+06        1453775.0   \n",
       "63       15731700.0        2635800.0     4.053895e+06        1455757.0   \n",
       "\n",
       "    LFWA55MAAUM647S  \n",
       "0      1.062949e+06  \n",
       "1      1.073819e+06  \n",
       "2      1.081815e+06  \n",
       "3      1.090723e+06  \n",
       "4      1.098971e+06  \n",
       "..              ...  \n",
       "59     1.438242e+06  \n",
       "60     1.443890e+06  \n",
       "61     1.449740e+06  \n",
       "62     1.453747e+06  \n",
       "63     1.456060e+06  \n",
       "\n",
       "[64 rows x 87827 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = csv2.dropna(axis=\"columns\")[8::3].reset_index(drop=True)\n",
    "date = data['Unnamed: 0']\n",
    "data = data.drop([\"Unnamed: 0\"], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aef69c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>5438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>6056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>5773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>6236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sales\n",
       "0    1308\n",
       "1    1054\n",
       "2    1258\n",
       "3    1362\n",
       "4    1225\n",
       "..    ...\n",
       "60   4797\n",
       "61   5438\n",
       "62   6056\n",
       "63   5773\n",
       "64   6236\n",
       "\n",
       "[65 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.read_csv(\"micron.csv\").drop(\"date\", axis=1)[::-1].reset_index().drop(\"index\", axis=1)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "286567d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = data.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f58a136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = full\n",
    "scaler = MinMaxScaler()\n",
    "full = MinMaxScaler().fit_transform(full)\n",
    "som_x = som_y = math.ceil(math.sqrt(math.sqrt(len(full))))\n",
    "full = np.transpose(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f40a2a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOM clustering executed in 0.0m 5.60675835609436s \n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "som_x = som_y = math.ceil(math.sqrt(math.sqrt(len(full))))\n",
    "# I didn't see its significance but to make the map square,\n",
    "# I calculated square root of map size which is \n",
    "# the square root of the number of series\n",
    "# for the row and column counts of som\n",
    "\n",
    "som = MiniSom(som_x, som_y,len(full[0]), sigma=0.3, learning_rate = 0.1)\n",
    "\n",
    "som.random_weights_init(full)\n",
    "som.train(full, 50000)\n",
    "time_elapsed = time.time() - since\n",
    "print(f\"SOM clustering executed in {time_elapsed // 60}m {time_elapsed % 60}s \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79a65763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA clustering executed in 0.0m 0.36398911476135254s \n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "mySeries_transformed = pca.fit_transform(full)\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print(f\"PCA clustering executed in {time_elapsed // 60}m {time_elapsed % 60}s \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91bde7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n",
      "Kmeans on PCA clustering executed in 0.0m 38.45721125602722s \n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "cluster_count = math.ceil(math.sqrt(len(full))) \n",
    "print(cluster_count)\n",
    "kmeans = KMeans(n_clusters=60,max_iter=5000)\n",
    "\n",
    "labels = kmeans.fit_predict(mySeries_transformed)\n",
    "time_elapsed = time.time() - since\n",
    "print(f\"Kmeans on PCA clustering executed in {time_elapsed // 60}m {time_elapsed % 60}s \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39f32117",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_map = []\n",
    "i = 0\n",
    "cluster_x = []\n",
    "cluster_dict = {}\n",
    "for idx in names:\n",
    "    winner_node = som.winner(full[i])\n",
    "    name = f\"Cluster {winner_node[0] * som_y + winner_node[1] + 1}\"\n",
    "    cluster_map.append((idx, name))\n",
    "\n",
    "    cluster_x.append(name)\n",
    "    i += 1\n",
    "\n",
    "cluster_x = Counter(cluster_x)\n",
    "clusters = pd.DataFrame(cluster_map, columns=[\"Name\", \"Cluster\"]).sort_values(by=\"Cluster\").set_index(\"Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b532b2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WPS011101',\n",
       " 'WPU011101',\n",
       " 'MADI555SRVON',\n",
       " 'SMU15000006562200001SA',\n",
       " 'SMU17294044300000001SA',\n",
       " 'PCU5242105242104',\n",
       " 'SMU55333403000000030SA',\n",
       " 'SMU55333403000000030',\n",
       " 'SMU17294044300000001',\n",
       " 'SMU54000006562000007SA',\n",
       " 'SMU25731046055000001SA',\n",
       " 'CP0510MTM086NEST',\n",
       " 'SMU54000006562000007',\n",
       " 'SMU25716543000000030',\n",
       " 'SMU29000005000000008',\n",
       " 'MADI555SRVO',\n",
       " 'SMU15000006562200001',\n",
       " 'SMU25731046055000001',\n",
       " 'SMU25716543000000030SA']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = clusters.loc[\"sales\"].values\n",
    "value = clusters[\"Cluster\"].values\n",
    "index = list(np.where(value ==cluster)[0])\n",
    "names = list(clusters.index.values[index])\n",
    "names.remove(\"sales\")\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f4f4d0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c521f7c75b2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcorrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sales\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mcorr\u001b[0;34m(self, other, method, min_periods)\u001b[0m\n\u001b[1;32m   2331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"pearson\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"spearman\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kendall\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2333\u001b[0;31m             return nanops.nancorr(\n\u001b[0m\u001b[1;32m   2334\u001b[0m                 \u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_periods\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_periods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2335\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnancorr\u001b[0;34m(a, b, method, min_periods)\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_corr_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mfunc\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrcoef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcorrcoef\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mcorrcoef\u001b[0;34m(x, y, rowvar, bias, ddof)\u001b[0m\n\u001b[1;32m   2557\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2558\u001b[0m     \u001b[0mstddev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2559\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2560\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corrs = {}\n",
    "for name in data:\n",
    "    x = data[name]\n",
    "    \n",
    "    corrs[abs(x.corr(target[\"sales\"]))] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0397054f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53a499be",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for num in sorted(corrs)[:-40:-1]:\n",
    "    names.append(corrs[num])\n",
    "for num in sorted(corrs)[:10:1]:\n",
    "    names.append(corrs[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f0cd12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[names].iloc(axis=1)[np.where(clf.coef_ != 0)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62411cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = data.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55a30586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WPS011101</th>\n",
       "      <th>WPU011101</th>\n",
       "      <th>PCU5242105242104</th>\n",
       "      <th>SMU55333403000000030SA</th>\n",
       "      <th>CP0510MTM086NEST</th>\n",
       "      <th>SMU25716543000000030</th>\n",
       "      <th>SMU25716543000000030SA</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>155.5</td>\n",
       "      <td>149.3</td>\n",
       "      <td>100.2</td>\n",
       "      <td>705.442981</td>\n",
       "      <td>93.64</td>\n",
       "      <td>781.15</td>\n",
       "      <td>776.930845</td>\n",
       "      <td>1308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>153.1</td>\n",
       "      <td>121.9</td>\n",
       "      <td>100.2</td>\n",
       "      <td>687.649054</td>\n",
       "      <td>94.11</td>\n",
       "      <td>796.22</td>\n",
       "      <td>789.360768</td>\n",
       "      <td>1054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>157.2</td>\n",
       "      <td>150.1</td>\n",
       "      <td>100.2</td>\n",
       "      <td>683.784385</td>\n",
       "      <td>94.10</td>\n",
       "      <td>790.66</td>\n",
       "      <td>796.977835</td>\n",
       "      <td>1258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>125.8</td>\n",
       "      <td>154.7</td>\n",
       "      <td>100.2</td>\n",
       "      <td>679.273921</td>\n",
       "      <td>94.45</td>\n",
       "      <td>762.40</td>\n",
       "      <td>768.004777</td>\n",
       "      <td>1362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151.8</td>\n",
       "      <td>143.2</td>\n",
       "      <td>100.2</td>\n",
       "      <td>737.000379</td>\n",
       "      <td>94.58</td>\n",
       "      <td>738.40</td>\n",
       "      <td>734.249098</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>218.9</td>\n",
       "      <td>227.2</td>\n",
       "      <td>103.0</td>\n",
       "      <td>931.342190</td>\n",
       "      <td>117.12</td>\n",
       "      <td>1079.89</td>\n",
       "      <td>1075.492022</td>\n",
       "      <td>5144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>262.5</td>\n",
       "      <td>264.8</td>\n",
       "      <td>103.0</td>\n",
       "      <td>896.692752</td>\n",
       "      <td>116.80</td>\n",
       "      <td>1065.08</td>\n",
       "      <td>1062.832261</td>\n",
       "      <td>4797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>272.2</td>\n",
       "      <td>260.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>927.025729</td>\n",
       "      <td>115.80</td>\n",
       "      <td>1011.78</td>\n",
       "      <td>1011.961542</td>\n",
       "      <td>5438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>291.5</td>\n",
       "      <td>290.1</td>\n",
       "      <td>103.0</td>\n",
       "      <td>848.513674</td>\n",
       "      <td>115.99</td>\n",
       "      <td>1082.68</td>\n",
       "      <td>1072.706376</td>\n",
       "      <td>6056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>323.9</td>\n",
       "      <td>337.2</td>\n",
       "      <td>103.1</td>\n",
       "      <td>908.851030</td>\n",
       "      <td>114.49</td>\n",
       "      <td>1078.34</td>\n",
       "      <td>1076.370859</td>\n",
       "      <td>5773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    WPS011101  WPU011101  PCU5242105242104  SMU55333403000000030SA  \\\n",
       "0       155.5      149.3             100.2              705.442981   \n",
       "1       153.1      121.9             100.2              687.649054   \n",
       "2       157.2      150.1             100.2              683.784385   \n",
       "3       125.8      154.7             100.2              679.273921   \n",
       "4       151.8      143.2             100.2              737.000379   \n",
       "..        ...        ...               ...                     ...   \n",
       "59      218.9      227.2             103.0              931.342190   \n",
       "60      262.5      264.8             103.0              896.692752   \n",
       "61      272.2      260.0             103.0              927.025729   \n",
       "62      291.5      290.1             103.0              848.513674   \n",
       "63      323.9      337.2             103.1              908.851030   \n",
       "\n",
       "    CP0510MTM086NEST  SMU25716543000000030  SMU25716543000000030SA  sales  \n",
       "0              93.64                781.15              776.930845   1308  \n",
       "1              94.11                796.22              789.360768   1054  \n",
       "2              94.10                790.66              796.977835   1258  \n",
       "3              94.45                762.40              768.004777   1362  \n",
       "4              94.58                738.40              734.249098   1225  \n",
       "..               ...                   ...                     ...    ...  \n",
       "59            117.12               1079.89             1075.492022   5144  \n",
       "60            116.80               1065.08             1062.832261   4797  \n",
       "61            115.80               1011.78             1011.961542   5438  \n",
       "62            115.99               1082.68             1072.706376   6056  \n",
       "63            114.49               1078.34             1076.370859   5773  \n",
       "\n",
       "[64 rows x 8 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc580370",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = full.iloc[:,:-1],full.iloc[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a7620c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f1c00dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c43a790",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3,learning_rate = 0.2,\n",
    "                max_depth = 10, alpha = 5,n_estimators = 300, tree_method=\"gpu_hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48089956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:59:25] WARNING: ../src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(alpha=5, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=0,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.2, max_delta_step=0, max_depth=10,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=300, n_jobs=8, num_parallel_tree=1,\n",
       "             objective='reg:linear', random_state=0, reg_alpha=5, reg_lambda=1,\n",
       "             scale_pos_weight=1, subsample=1, tree_method='gpu_hist',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_reg.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f3b69ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xg_reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0591da6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 617.414054\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70a7267e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(normalize=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression(normalize=True)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1892241b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 544.266053\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eafd479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LassoCV(cv = 5, normalize=True, max_iter=200000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8330f43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  5,  6, 11, 13, 18])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(clf.coef_ != 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69ce57c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 19 is different from 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-aa5445f8ab62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RMSE: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \"\"\"\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         return safe_sparse_dot(X, self.coef_.T,\n\u001b[0m\u001b[1;32m    222\u001b[0m                                dense_output=True) + self.intercept_\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 19 is different from 7)"
     ]
    }
   ],
   "source": [
    "preds = clf.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3147c653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMU12000003200000030</th>\n",
       "      <th>SMU47349804300000001SA</th>\n",
       "      <th>SMU26000002023800008SA</th>\n",
       "      <th>SMU42383005552400001SA</th>\n",
       "      <th>CP0630TRM086NEST</th>\n",
       "      <th>DGDSRX1</th>\n",
       "      <th>SMU36000003000000030SA</th>\n",
       "      <th>SMU31000003231100001SA</th>\n",
       "      <th>SMU06000004340008901</th>\n",
       "      <th>DNXRLTSA</th>\n",
       "      <th>...</th>\n",
       "      <th>SMU34000003231100001SA</th>\n",
       "      <th>TURCPHPLA01IXOBM</th>\n",
       "      <th>SMU47000003231200001SA</th>\n",
       "      <th>SMU42979614300000001</th>\n",
       "      <th>IPG211112N</th>\n",
       "      <th>SMU24251804300000001</th>\n",
       "      <th>SMS04000004300000001</th>\n",
       "      <th>DEUSLRTTO02IXOBSAM</th>\n",
       "      <th>STOC706TRAD</th>\n",
       "      <th>SMU41414204300000001SA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>4049.160130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>533.736984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-405.373524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>3542.897697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>466.784070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-368.798469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>4587.979212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>575.098681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-400.801642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>4076.850187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>567.740613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-403.849563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>3481.917227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>473.385147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-365.750548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>3676.287729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>508.842048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-393.181839</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>3790.622103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490.527710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-414.517288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>3786.158971</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>488.201635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-394.705800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>3544.526274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>475.035941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-373.370351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>3541.619638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>467.196807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-362.702627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>3513.283049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>478.842345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-376.418272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>3497.493288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>454.341112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-361.178666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>3776.807643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>500.137550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-370.322430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 502 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    SMU12000003200000030  SMU47349804300000001SA  SMU26000002023800008SA  \\\n",
       "51                  -0.0                    -0.0             4049.160130   \n",
       "31                  -0.0                    -0.0             3542.897697   \n",
       "63                  -0.0                    -0.0             4587.979212   \n",
       "53                  -0.0                    -0.0             4076.850187   \n",
       "23                  -0.0                    -0.0             3481.917227   \n",
       "40                  -0.0                    -0.0             3676.287729   \n",
       "44                  -0.0                    -0.0             3790.622103   \n",
       "43                  -0.0                    -0.0             3786.158971   \n",
       "27                  -0.0                    -0.0             3544.526274   \n",
       "15                  -0.0                    -0.0             3541.619638   \n",
       "24                  -0.0                    -0.0             3513.283049   \n",
       "18                  -0.0                    -0.0             3497.493288   \n",
       "39                  -0.0                    -0.0             3776.807643   \n",
       "\n",
       "    SMU42383005552400001SA  CP0630TRM086NEST  DGDSRX1  SMU36000003000000030SA  \\\n",
       "51                     0.0              -0.0      0.0              533.736984   \n",
       "31                     0.0              -0.0      0.0              466.784070   \n",
       "63                     0.0              -0.0      0.0              575.098681   \n",
       "53                     0.0              -0.0      0.0              567.740613   \n",
       "23                     0.0              -0.0      0.0              473.385147   \n",
       "40                     0.0              -0.0      0.0              508.842048   \n",
       "44                     0.0              -0.0      0.0              490.527710   \n",
       "43                     0.0              -0.0      0.0              488.201635   \n",
       "27                     0.0              -0.0      0.0              475.035941   \n",
       "15                     0.0              -0.0      0.0              467.196807   \n",
       "24                     0.0              -0.0      0.0              478.842345   \n",
       "18                     0.0              -0.0      0.0              454.341112   \n",
       "39                     0.0              -0.0      0.0              500.137550   \n",
       "\n",
       "    SMU31000003231100001SA  SMU06000004340008901  DNXRLTSA  ...  \\\n",
       "51                     0.0                  -0.0       0.0  ...   \n",
       "31                     0.0                  -0.0       0.0  ...   \n",
       "63                     0.0                  -0.0       0.0  ...   \n",
       "53                     0.0                  -0.0       0.0  ...   \n",
       "23                     0.0                  -0.0       0.0  ...   \n",
       "40                     0.0                  -0.0       0.0  ...   \n",
       "44                     0.0                  -0.0       0.0  ...   \n",
       "43                     0.0                  -0.0       0.0  ...   \n",
       "27                     0.0                  -0.0       0.0  ...   \n",
       "15                     0.0                  -0.0       0.0  ...   \n",
       "24                     0.0                  -0.0       0.0  ...   \n",
       "18                     0.0                  -0.0       0.0  ...   \n",
       "39                     0.0                  -0.0       0.0  ...   \n",
       "\n",
       "    SMU34000003231100001SA  TURCPHPLA01IXOBM  SMU47000003231200001SA  \\\n",
       "51                     0.0              -0.0                     0.0   \n",
       "31                     0.0              -0.0                     0.0   \n",
       "63                     0.0              -0.0                     0.0   \n",
       "53                     0.0              -0.0                     0.0   \n",
       "23                     0.0              -0.0                     0.0   \n",
       "40                     0.0              -0.0                     0.0   \n",
       "44                     0.0              -0.0                     0.0   \n",
       "43                     0.0              -0.0                     0.0   \n",
       "27                     0.0              -0.0                     0.0   \n",
       "15                     0.0              -0.0                     0.0   \n",
       "24                     0.0              -0.0                     0.0   \n",
       "18                     0.0              -0.0                     0.0   \n",
       "39                     0.0              -0.0                     0.0   \n",
       "\n",
       "    SMU42979614300000001  IPG211112N  SMU24251804300000001  \\\n",
       "51           -405.373524         0.0                   0.0   \n",
       "31           -368.798469         0.0                   0.0   \n",
       "63           -400.801642         0.0                   0.0   \n",
       "53           -403.849563         0.0                   0.0   \n",
       "23           -365.750548         0.0                   0.0   \n",
       "40           -393.181839         0.0                   0.0   \n",
       "44           -414.517288         0.0                   0.0   \n",
       "43           -394.705800         0.0                   0.0   \n",
       "27           -373.370351         0.0                   0.0   \n",
       "15           -362.702627         0.0                   0.0   \n",
       "24           -376.418272         0.0                   0.0   \n",
       "18           -361.178666         0.0                   0.0   \n",
       "39           -370.322430         0.0                   0.0   \n",
       "\n",
       "    SMS04000004300000001  DEUSLRTTO02IXOBSAM  STOC706TRAD  \\\n",
       "51                  -0.0                -0.0          0.0   \n",
       "31                  -0.0                -0.0          0.0   \n",
       "63                  -0.0                -0.0          0.0   \n",
       "53                  -0.0                -0.0          0.0   \n",
       "23                  -0.0                -0.0          0.0   \n",
       "40                  -0.0                -0.0          0.0   \n",
       "44                  -0.0                -0.0          0.0   \n",
       "43                  -0.0                -0.0          0.0   \n",
       "27                  -0.0                -0.0          0.0   \n",
       "15                  -0.0                -0.0          0.0   \n",
       "24                  -0.0                -0.0          0.0   \n",
       "18                  -0.0                -0.0          0.0   \n",
       "39                  -0.0                -0.0          0.0   \n",
       "\n",
       "    SMU41414204300000001SA  \n",
       "51                    -0.0  \n",
       "31                    -0.0  \n",
       "63                    -0.0  \n",
       "53                    -0.0  \n",
       "23                    -0.0  \n",
       "40                    -0.0  \n",
       "44                    -0.0  \n",
       "43                    -0.0  \n",
       "27                    -0.0  \n",
       "15                    -0.0  \n",
       "24                    -0.0  \n",
       "18                    -0.0  \n",
       "39                    -0.0  \n",
       "\n",
       "[13 rows x 502 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf9cb443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 544.647758\n"
     ]
    }
   ],
   "source": [
    "model = Lasso(alpha = 0.29145927, normalize=True, max_iter=200000)\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test )\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46c136f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1468\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1469\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1470\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   3362\u001b[0m         \"\"\"\n\u001b[0;32m-> 3363\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3350\u001b[0;31m         new_data = self._mgr.take(\n\u001b[0m\u001b[1;32m   3351\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[1;32m   1439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1440\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexers.py\u001b[0m in \u001b[0;36mmaybe_convert_indices\u001b[0;34m(indices, n)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"indices are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: indices are out-of-bounds",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-b124e959d338>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1485\u001b[0m         \u001b[0;31m# a list of integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_list_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0;31m# a single integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m             \u001b[0;31m# re-raise with different error message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"positional indexers are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "X = X.iloc(axis=1)[np.where(clf.coef_ != 0)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a5b7ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d16fb09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwight/miniconda3/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:524: ValueWarning: No frequency information was provided, so inferred frequency QS-NOV will be used.\n",
      "  warnings.warn('No frequency information was'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  Summary of Regression Results   \n",
       "==================================\n",
       "Model:                         VAR\n",
       "Method:                        OLS\n",
       "Date:           Wed, 14, Jul, 2021\n",
       "Time:                     10:13:33\n",
       "--------------------------------------------------------------------\n",
       "No. of Equations:         8.00000    BIC:                   -500.470\n",
       "Nobs:                     56.0000    HQIC:                  -510.568\n",
       "Log likelihood:           14295.3    FPE:               6.88508e-216\n",
       "AIC:                     -516.962    Det(Omega_mle):    2.50487e-218\n",
       "--------------------------------------------------------------------\n",
       "Results for equation WPS011101\n",
       "============================================================================================\n",
       "                               coefficient       std. error           t-stat            prob\n",
       "--------------------------------------------------------------------------------------------\n",
       "const                            -0.015847         0.000000    -11128006.261           0.000\n",
       "L1.WPS011101                      0.704560         0.000000      5799163.065           0.000\n",
       "L1.WPU011101                      1.639739         0.000000      5383348.137           0.000\n",
       "L1.PCU5242105242104              -3.945413         0.000000    -17217440.858           0.000\n",
       "L1.SMU55333403000000030SA        -2.230232         0.000000    -25559622.731           0.000\n",
       "L1.CP0510MTM086NEST              -4.139651         0.000000    -10465109.957           0.000\n",
       "L1.SMU25716543000000030         -67.433220         0.000007     -9853773.192           0.000\n",
       "L1.SMU25716543000000030SA        63.303973         0.000007      9676333.542           0.000\n",
       "L1.sales                          2.739886         0.000000     17381299.993           0.000\n",
       "L2.WPS011101                    -18.941828         0.000002    -12040829.030           0.000\n",
       "L2.WPU011101                     18.296981         0.000002     11811557.508           0.000\n",
       "L2.PCU5242105242104             -25.359265         0.000000    -69607917.230           0.000\n",
       "L2.SMU55333403000000030SA         1.525993         0.000000      4736540.126           0.000\n",
       "L2.CP0510MTM086NEST               6.370542         0.000001     10047015.888           0.000\n",
       "L2.SMU25716543000000030          49.410010         0.000001     73219852.544           0.000\n",
       "L2.SMU25716543000000030SA       -51.774014         0.000000   -112319288.246           0.000\n",
       "L2.sales                         -3.248538         0.000000    -13859555.538           0.000\n",
       "L3.WPS011101                     -6.479302         0.000001     -8822098.160           0.000\n",
       "L3.WPU011101                      8.854845         0.000001     10107177.626           0.000\n",
       "L3.PCU5242105242104               7.260039         0.000000     58976207.805           0.000\n",
       "L3.SMU55333403000000030SA        -2.556610         0.000000     -8960487.651           0.000\n",
       "L3.CP0510MTM086NEST             -16.138763         0.000001    -16167944.533           0.000\n",
       "L3.SMU25716543000000030          -1.671509         0.000007      -237564.103           0.000\n",
       "L3.SMU25716543000000030SA         2.552696         0.000007       352690.176           0.000\n",
       "L3.sales                          1.619339         0.000000     18699819.889           0.000\n",
       "L4.WPS011101                    -25.900356         0.000001    -18226703.286           0.000\n",
       "L4.WPU011101                     24.224924         0.000001     18611611.845           0.000\n",
       "L4.PCU5242105242104             -30.184183         0.000002    -13788034.195           0.000\n",
       "L4.SMU55333403000000030SA         3.376388         0.000000     36969765.547           0.000\n",
       "L4.CP0510MTM086NEST               7.843859         0.000000     15792197.721           0.000\n",
       "L4.SMU25716543000000030          25.938875         0.000011      2425020.211           0.000\n",
       "L4.SMU25716543000000030SA       -25.390620         0.000011     -2360016.970           0.000\n",
       "L4.sales                         -2.071794         0.000000    -26059108.059           0.000\n",
       "L5.WPS011101                     -7.190136         0.000000    -49846601.777           0.000\n",
       "L5.WPU011101                      9.217270         0.000000     35525808.845           0.000\n",
       "L5.PCU5242105242104              11.951522         0.000004      3375689.744           0.000\n",
       "L5.SMU55333403000000030SA        -4.399047         0.000000    -18675292.019           0.000\n",
       "L5.CP0510MTM086NEST               2.433593         0.000000      5950950.670           0.000\n",
       "L5.SMU25716543000000030         101.907280         0.000004     28079144.809           0.000\n",
       "L5.SMU25716543000000030SA      -102.231511         0.000003    -30032593.056           0.000\n",
       "L5.sales                          2.253468         0.000000     22506857.255           0.000\n",
       "L6.WPS011101                     -9.466263         0.000000  -1146314214.770           0.000\n",
       "L6.WPU011101                      8.720303         0.000000    851416152.337           0.000\n",
       "L6.PCU5242105242104               1.671243         0.000001      1718480.925           0.000\n",
       "L6.SMU55333403000000030SA         2.903900         0.000000     12791183.566           0.000\n",
       "L6.CP0510MTM086NEST               5.243297         0.000000     12400503.164           0.000\n",
       "L6.SMU25716543000000030         -19.518940         0.000010     -1961516.365           0.000\n",
       "L6.SMU25716543000000030SA        18.235976         0.000010      1843473.045           0.000\n",
       "L6.sales                         -0.352162         0.000000    -10197868.353           0.000\n",
       "L7.WPS011101                     -1.876882         0.000000     -6120658.304           0.000\n",
       "L7.WPU011101                      3.810243         0.000000     20935502.613           0.000\n",
       "L7.PCU5242105242104             -18.676506         0.000001    -14430185.906           0.000\n",
       "L7.SMU55333403000000030SA        -1.116768         0.000000    -11367465.392           0.000\n",
       "L7.CP0510MTM086NEST              -0.725281         0.000000    -19085124.682           0.000\n",
       "L7.SMU25716543000000030          52.793080         0.000003     20792975.527           0.000\n",
       "L7.SMU25716543000000030SA       -52.676109         0.000003    -19900441.683           0.000\n",
       "L7.sales                         -1.099648         0.000000    -12158440.657           0.000\n",
       "============================================================================================\n",
       "\n",
       "Results for equation WPU011101\n",
       "============================================================================================\n",
       "                               coefficient       std. error           t-stat            prob\n",
       "--------------------------------------------------------------------------------------------\n",
       "const                            -0.014955         0.000000     -9929168.298           0.000\n",
       "L1.WPS011101                      2.301912         0.000000     17914006.785           0.000\n",
       "L1.WPU011101                     -0.147430         0.000000      -457634.743           0.000\n",
       "L1.PCU5242105242104              -5.389009         0.000000    -22235232.733           0.000\n",
       "L1.SMU55333403000000030SA        -2.034994         0.000000    -22050782.174           0.000\n",
       "L1.CP0510MTM086NEST              -4.163461         0.000000     -9951558.074           0.000\n",
       "L1.SMU25716543000000030         -69.445165         0.000007     -9594608.359           0.000\n",
       "L1.SMU25716543000000030SA        65.843000         0.000007      9515816.356           0.000\n",
       "L1.sales                          2.663173         0.000000     15973706.221           0.000\n",
       "L2.WPS011101                    -16.642833         0.000002    -10002724.697           0.000\n",
       "L2.WPU011101                     16.026022         0.000002      9781602.951           0.000\n",
       "L2.PCU5242105242104             -23.446621         0.000000    -60849751.931           0.000\n",
       "L2.SMU55333403000000030SA         1.771457         0.000000      5198714.245           0.000\n",
       "L2.CP0510MTM086NEST               6.042337         0.000001      9009946.284           0.000\n",
       "L2.SMU25716543000000030          48.731939         0.000001     68278530.768           0.000\n",
       "L2.SMU25716543000000030SA       -51.097219         0.000000   -104808460.561           0.000\n",
       "L2.sales                         -3.090285         0.000000    -12465694.673           0.000\n",
       "L3.WPS011101                     -5.437496         0.000001     -7000018.633           0.000\n",
       "L3.WPU011101                      7.760497         0.000001      8375197.011           0.000\n",
       "L3.PCU5242105242104               6.578298         0.000000     50525198.284           0.000\n",
       "L3.SMU55333403000000030SA        -2.709401         0.000000     -8978360.088           0.000\n",
       "L3.CP0510MTM086NEST             -15.192890         0.000001    -14390686.709           0.000\n",
       "L3.SMU25716543000000030          -3.805103         0.000007      -511322.589           0.000\n",
       "L3.SMU25716543000000030SA         4.784724         0.000008       625039.845           0.000\n",
       "L3.sales                          1.482211         0.000000     16183266.323           0.000\n",
       "L4.WPS011101                    -24.309275         0.000002    -16174503.570           0.000\n",
       "L4.WPU011101                     22.702541         0.000001     16491213.049           0.000\n",
       "L4.PCU5242105242104             -31.703186         0.000002    -13692489.605           0.000\n",
       "L4.SMU55333403000000030SA         3.139098         0.000000     32497946.095           0.000\n",
       "L4.CP0510MTM086NEST               6.344416         0.000001     12077054.769           0.000\n",
       "L4.SMU25716543000000030          25.316392         0.000011      2237806.883           0.000\n",
       "L4.SMU25716543000000030SA       -24.569571         0.000011     -2159215.380           0.000\n",
       "L4.sales                         -1.927619         0.000000    -22924025.946           0.000\n",
       "L5.WPS011101                     -7.325552         0.000000    -48017042.193           0.000\n",
       "L5.WPU011101                      9.173308         0.000000     33429063.141           0.000\n",
       "L5.PCU5242105242104              13.515442         0.000004      3609326.243           0.000\n",
       "L5.SMU55333403000000030SA        -3.862309         0.000000    -15502881.554           0.000\n",
       "L5.CP0510MTM086NEST               2.073000         0.000000      4792855.635           0.000\n",
       "L5.SMU25716543000000030         104.088470         0.000004     27116764.500           0.000\n",
       "L5.SMU25716543000000030SA      -104.156362         0.000004    -28930133.072           0.000\n",
       "L5.sales                          2.084760         0.000000     19686840.084           0.000\n",
       "L6.WPS011101                     -9.200111         0.000000  -1053355063.301           0.000\n",
       "L6.WPU011101                      8.491225         0.000000    783857706.782           0.000\n",
       "L6.PCU5242105242104               4.274828         0.000001      4156044.628           0.000\n",
       "L6.SMU55333403000000030SA         2.809430         0.000000     11700485.387           0.000\n",
       "L6.CP0510MTM086NEST               4.575709         0.000000     10231747.760           0.000\n",
       "L6.SMU25716543000000030         -17.019983         0.000011     -1617154.050           0.000\n",
       "L6.SMU25716543000000030SA        15.795285         0.000010      1509704.033           0.000\n",
       "L6.sales                         -0.309617         0.000000     -8477115.247           0.000\n",
       "L7.WPS011101                     -1.142616         0.000000     -3523041.835           0.000\n",
       "L7.WPU011101                      3.007739         0.000000     15625266.071           0.000\n",
       "L7.PCU5242105242104             -17.287753         0.000001    -12629071.373           0.000\n",
       "L7.SMU55333403000000030SA        -1.087062         0.000000    -10461922.246           0.000\n",
       "L7.CP0510MTM086NEST              -0.508304         0.000000    -12646447.917           0.000\n",
       "L7.SMU25716543000000030          54.303506         0.000003     20221999.038           0.000\n",
       "L7.SMU25716543000000030SA       -54.148454         0.000003    -19341567.043           0.000\n",
       "L7.sales                         -1.014570         0.000000    -10606273.187           0.000\n",
       "============================================================================================\n",
       "\n",
       "Results for equation PCU5242105242104\n",
       "============================================================================================\n",
       "                               coefficient       std. error           t-stat            prob\n",
       "--------------------------------------------------------------------------------------------\n",
       "const                            -0.000202         0.000000     -6302938.601           0.000\n",
       "L1.WPS011101                      0.240090         0.000000     87646596.618           0.000\n",
       "L1.WPU011101                     -0.232755         0.000000    -33891481.348           0.000\n",
       "L1.PCU5242105242104              -0.614233         0.000000   -118883638.577           0.000\n",
       "L1.SMU55333403000000030SA         0.130342         0.000000     66252192.549           0.000\n",
       "L1.CP0510MTM086NEST              -0.415156         0.000000    -46548396.077           0.000\n",
       "L1.SMU25716543000000030          -1.168885         0.000000     -7575540.307           0.000\n",
       "L1.SMU25716543000000030SA         1.314822         0.000000      8913737.775           0.000\n",
       "L1.sales                          0.018622         0.000000      5239405.932           0.000\n",
       "L2.WPS011101                     -0.247558         0.000000     -6979512.304           0.000\n",
       "L2.WPU011101                      0.255599         0.000000      7318118.856           0.000\n",
       "L2.PCU5242105242104               0.777688         0.000000     94676067.272           0.000\n",
       "L2.SMU55333403000000030SA         0.148831         0.000000     20488691.989           0.000\n",
       "L2.CP0510MTM086NEST               0.000184         0.000000        12889.641           0.000\n",
       "L2.SMU25716543000000030           1.015420         0.000000     66737938.972           0.000\n",
       "L2.SMU25716543000000030SA        -1.142103         0.000000   -109890657.187           0.000\n",
       "L2.sales                         -0.048301         0.000000     -9139684.597           0.000\n",
       "L3.WPS011101                     -0.292660         0.000000    -17673380.525           0.000\n",
       "L3.WPU011101                      0.323338         0.000000     16368850.847           0.000\n",
       "L3.PCU5242105242104              -0.284186         0.000000   -102389045.341           0.000\n",
       "L3.SMU55333403000000030SA        -0.035426         0.000000     -5506913.553           0.000\n",
       "L3.CP0510MTM086NEST              -0.064945         0.000000     -2885660.306           0.000\n",
       "L3.SMU25716543000000030          -0.770728         0.000000     -4858319.053           0.000\n",
       "L3.SMU25716543000000030SA         0.846158         0.000000      5185119.612           0.000\n",
       "L3.sales                          0.008428         0.000000      4316417.641           0.000\n",
       "L4.WPS011101                     -0.064176         0.000000     -2003047.982           0.000\n",
       "L4.WPU011101                      0.054633         0.000000      1861602.798           0.000\n",
       "L4.PCU5242105242104              -0.908960         0.000000    -18415383.211           0.000\n",
       "L4.SMU55333403000000030SA         0.071173         0.000000     34563763.018           0.000\n",
       "L4.CP0510MTM086NEST              -0.176296         0.000000    -15742331.308           0.000\n",
       "L4.SMU25716543000000030           0.089076         0.000000       369350.456           0.000\n",
       "L4.SMU25716543000000030SA         0.031019         0.000000       127873.016           0.000\n",
       "L4.sales                          0.016466         0.000000      9185865.338           0.000\n",
       "L5.WPS011101                     -0.314367         0.000000    -96660388.519           0.000\n",
       "L5.WPU011101                      0.295489         0.000000     50512096.721           0.000\n",
       "L5.PCU5242105242104               0.775213         0.000000      9711207.780           0.000\n",
       "L5.SMU55333403000000030SA         0.088611         0.000000     16684401.045           0.000\n",
       "L5.CP0510MTM086NEST               0.058487         0.000000      6343258.231           0.000\n",
       "L5.SMU25716543000000030           2.386857         0.000000     29168770.513           0.000\n",
       "L5.SMU25716543000000030SA        -2.392008         0.000000    -31166189.036           0.000\n",
       "L5.sales                         -0.008133         0.000000     -3602567.225           0.000\n",
       "L6.WPS011101                      0.129247         0.000000    694159473.727           0.000\n",
       "L6.WPU011101                     -0.129668         0.000000   -561510094.901           0.000\n",
       "L6.PCU5242105242104               0.274586         0.000000     12522675.796           0.000\n",
       "L6.SMU55333403000000030SA        -0.071661         0.000000    -13999856.719           0.000\n",
       "L6.CP0510MTM086NEST              -0.110473         0.000000    -11587854.388           0.000\n",
       "L6.SMU25716543000000030           0.413420         0.000000      1842639.087           0.000\n",
       "L6.SMU25716543000000030SA        -0.505448         0.000000     -2266195.052           0.000\n",
       "L6.sales                         -0.026463         0.000000    -33987944.012           0.000\n",
       "L7.WPS011101                      0.321720         0.000000     46532101.541           0.000\n",
       "L7.WPU011101                     -0.328560         0.000000    -80067940.580           0.000\n",
       "L7.PCU5242105242104               0.303448         0.000000     10398584.488           0.000\n",
       "L7.SMU55333403000000030SA         0.058293         0.000000     26316411.666           0.000\n",
       "L7.CP0510MTM086NEST               0.243273         0.000000    283919475.154           0.000\n",
       "L7.SMU25716543000000030           1.304316         0.000000     22784293.761           0.000\n",
       "L7.SMU25716543000000030SA        -1.277615         0.000000    -21407300.816           0.000\n",
       "L7.sales                          0.005830         0.000000      2858968.120           0.000\n",
       "============================================================================================\n",
       "\n",
       "Results for equation SMU55333403000000030SA\n",
       "============================================================================================\n",
       "                               coefficient       std. error           t-stat            prob\n",
       "--------------------------------------------------------------------------------------------\n",
       "const                            -0.005820         0.000000     -9509293.318           0.000\n",
       "L1.WPS011101                      2.398771         0.000000     45938233.066           0.000\n",
       "L1.WPU011101                     -2.344086         0.000000    -17905618.703           0.000\n",
       "L1.PCU5242105242104              -6.357054         0.000000    -64546115.128           0.000\n",
       "L1.SMU55333403000000030SA        -0.241752         0.000000     -6446334.187           0.000\n",
       "L1.CP0510MTM086NEST              -1.707878         0.000000    -10045577.327           0.000\n",
       "L1.SMU25716543000000030         -16.984888         0.000003     -5774699.617           0.000\n",
       "L1.SMU25716543000000030SA        17.994567         0.000003      6399695.483           0.000\n",
       "L1.sales                          0.055373         0.000000       817314.583           0.000\n",
       "L2.WPS011101                     -0.782362         0.000001     -1157124.561           0.000\n",
       "L2.WPU011101                      0.771914         0.000001      1159404.196           0.000\n",
       "L2.PCU5242105242104               6.300063         0.000000     40235084.761           0.000\n",
       "L2.SMU55333403000000030SA         1.282111         0.000000      9259176.240           0.000\n",
       "L2.CP0510MTM086NEST               0.533774         0.000000      1958647.589           0.000\n",
       "L2.SMU25716543000000030           7.640926         0.000000     26344983.705           0.000\n",
       "L2.SMU25716543000000030SA        -8.136885         0.000000    -41071326.681           0.000\n",
       "L2.sales                         -0.110000         0.000000     -1091920.953           0.000\n",
       "L3.WPS011101                     -1.801376         0.000000     -5706715.023           0.000\n",
       "L3.WPU011101                      2.040746         0.000000      5419709.751           0.000\n",
       "L3.PCU5242105242104               0.841823         0.000000     15910966.073           0.000\n",
       "L3.SMU55333403000000030SA        -0.166516         0.000000     -1357875.865           0.000\n",
       "L3.CP0510MTM086NEST              -0.091833         0.000000      -214054.000           0.000\n",
       "L3.SMU25716543000000030          -1.222157         0.000003      -404144.967           0.000\n",
       "L3.SMU25716543000000030SA         1.556558         0.000003       500377.206           0.000\n",
       "L3.sales                         -0.071537         0.000000     -1922080.104           0.000\n",
       "L4.WPS011101                     -1.769063         0.000001     -2896568.786           0.000\n",
       "L4.WPU011101                      1.752927         0.000001      3133455.665           0.000\n",
       "L4.PCU5242105242104              -9.418987         0.000001    -10010717.109           0.000\n",
       "L4.SMU55333403000000030SA         0.023239         0.000000       592037.987           0.000\n",
       "L4.CP0510MTM086NEST              -1.550218         0.000000     -7261787.641           0.000\n",
       "L4.SMU25716543000000030           3.467822         0.000005       754326.065           0.000\n",
       "L4.SMU25716543000000030SA        -3.279271         0.000005      -709181.137           0.000\n",
       "L4.sales                          0.077514         0.000000      2268460.905           0.000\n",
       "L5.WPS011101                     -3.718724         0.000000    -59983317.759           0.000\n",
       "L5.WPU011101                      3.674836         0.000000     32954721.290           0.000\n",
       "L5.PCU5242105242104               5.235036         0.000002      3440305.429           0.000\n",
       "L5.SMU55333403000000030SA         0.190969         0.000000      1886291.606           0.000\n",
       "L5.CP0510MTM086NEST               0.821336         0.000000      4673015.116           0.000\n",
       "L5.SMU25716543000000030          21.661976         0.000002     13887202.800           0.000\n",
       "L5.SMU25716543000000030SA       -21.252472         0.000001    -14526318.490           0.000\n",
       "L5.sales                         -0.031255         0.000000      -726300.526           0.000\n",
       "L6.WPS011101                     -1.473189         0.000000   -415070263.233           0.000\n",
       "L6.WPU011101                      1.465413         0.000000    332895950.585           0.000\n",
       "L6.PCU5242105242104               1.887116         0.000000      4514833.482           0.000\n",
       "L6.SMU55333403000000030SA        -0.333363         0.000000     -3416520.574           0.000\n",
       "L6.CP0510MTM086NEST              -1.061860         0.000000     -5843057.436           0.000\n",
       "L6.SMU25716543000000030           1.012891         0.000004       236829.799           0.000\n",
       "L6.SMU25716543000000030SA        -1.019545         0.000004      -239801.664           0.000\n",
       "L6.sales                         -0.015400         0.000000     -1037576.326           0.000\n",
       "L7.WPS011101                      0.409960         0.000000      3110577.790           0.000\n",
       "L7.WPU011101                     -0.457491         0.000000     -5848596.909           0.000\n",
       "L7.PCU5242105242104               1.809318         0.000001      3252590.227           0.000\n",
       "L7.SMU55333403000000030SA         0.408323         0.000000      9670356.691           0.000\n",
       "L7.CP0510MTM086NEST               1.080316         0.000000     66142026.843           0.000\n",
       "L7.SMU25716543000000030           9.558755         0.000001      8759494.238           0.000\n",
       "L7.SMU25716543000000030SA        -9.987381         0.000001     -8778871.144           0.000\n",
       "L7.sales                          0.010530         0.000000       270877.812           0.000\n",
       "============================================================================================\n",
       "\n",
       "Results for equation CP0510MTM086NEST\n",
       "============================================================================================\n",
       "                               coefficient       std. error           t-stat            prob\n",
       "--------------------------------------------------------------------------------------------\n",
       "const                            -0.000491         0.000000     -2206941.386           0.000\n",
       "L1.WPS011101                     -0.933046         0.000000    -49114462.230           0.000\n",
       "L1.WPU011101                      1.167432         0.000000     24511399.284           0.000\n",
       "L1.PCU5242105242104              -0.335530         0.000000     -9364109.640           0.000\n",
       "L1.SMU55333403000000030SA         0.263477         0.000000     19311036.174           0.000\n",
       "L1.CP0510MTM086NEST               0.045942         0.000000       742762.555           0.000\n",
       "L1.SMU25716543000000030          -3.656156         0.000001     -3416743.268           0.000\n",
       "L1.SMU25716543000000030SA         2.990620         0.000001      2923479.340           0.000\n",
       "L1.sales                          0.258835         0.000000     10501006.247           0.000\n",
       "L2.WPS011101                     -3.407766         0.000000    -13853617.420           0.000\n",
       "L2.WPU011101                      3.379284         0.000000     13951193.845           0.000\n",
       "L2.PCU5242105242104              -1.493928         0.000000    -26224710.229           0.000\n",
       "L2.SMU55333403000000030SA         0.423768         0.000000      8411926.810           0.000\n",
       "L2.CP0510MTM086NEST               0.700739         0.000000      7067662.116           0.000\n",
       "L2.SMU25716543000000030          10.587111         0.000000    100334513.839           0.000\n",
       "L2.SMU25716543000000030SA       -10.858550         0.000000   -150651451.468           0.000\n",
       "L2.sales                         -0.370381         0.000000    -10105758.617           0.000\n",
       "L3.WPS011101                     -1.777559         0.000000    -15478417.582           0.000\n",
       "L3.WPU011101                      2.008913         0.000000     14664558.663           0.000\n",
       "L3.PCU5242105242104              -0.721530         0.000000    -37484470.066           0.000\n",
       "L3.SMU55333403000000030SA        -0.447430         0.000000    -10028858.575           0.000\n",
       "L3.CP0510MTM086NEST              -1.891710         0.000000    -12119873.962           0.000\n",
       "L3.SMU25716543000000030           1.965846         0.000001      1786818.389           0.000\n",
       "L3.SMU25716543000000030SA        -1.863731         0.000001     -1646782.896           0.000\n",
       "L3.sales                          0.226023         0.000000     16692096.420           0.000\n",
       "L4.WPS011101                     -3.188662         0.000000    -14350592.996           0.000\n",
       "L4.WPU011101                      3.000155         0.000000     14740909.932           0.000\n",
       "L4.PCU5242105242104              -4.091678         0.000000    -11953176.639           0.000\n",
       "L4.SMU55333403000000030SA         0.363952         0.000000     25485774.736           0.000\n",
       "L4.CP0510MTM086NEST               1.170665         0.000000     15073170.954           0.000\n",
       "L4.SMU25716543000000030           3.159446         0.000002      1889009.281           0.000\n",
       "L4.SMU25716543000000030SA        -3.349792         0.000002     -1991217.483           0.000\n",
       "L4.sales                         -0.258082         0.000000    -20760136.340           0.000\n",
       "L5.WPS011101                     -0.393559         0.000000    -17448863.449           0.000\n",
       "L5.WPU011101                      0.589740         0.000000     14536550.753           0.000\n",
       "L5.PCU5242105242104               3.664475         0.000001      6619269.916           0.000\n",
       "L5.SMU55333403000000030SA        -0.328223         0.000000     -8911207.078           0.000\n",
       "L5.CP0510MTM086NEST               0.627983         0.000000      9820761.346           0.000\n",
       "L5.SMU25716543000000030           9.579104         0.000001     16879615.572           0.000\n",
       "L5.SMU25716543000000030SA        -9.892574         0.000001    -18585589.774           0.000\n",
       "L5.sales                          0.302728         0.000000     19336362.617           0.000\n",
       "L6.WPS011101                     -0.138128         0.000000   -106970603.002           0.000\n",
       "L6.WPU011101                      0.069047         0.000000     43113559.713           0.000\n",
       "L6.PCU5242105242104              -1.369986         0.000000     -9009073.266           0.000\n",
       "L6.SMU55333403000000030SA         0.610269         0.000000     17191317.563           0.000\n",
       "L6.CP0510MTM086NEST               1.052705         0.000000     15922113.008           0.000\n",
       "L6.SMU25716543000000030          -5.173043         0.000002     -3324610.240           0.000\n",
       "L6.SMU25716543000000030SA         5.108441         0.000002      3302593.791           0.000\n",
       "L6.sales                         -0.111902         0.000000    -20723450.338           0.000\n",
       "L7.WPS011101                      0.493901         0.000000     10300541.604           0.000\n",
       "L7.WPU011101                     -0.263009         0.000000     -9241869.022           0.000\n",
       "L7.PCU5242105242104              -4.525084         0.000000    -22359491.300           0.000\n",
       "L7.SMU55333403000000030SA        -0.220364         0.000000    -14344998.639           0.000\n",
       "L7.CP0510MTM086NEST              -0.422330         0.000000    -71072212.188           0.000\n",
       "L7.SMU25716543000000030           4.357303         0.000000     10975300.366           0.000\n",
       "L7.SMU25716543000000030SA        -4.635001         0.000000    -11198448.870           0.000\n",
       "L7.sales                         -0.095572         0.000000     -6757932.714           0.000\n",
       "============================================================================================\n",
       "\n",
       "Results for equation SMU25716543000000030\n",
       "============================================================================================\n",
       "                               coefficient       std. error           t-stat            prob\n",
       "--------------------------------------------------------------------------------------------\n",
       "const                             0.004247         0.000000      6730830.940           0.000\n",
       "L1.WPS011101                     -0.376396         0.000000     -6992040.937           0.000\n",
       "L1.WPU011101                      0.979719         0.000000      7259234.958           0.000\n",
       "L1.PCU5242105242104             -10.207337         0.000000   -100531072.228           0.000\n",
       "L1.SMU55333403000000030SA         0.869214         0.000000     22482411.131           0.000\n",
       "L1.CP0510MTM086NEST              -1.215449         0.000000     -6934712.092           0.000\n",
       "L1.SMU25716543000000030         -19.924446         0.000003     -6570926.648           0.000\n",
       "L1.SMU25716543000000030SA        19.013554         0.000003      6559260.360           0.000\n",
       "L1.sales                          0.733917         0.000000     10507716.650           0.000\n",
       "L2.WPS011101                     -6.517587         0.000001     -9350461.338           0.000\n",
       "L2.WPU011101                      6.313497         0.000001      9198338.852           0.000\n",
       "L2.PCU5242105242104              -5.094564         0.000000    -31560269.136           0.000\n",
       "L2.SMU55333403000000030SA         1.847684         0.000000     12943391.886           0.000\n",
       "L2.CP0510MTM086NEST               0.455271         0.000000      1620476.544           0.000\n",
       "L2.SMU25716543000000030          41.813839         0.000000    139844582.165           0.000\n",
       "L2.SMU25716543000000030SA       -42.726473         0.000000   -209194972.727           0.000\n",
       "L2.sales                         -0.916243         0.000000     -8822341.050           0.000\n",
       "L3.WPS011101                     -3.920309         0.000000    -12046908.737           0.000\n",
       "L3.WPU011101                      4.549190         0.000000     11719111.297           0.000\n",
       "L3.PCU5242105242104              -3.900896         0.000000    -71517768.662           0.000\n",
       "L3.SMU55333403000000030SA        -1.969920         0.000000    -15582153.979           0.000\n",
       "L3.CP0510MTM086NEST              -4.655914         0.000000    -10526917.391           0.000\n",
       "L3.SMU25716543000000030           2.774800         0.000003       890052.149           0.000\n",
       "L3.SMU25716543000000030SA        -2.435556         0.000003      -759458.404           0.000\n",
       "L3.sales                          0.296438         0.000000      7725825.202           0.000\n",
       "L4.WPS011101                     -6.680329         0.000001    -10609915.631           0.000\n",
       "L4.WPU011101                      6.267665         0.000001     10867734.497           0.000\n",
       "L4.PCU5242105242104             -13.063651         0.000001    -13467879.249           0.000\n",
       "L4.SMU55333403000000030SA         0.528951         0.000000     13071391.302           0.000\n",
       "L4.CP0510MTM086NEST               1.127696         0.000000      5124089.581           0.000\n",
       "L4.SMU25716543000000030          13.342732         0.000005      2815274.116           0.000\n",
       "L4.SMU25716543000000030SA       -13.518499         0.000005     -2835841.637           0.000\n",
       "L4.sales                         -0.350521         0.000000     -9950359.716           0.000\n",
       "L5.WPS011101                     -1.902826         0.000000    -29772087.824           0.000\n",
       "L5.WPU011101                      2.404221         0.000000     20913545.411           0.000\n",
       "L5.PCU5242105242104              15.124660         0.000002      9641322.940           0.000\n",
       "L5.SMU55333403000000030SA         0.152808         0.000000      1464082.501           0.000\n",
       "L5.CP0510MTM086NEST               2.398296         0.000000     13235885.198           0.000\n",
       "L5.SMU25716543000000030          36.865424         0.000002     22925013.597           0.000\n",
       "L5.SMU25716543000000030SA       -36.628799         0.000002    -24285241.448           0.000\n",
       "L5.sales                          0.564000         0.000000     12713192.346           0.000\n",
       "L6.WPS011101                     -2.091962         0.000000   -571729456.595           0.000\n",
       "L6.WPU011101                      1.956097         0.000000    431034774.457           0.000\n",
       "L6.PCU5242105242104              -4.119291         0.000000     -9559589.147           0.000\n",
       "L6.SMU55333403000000030SA         1.495312         0.000000     14865257.156           0.000\n",
       "L6.CP0510MTM086NEST               0.104612         0.000000       558376.224           0.000\n",
       "L6.SMU25716543000000030         -17.520471         0.000004     -3973682.551           0.000\n",
       "L6.SMU25716543000000030SA        17.145599         0.000004      3911759.421           0.000\n",
       "L6.sales                         -0.200087         0.000000    -13076682.602           0.000\n",
       "L7.WPS011101                      1.373417         0.000000     10108229.355           0.000\n",
       "L7.WPU011101                     -0.852013         0.000000    -10565459.652           0.000\n",
       "L7.PCU5242105242104             -11.778261         0.000001    -20538520.846           0.000\n",
       "L7.SMU55333403000000030SA        -0.010382         0.000000      -238512.609           0.000\n",
       "L7.CP0510MTM086NEST               0.247704         0.000000     14710715.305           0.000\n",
       "L7.SMU25716543000000030          24.227865         0.000001     21536071.677           0.000\n",
       "L7.SMU25716543000000030SA       -24.542755         0.000001    -20925894.185           0.000\n",
       "L7.sales                         -0.441962         0.000000    -11028604.640           0.000\n",
       "============================================================================================\n",
       "\n",
       "Results for equation SMU25716543000000030SA\n",
       "============================================================================================\n",
       "                               coefficient       std. error           t-stat            prob\n",
       "--------------------------------------------------------------------------------------------\n",
       "const                             0.004216         0.000000      5715450.910           0.000\n",
       "L1.WPS011101                     -0.326837         0.000000     -5193179.168           0.000\n",
       "L1.WPU011101                      0.906189         0.000000      5743170.808           0.000\n",
       "L1.PCU5242105242104             -10.147183         0.000000    -85482435.698           0.000\n",
       "L1.SMU55333403000000030SA         0.863233         0.000000     19097995.530           0.000\n",
       "L1.CP0510MTM086NEST              -1.195854         0.000000     -5835974.246           0.000\n",
       "L1.SMU25716543000000030         -19.143815         0.000004     -5400231.589           0.000\n",
       "L1.SMU25716543000000030SA        18.288238         0.000003      5396435.084           0.000\n",
       "L1.sales                          0.718492         0.000000      8798865.800           0.000\n",
       "L2.WPS011101                     -6.342906         0.000001     -7783555.407           0.000\n",
       "L2.WPU011101                      6.142639         0.000001      7654872.451           0.000\n",
       "L2.PCU5242105242104              -4.798308         0.000000    -25425254.927           0.000\n",
       "L2.SMU55333403000000030SA         1.837441         0.000000     11009750.039           0.000\n",
       "L2.CP0510MTM086NEST               0.419848         0.000000      1278225.886           0.000\n",
       "L2.SMU25716543000000030          41.960876         0.000000    120036596.763           0.000\n",
       "L2.SMU25716543000000030SA       -42.882982         0.000000   -179590231.848           0.000\n",
       "L2.sales                         -0.890568         0.000000     -7334723.843           0.000\n",
       "L3.WPS011101                     -3.877410         0.000000    -10191559.508           0.000\n",
       "L3.WPU011101                      4.493172         0.000000      9900501.749           0.000\n",
       "L3.PCU5242105242104              -3.751559         0.000000    -58830819.631           0.000\n",
       "L3.SMU55333403000000030SA        -1.957050         0.000000    -13241104.675           0.000\n",
       "L3.CP0510MTM086NEST              -4.460167         0.000001     -8625631.486           0.000\n",
       "L3.SMU25716543000000030           3.401717         0.000004       933308.813           0.000\n",
       "L3.SMU25716543000000030SA        -3.070174         0.000004      -818865.213           0.000\n",
       "L3.sales                          0.283160         0.000000      6312293.008           0.000\n",
       "L4.WPS011101                     -6.472891         0.000001     -8793381.290           0.000\n",
       "L4.WPU011101                      6.071924         0.000001      9005402.056           0.000\n",
       "L4.PCU5242105242104             -13.103830         0.000001    -11555171.664           0.000\n",
       "L4.SMU55333403000000030SA         0.504741         0.000000     10668868.741           0.000\n",
       "L4.CP0510MTM086NEST               1.012904         0.000000      3936734.424           0.000\n",
       "L4.SMU25716543000000030          12.619029         0.000006      2277431.838           0.000\n",
       "L4.SMU25716543000000030SA       -12.788429         0.000006     -2294638.347           0.000\n",
       "L4.sales                         -0.333311         0.000000     -8093168.473           0.000\n",
       "L5.WPS011101                     -1.884740         0.000000    -25223487.119           0.000\n",
       "L5.WPU011101                      2.362903         0.000000     17580967.612           0.000\n",
       "L5.PCU5242105242104              14.938586         0.000002      8145242.914           0.000\n",
       "L5.SMU55333403000000030SA         0.204371         0.000000      1674881.074           0.000\n",
       "L5.CP0510MTM086NEST               2.383987         0.000000     11253755.272           0.000\n",
       "L5.SMU25716543000000030          36.411605         0.000002     19367507.022           0.000\n",
       "L5.SMU25716543000000030SA       -36.180731         0.000002    -20518264.157           0.000\n",
       "L5.sales                          0.548331         0.000000     10572104.095           0.000\n",
       "L6.WPS011101                     -2.016853         0.000000   -471470639.195           0.000\n",
       "L6.WPU011101                      1.886859         0.000000    355635499.939           0.000\n",
       "L6.PCU5242105242104              -4.015831         0.000001     -7971419.426           0.000\n",
       "L6.SMU55333403000000030SA         1.449601         0.000000     12326304.000           0.000\n",
       "L6.CP0510MTM086NEST               0.060284         0.000000       275229.184           0.000\n",
       "L6.SMU25716543000000030         -17.483792         0.000005     -3391771.292           0.000\n",
       "L6.SMU25716543000000030SA        17.108228         0.000005      3338628.007           0.000\n",
       "L6.sales                         -0.198803         0.000000    -11113337.595           0.000\n",
       "L7.WPS011101                      1.436303         0.000000      9041958.131           0.000\n",
       "L7.WPU011101                     -0.926713         0.000000     -9829494.219           0.000\n",
       "L7.PCU5242105242104             -11.656724         0.000001    -17386333.926           0.000\n",
       "L7.SMU55333403000000030SA        -0.018028         0.000000      -354245.418           0.000\n",
       "L7.CP0510MTM086NEST               0.267090         0.000000     13567558.555           0.000\n",
       "L7.SMU25716543000000030          23.544076         0.000001     17900968.004           0.000\n",
       "L7.SMU25716543000000030SA       -23.864793         0.000001    -17404514.999           0.000\n",
       "L7.sales                         -0.426013         0.000000     -9092898.191           0.000\n",
       "============================================================================================\n",
       "\n",
       "Results for equation sales\n",
       "============================================================================================\n",
       "                               coefficient       std. error           t-stat            prob\n",
       "--------------------------------------------------------------------------------------------\n",
       "const                             0.030552         0.000000     18455578.949           0.000\n",
       "L1.WPS011101                     -1.408783         0.000000     -9974975.392           0.000\n",
       "L1.WPU011101                      2.670046         0.000000      7540783.582           0.000\n",
       "L1.PCU5242105242104             -10.813232         0.000000    -40593057.554           0.000\n",
       "L1.SMU55333403000000030SA         3.799925         0.000000     37462737.860           0.000\n",
       "L1.CP0510MTM086NEST              -3.467267         0.000000     -7540278.832           0.000\n",
       "L1.SMU25716543000000030          -4.529858         0.000008      -569420.737           0.000\n",
       "L1.SMU25716543000000030SA         1.898310         0.000008       249612.871           0.000\n",
       "L1.sales                          1.781654         0.000000      9722842.259           0.000\n",
       "L2.WPS011101                    -13.543743         0.000002     -7406162.727           0.000\n",
       "L2.WPU011101                     12.558682         0.000002      6974159.118           0.000\n",
       "L2.PCU5242105242104              -3.106818         0.000000     -7335975.722           0.000\n",
       "L2.SMU55333403000000030SA         5.595415         0.000000     14940359.724           0.000\n",
       "L2.CP0510MTM086NEST               6.247471         0.000001      8475883.670           0.000\n",
       "L2.SMU25716543000000030          94.043648         0.000001    119884608.609           0.000\n",
       "L2.SMU25716543000000030SA       -95.388924         0.000001   -178016606.313           0.000\n",
       "L2.sales                         -1.818427         0.000000     -6673863.093           0.000\n",
       "L3.WPS011101                     -2.151259         0.000001     -2519743.690           0.000\n",
       "L3.WPU011101                      3.030007         0.000001      2975175.969           0.000\n",
       "L3.PCU5242105242104             -12.175603         0.000000    -85084106.358           0.000\n",
       "L3.SMU55333403000000030SA        -4.120891         0.000000    -12424482.681           0.000\n",
       "L3.CP0510MTM086NEST             -12.426981         0.000001    -10709525.722           0.000\n",
       "L3.SMU25716543000000030          41.456535         0.000008      5068565.444           0.000\n",
       "L3.SMU25716543000000030SA       -40.941177         0.000008     -4866028.054           0.000\n",
       "L3.sales                          1.071997         0.000000     10649112.818           0.000\n",
       "L4.WPS011101                     -7.130278         0.000002     -4316471.518           0.000\n",
       "L4.WPU011101                      5.915119         0.000002      3909354.793           0.000\n",
       "L4.PCU5242105242104             -23.920682         0.000003     -9399755.143           0.000\n",
       "L4.SMU55333403000000030SA         1.447099         0.000000     13630529.965           0.000\n",
       "L4.CP0510MTM086NEST               4.206331         0.000001      7285112.701           0.000\n",
       "L4.SMU25716543000000030          22.219496         0.000012      1786975.042           0.000\n",
       "L4.SMU25716543000000030SA       -22.768049         0.000013     -1820487.438           0.000\n",
       "L4.sales                         -0.856434         0.000000     -9266746.172           0.000\n",
       "L5.WPS011101                      4.466779         0.000000     26638698.751           0.000\n",
       "L5.WPU011101                     -3.439288         0.000000    -11403293.881           0.000\n",
       "L5.PCU5242105242104              36.832779         0.000004      8949398.772           0.000\n",
       "L5.SMU55333403000000030SA        -0.654125         0.000000     -2388852.996           0.000\n",
       "L5.CP0510MTM086NEST               3.259095         0.000000      6855758.657           0.000\n",
       "L5.SMU25716543000000030          31.894181         0.000004      7559797.930           0.000\n",
       "L5.SMU25716543000000030SA       -33.339389         0.000004     -8425307.262           0.000\n",
       "L5.sales                          0.832514         0.000000      7152780.662           0.000\n",
       "L6.WPS011101                      2.311337         0.000000    240773358.189           0.000\n",
       "L6.WPU011101                     -3.044754         0.000000   -255730503.190           0.000\n",
       "L6.PCU5242105242104             -14.227172         0.000001    -12584722.367           0.000\n",
       "L6.SMU55333403000000030SA         3.823297         0.000000     14487287.380           0.000\n",
       "L6.CP0510MTM086NEST               2.194926         0.000000      4465549.322           0.000\n",
       "L6.SMU25716543000000030         -55.054967         0.000012     -4759400.668           0.000\n",
       "L6.SMU25716543000000030SA        54.389519         0.000011      4729803.127           0.000\n",
       "L6.sales                          0.011024         0.000000       274615.552           0.000\n",
       "L7.WPS011101                      3.364459         0.000000      9438356.373           0.000\n",
       "L7.WPU011101                     -2.864973         0.000000    -13541643.404           0.000\n",
       "L7.PCU5242105242104             -30.798071         0.000002    -20470095.123           0.000\n",
       "L7.SMU55333403000000030SA         1.379239         0.000000     12077031.730           0.000\n",
       "L7.CP0510MTM086NEST              -1.865078         0.000000    -42218799.581           0.000\n",
       "L7.SMU25716543000000030           5.032031         0.000003      1704916.243           0.000\n",
       "L7.SMU25716543000000030SA        -5.575354         0.000003     -1811930.406           0.000\n",
       "L7.sales                         -0.993005         0.000000     -9444860.831           0.000\n",
       "============================================================================================\n",
       "\n",
       "Correlation matrix of residuals\n",
       "                          WPS011101  WPU011101  PCU5242105242104  SMU55333403000000030SA  CP0510MTM086NEST  SMU25716543000000030  SMU25716543000000030SA     sales\n",
       "WPS011101                  1.000000   0.327626          0.238028                0.121708          0.901684              0.194010               -0.174371  0.887664\n",
       "WPU011101                  0.327626   1.000000          0.686480                0.697208          0.239027              0.020338                0.050041  0.299890\n",
       "PCU5242105242104           0.238028   0.686480          1.000000                0.581793          0.035847              0.108877                0.134707  0.193727\n",
       "SMU55333403000000030SA     0.121708   0.697208          0.581793                1.000000          0.091406             -0.138724                0.039859  0.116897\n",
       "CP0510MTM086NEST           0.901684   0.239027          0.035847                0.091406          1.000000              0.270668               -0.081149  0.887942\n",
       "SMU25716543000000030       0.194010   0.020338          0.108877               -0.138724          0.270668              1.000000                0.811021  0.153917\n",
       "SMU25716543000000030SA    -0.174371   0.050041          0.134707                0.039859         -0.081149              0.811021                1.000000 -0.150743\n",
       "sales                      0.887664   0.299890          0.193727                0.116897          0.887942              0.153917               -0.150743  1.000000\n",
       "\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdata = sm.datasets.macrodata.load_pandas().data\n",
    "dates = mdata[['year', 'quarter']].astype(int).astype(str)\n",
    "quarterly = dates[\"year\"] + \"Q\" + dates[\"quarter\"]\n",
    "quarterly = dates_from_str(quarterly)\n",
    "mdata = mdata[['realgdp','realcons','realinv']]\n",
    "mdata.index = pd.DatetimeIndex(quarterly)\n",
    "full.index = pd.DatetimeIndex(date)\n",
    "data = np.log(full).diff().dropna()\n",
    "model = VAR(data)\n",
    "results = model.fit(7)\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d612328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 64 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results.plot_acorr()\n",
    "'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ad604a35",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-d34fc0756e9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'aic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlag_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_ar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_forecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlag_order\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/statsmodels/tsa/vector_ar/var_model.py\u001b[0m in \u001b[0;36mplot_forecast\u001b[0;34m(self, steps, alpha, plot_stderr)\u001b[0m\n\u001b[1;32m   1509\u001b[0m         \u001b[0mPlot\u001b[0m \u001b[0mforecast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \"\"\"\n\u001b[0;32m-> 1511\u001b[0;31m         mid, lower, upper = self.forecast_interval(self.endog[-self.k_ar:],\n\u001b[0m\u001b[1;32m   1512\u001b[0m                                                    \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m                                                    alpha=alpha)\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/statsmodels/tsa/vector_ar/var_model.py\u001b[0m in \u001b[0;36mforecast_interval\u001b[0;34m(self, y, steps, alpha, exog_future)\u001b[0m\n\u001b[1;32m   1161\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_signif_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1163\u001b[0;31m         \u001b[0mpoint_forecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog_future\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexog_future\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1164\u001b[0m         \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forecast_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/statsmodels/tsa/vector_ar/var_model.py\u001b[0m in \u001b[0;36mforecast\u001b[0;34m(self, y, steps, exog_future)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m             \u001b[0mexog_future\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mforecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrend_coefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog_future\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0;31m# TODO: use `mse` module-level function?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/statsmodels/tsa/vector_ar/var_model.py\u001b[0m in \u001b[0;36mforecast\u001b[0;34m(y, coefs, trend_coefs, steps, exog)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \"\"\"\n\u001b[1;32m    228\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoefs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;31m# initial value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mforcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "model.select_order(15)\n",
    "results = model.fit(49, ic='aic')\n",
    "lag_order = results.k_ar\n",
    "results.plot_forecast(10)\n",
    "results.forecast(data.values[-lag_order:], 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "584a998e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Bad Request.  Invalid value for variable series_id.  Series IDs should be 25 or less alphanumeric characters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/fredapi/fred.py\u001b[0m in \u001b[0;36m__fetch_data\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    641\u001b[0m                 'http', request, response, code, msg, hdrs)\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 400: Bad Request",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-a6ad9faaf4bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2/1/2021\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2/1/2021\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/fredapi/fred.py\u001b[0m in \u001b[0;36mget_series\u001b[0;34m(self, series_id, observation_start, observation_end, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0murl\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'&'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murlencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__fetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No data exists for series id: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseries_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/fredapi/fred.py\u001b[0m in \u001b[0;36m__fetch_data\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Bad Request.  Invalid value for variable series_id.  Series IDs should be 25 or less alphanumeric characters."
     ]
    }
   ],
   "source": [
    "values = {}\n",
    "for name in names:\n",
    "    d = fred.get_series(name, observation_start=\"2/1/2021\", observation_end = \"2/1/2021\")\n",
    "    values[name] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c156de",
   "metadata": {},
   "outputs": [],
   "source": [
    "values[\"sales\"] = 6236\n",
    "test = pd.DataFrame(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d5b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.from_numpy(test.to_numpy())\n",
    "test_data2 = torch.from_numpy(full[:-3:-1].to_numpy())\n",
    "test_data = torch.cat([test_data2, test_data]).unsqueeze(0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4752e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.load_state_dict(torch.load(\"model.pt\"))\n",
    "model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2918052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d5189e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "     \n",
    "\n",
    "        self.output_block = nn.Sequential(nn.Conv1d(3, 16, 2, 1, 0), nn.ReLU(), \n",
    "                                          nn.Conv1d(16, 32, 2, 1, 0), nn.ReLU(), \n",
    "                                          nn.Conv1d(32, 32, 2, 1, 0), nn.ReLU(), \n",
    "                                          nn.Conv1d(32, 16, 2, 1, 0), nn.ReLU())\n",
    "        self.fc = nn.Sequential(nn.LazyLinear(512), nn.ReLU(), nn.Linear(512, 256), nn.ReLU(), nn.Linear(256, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        net = self.output_block(x)\n",
    "        net = net.view(net.shape[0], -1)\n",
    "        net = self.fc(net)\n",
    "        \n",
    "        return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a396928d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>5438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>6056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>5773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>6236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sales\n",
       "3    1362\n",
       "4    1225\n",
       "5    1312\n",
       "6    1373\n",
       "7    1530\n",
       "..    ...\n",
       "60   4797\n",
       "61   5438\n",
       "62   6056\n",
       "63   5773\n",
       "64   6236\n",
       "\n",
       "[62 rows x 1 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "16e1a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, df, target):\n",
    "        data = df[1:].reset_index(drop=True)\n",
    "        sales = target[:-1].reset_index(drop=True)\n",
    "        self.data = data.join(sales)\n",
    "        self.target = target[3:].to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)-3\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data.iloc[idx:idx+3].to_numpy()\n",
    "        target = self.target[idx]\n",
    "        return torch.tensor(data).float(), torch.tensor(target).float()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "32c5f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = int(len(data) * 0.8)\n",
    "train, test = data.iloc(axis=1)[np.where(clf.coef_ != 0)[0]][:idx], data.iloc(axis=1)[np.where(clf.coef_ != 0)[0]][idx:]\n",
    "traint, testt = target[:idx], target[idx:]\n",
    "trainset = dataset(train, traint)\n",
    "testset = dataset(test, testt)\n",
    "train_loader = DataLoader(trainset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5d1bd6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwight/miniconda3/lib/python3.8/site-packages/torch/nn/modules/lazy.py:175: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=25, factor=0.5, min_lr=0.000001)\n",
    "def Train(epochs, model, train_loader):\n",
    "    valid_loss_min = np.Inf\n",
    "    min_rmse = np.Inf\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        valid_loss = 0.0\n",
    "        rmse = 0.0\n",
    "        model.train()\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "       \n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            output = model(images)\n",
    "        \n",
    "            loss = torch.sqrt(criterion(output,labels))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "        model.eval()\n",
    "        for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "       \n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            output = model(images)\n",
    "        \n",
    "            loss = torch.sqrt(criterion(output,labels))\n",
    "          \n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "                \n",
    "        scheduler.step(valid_loss)\n",
    "        print(f\"Train Loss: {train_loss.item()}\")\n",
    "        print(f\"Valid Loss: {valid_loss.item()}\")\n",
    "        if valid_loss.item() < valid_loss_min:\n",
    "            valid_loss_min = valid_loss.item()\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "    print(f\"Min RMSE: {valid_loss_min}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "970d5267",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 36804692.0\n",
      "Valid Loss: 6631559.0\n",
      "Train Loss: 965717.0\n",
      "Valid Loss: 1003341.0625\n",
      "Train Loss: 157169.8125\n",
      "Valid Loss: 148701.671875\n",
      "Train Loss: 18546.599609375\n",
      "Valid Loss: 24466.734375\n",
      "Train Loss: 4002.153076171875\n",
      "Valid Loss: 10286.81640625\n",
      "Train Loss: 6023.8115234375\n",
      "Valid Loss: 6605.6181640625\n",
      "Train Loss: 3065.498291015625\n",
      "Valid Loss: 5965.46826171875\n",
      "Train Loss: 2647.583740234375\n",
      "Valid Loss: 5734.28955078125\n",
      "Train Loss: 2695.34619140625\n",
      "Valid Loss: 5899.0791015625\n",
      "Train Loss: 2686.925537109375\n",
      "Valid Loss: 5821.5146484375\n",
      "Train Loss: 2675.387939453125\n",
      "Valid Loss: 6000.1162109375\n",
      "Train Loss: 2690.362060546875\n",
      "Valid Loss: 5821.48486328125\n",
      "Train Loss: 2679.12353515625\n",
      "Valid Loss: 5795.2333984375\n",
      "Train Loss: 2693.272216796875\n",
      "Valid Loss: 5820.26513671875\n",
      "Train Loss: 2694.411376953125\n",
      "Valid Loss: 6024.09033203125\n",
      "Train Loss: 2677.86962890625\n",
      "Valid Loss: 5872.21826171875\n",
      "Train Loss: 2670.9658203125\n",
      "Valid Loss: 5999.6689453125\n",
      "Train Loss: 2689.29345703125\n",
      "Valid Loss: 6054.47216796875\n",
      "Train Loss: 2643.35400390625\n",
      "Valid Loss: 5745.2568359375\n",
      "Train Loss: 2680.468017578125\n",
      "Valid Loss: 5717.5556640625\n",
      "Train Loss: 3433.148193359375\n",
      "Valid Loss: 1055387.0\n",
      "Train Loss: 131790.703125\n",
      "Valid Loss: 5705.490234375\n",
      "Train Loss: 2815.525390625\n",
      "Valid Loss: 4308.49609375\n",
      "Train Loss: 4184.875\n",
      "Valid Loss: 14085.4892578125\n",
      "Train Loss: 6954.72021484375\n",
      "Valid Loss: 12651.6669921875\n",
      "Train Loss: 5717.1279296875\n",
      "Valid Loss: 38640.93359375\n",
      "Train Loss: 6809.52392578125\n",
      "Valid Loss: 5995.1845703125\n",
      "Train Loss: 3804.1572265625\n",
      "Valid Loss: 37653.49609375\n",
      "Train Loss: 2747.203857421875\n",
      "Valid Loss: 116628.640625\n",
      "Train Loss: 36641.99609375\n",
      "Valid Loss: 5996.2900390625\n",
      "Train Loss: 2607.031005859375\n",
      "Valid Loss: 6579.49951171875\n",
      "Train Loss: 2910.22802734375\n",
      "Valid Loss: 5989.6318359375\n",
      "Train Loss: 5838.18994140625\n",
      "Valid Loss: 5686.6064453125\n",
      "Train Loss: 2696.173095703125\n",
      "Valid Loss: 6464.6669921875\n",
      "Train Loss: 2656.738525390625\n",
      "Valid Loss: 6578.24755859375\n",
      "Train Loss: 3517.93310546875\n",
      "Valid Loss: 6002.1640625\n",
      "Train Loss: 7415.0625\n",
      "Valid Loss: 5917.2724609375\n",
      "Train Loss: 2656.18701171875\n",
      "Valid Loss: 5697.71240234375\n",
      "Train Loss: 2643.725830078125\n",
      "Valid Loss: 11503.3837890625\n",
      "Train Loss: 3121.716796875\n",
      "Valid Loss: 5903.849609375\n",
      "Train Loss: 27109.25390625\n",
      "Valid Loss: 6399.7109375\n",
      "Train Loss: 2628.498046875\n",
      "Valid Loss: 5658.23388671875\n",
      "Train Loss: 2606.36279296875\n",
      "Valid Loss: 22743.80078125\n",
      "Train Loss: 5431.482421875\n",
      "Valid Loss: 5619.0380859375\n",
      "Train Loss: 2544.482177734375\n",
      "Valid Loss: 6468.64794921875\n",
      "Train Loss: 2502.90234375\n",
      "Valid Loss: 14264924.0\n",
      "Train Loss: 152326.0625\n",
      "Valid Loss: 5721.0361328125\n",
      "Train Loss: 2651.81005859375\n",
      "Valid Loss: 6053.92724609375\n",
      "Train Loss: 2684.50048828125\n",
      "Valid Loss: 5723.228515625\n",
      "Train Loss: 2685.727294921875\n",
      "Valid Loss: 5725.1259765625\n",
      "Train Loss: 15678.6015625\n",
      "Valid Loss: 5937.7685546875\n",
      "Train Loss: 2669.68505859375\n",
      "Valid Loss: 5707.4873046875\n",
      "Train Loss: 2685.982421875\n",
      "Valid Loss: 5816.625\n",
      "Train Loss: 2696.87451171875\n",
      "Valid Loss: 5688.5712890625\n",
      "Train Loss: 2681.7158203125\n",
      "Valid Loss: 5891.8837890625\n",
      "Train Loss: 2700.07666015625\n",
      "Valid Loss: 5812.99853515625\n",
      "Train Loss: 2640.546142578125\n",
      "Valid Loss: 5720.29638671875\n",
      "Train Loss: 2684.89599609375\n",
      "Valid Loss: 6579.28076171875\n",
      "Train Loss: 2694.653076171875\n",
      "Valid Loss: 5890.7451171875\n",
      "Train Loss: 2662.69287109375\n",
      "Valid Loss: 5778.54541015625\n",
      "Train Loss: 2646.065185546875\n",
      "Valid Loss: 6045.51611328125\n",
      "Train Loss: 2672.09814453125\n",
      "Valid Loss: 5738.48876953125\n",
      "Train Loss: 2617.161376953125\n",
      "Valid Loss: 5963.00732421875\n",
      "Train Loss: 2638.25146484375\n",
      "Valid Loss: 6579.18212890625\n",
      "Train Loss: 2691.090576171875\n",
      "Valid Loss: 5711.26416015625\n",
      "Train Loss: 2627.12060546875\n",
      "Valid Loss: 5706.1982421875\n",
      "Train Loss: 2648.948486328125\n",
      "Valid Loss: 5972.466796875\n",
      "Train Loss: 2661.294677734375\n",
      "Valid Loss: 5660.9033203125\n",
      "Train Loss: 2665.695068359375\n",
      "Valid Loss: 5766.82666015625\n",
      "Train Loss: 2671.25927734375\n",
      "Valid Loss: 5774.16796875\n",
      "Train Loss: 4591.5966796875\n",
      "Valid Loss: 5848.61083984375\n",
      "Train Loss: 2622.079345703125\n",
      "Valid Loss: 6558.74853515625\n",
      "Train Loss: 2652.841796875\n",
      "Valid Loss: 6026.88671875\n",
      "Train Loss: 2637.474609375\n",
      "Valid Loss: 5691.56787109375\n",
      "Train Loss: 2657.4208984375\n",
      "Valid Loss: 5646.12109375\n",
      "Train Loss: 2613.551025390625\n",
      "Valid Loss: 6545.9794921875\n",
      "Train Loss: 2621.2568359375\n",
      "Valid Loss: 6545.97705078125\n",
      "Train Loss: 2617.60595703125\n",
      "Valid Loss: 6516.2607421875\n",
      "Train Loss: 2611.728759765625\n",
      "Valid Loss: 6394.17236328125\n",
      "Train Loss: 2605.651123046875\n",
      "Valid Loss: 6376.8291015625\n",
      "Train Loss: 2575.60546875\n",
      "Valid Loss: 5926.10009765625\n",
      "Train Loss: 2568.75537109375\n",
      "Valid Loss: 5583.30029296875\n",
      "Train Loss: 2831.736572265625\n",
      "Valid Loss: 5562.9208984375\n",
      "Train Loss: 2550.15283203125\n",
      "Valid Loss: 5823.69580078125\n",
      "Train Loss: 21431.115234375\n",
      "Valid Loss: 5617.12158203125\n",
      "Train Loss: 2560.904052734375\n",
      "Valid Loss: 6482.21630859375\n",
      "Train Loss: 2569.68017578125\n",
      "Valid Loss: 5795.20556640625\n",
      "Train Loss: 2612.45849609375\n",
      "Valid Loss: 5781.20654296875\n",
      "Train Loss: 2611.890380859375\n",
      "Valid Loss: 5874.80224609375\n",
      "Train Loss: 2568.97216796875\n",
      "Valid Loss: 5957.3896484375\n",
      "Train Loss: 2621.89501953125\n",
      "Valid Loss: 5757.18310546875\n",
      "Train Loss: 2584.197509765625\n",
      "Valid Loss: 5902.6982421875\n",
      "Train Loss: 2560.396484375\n",
      "Valid Loss: 5901.291015625\n",
      "Train Loss: 2574.602294921875\n",
      "Valid Loss: 5625.75830078125\n",
      "Train Loss: 2599.896728515625\n",
      "Valid Loss: 5647.373046875\n",
      "Train Loss: 2606.69287109375\n",
      "Valid Loss: 5837.10595703125\n",
      "Train Loss: 2561.51220703125\n",
      "Valid Loss: 5618.380859375\n",
      "Train Loss: 2580.503173828125\n",
      "Valid Loss: 5786.74658203125\n",
      "Train Loss: 2598.28076171875\n",
      "Valid Loss: 5704.83544921875\n",
      "Train Loss: 2534.41357421875\n",
      "Valid Loss: 5662.79345703125\n",
      "Train Loss: 2597.779296875\n",
      "Valid Loss: 5886.236328125\n",
      "Train Loss: 2542.02685546875\n",
      "Valid Loss: 5662.5751953125\n",
      "Train Loss: 2557.21533203125\n",
      "Valid Loss: 5603.88232421875\n",
      "Train Loss: 2577.59765625\n",
      "Valid Loss: 5875.89599609375\n",
      "Train Loss: 2575.91357421875\n",
      "Valid Loss: 5600.84033203125\n",
      "Train Loss: 2562.70458984375\n",
      "Valid Loss: 5896.79931640625\n",
      "Train Loss: 2549.560791015625\n",
      "Valid Loss: 5550.9619140625\n",
      "Train Loss: 2561.103515625\n",
      "Valid Loss: 6460.5126953125\n",
      "Train Loss: 2573.93603515625\n",
      "Valid Loss: 5684.09423828125\n",
      "Train Loss: 2578.569580078125\n",
      "Valid Loss: 5915.017578125\n",
      "Train Loss: 2568.852294921875\n",
      "Valid Loss: 5571.3681640625\n",
      "Train Loss: 2528.291015625\n",
      "Valid Loss: 5860.82470703125\n",
      "Train Loss: 2547.92626953125\n",
      "Valid Loss: 6346.05126953125\n",
      "Train Loss: 2548.77490234375\n",
      "Valid Loss: 5605.53857421875\n",
      "Train Loss: 2561.62646484375\n",
      "Valid Loss: 5582.75634765625\n",
      "Train Loss: 2550.47265625\n",
      "Valid Loss: 6434.72265625\n",
      "Train Loss: 2469.553466796875\n",
      "Valid Loss: 5744.03955078125\n",
      "Train Loss: 2564.02783203125\n",
      "Valid Loss: 6321.3994140625\n",
      "Train Loss: 2534.05126953125\n",
      "Valid Loss: 5661.267578125\n",
      "Train Loss: 2513.800048828125\n",
      "Valid Loss: 6416.95654296875\n",
      "Train Loss: 2537.17431640625\n",
      "Valid Loss: 5655.43408203125\n",
      "Train Loss: 2527.658935546875\n",
      "Valid Loss: 5886.77490234375\n",
      "Train Loss: 2475.61279296875\n",
      "Valid Loss: 5642.9169921875\n",
      "Train Loss: 2555.04736328125\n",
      "Valid Loss: 6317.8271484375\n",
      "Train Loss: 2528.714599609375\n",
      "Valid Loss: 5826.08642578125\n",
      "Train Loss: 2528.45703125\n",
      "Valid Loss: 5555.3095703125\n",
      "Train Loss: 2523.694091796875\n",
      "Valid Loss: 5511.3564453125\n",
      "Train Loss: 2479.92724609375\n",
      "Valid Loss: 5547.7412109375\n",
      "Train Loss: 2492.35546875\n",
      "Valid Loss: 6415.83349609375\n",
      "Train Loss: 2529.7353515625\n",
      "Valid Loss: 5709.04150390625\n",
      "Train Loss: 2502.531005859375\n",
      "Valid Loss: 5691.72314453125\n",
      "Train Loss: 2531.66455078125\n",
      "Valid Loss: 5792.66064453125\n",
      "Train Loss: 2492.9541015625\n",
      "Valid Loss: 5865.11083984375\n",
      "Train Loss: 2511.55126953125\n",
      "Valid Loss: 5513.4365234375\n",
      "Train Loss: 2522.979736328125\n",
      "Valid Loss: 5538.05126953125\n",
      "Train Loss: 2509.671875\n",
      "Valid Loss: 5600.39697265625\n",
      "Train Loss: 2508.909912109375\n",
      "Valid Loss: 5859.6328125\n",
      "Train Loss: 2525.628173828125\n",
      "Valid Loss: 5624.9140625\n",
      "Train Loss: 2531.326171875\n",
      "Valid Loss: 5555.9775390625\n",
      "Train Loss: 2528.472900390625\n",
      "Valid Loss: 5516.21923828125\n",
      "Train Loss: 2496.84130859375\n",
      "Valid Loss: 5548.21240234375\n",
      "Train Loss: 2506.091064453125\n",
      "Valid Loss: 5549.75\n",
      "Train Loss: 2520.605224609375\n",
      "Valid Loss: 5616.00830078125\n",
      "Train Loss: 2494.711669921875\n",
      "Valid Loss: 6269.80615234375\n",
      "Train Loss: 2480.578857421875\n",
      "Valid Loss: 6380.8203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2480.059814453125\n",
      "Valid Loss: 5523.23095703125\n",
      "Train Loss: 2496.259033203125\n",
      "Valid Loss: 5485.56982421875\n",
      "Train Loss: 2473.904541015625\n",
      "Valid Loss: 5843.16650390625\n",
      "Train Loss: 2466.988037109375\n",
      "Valid Loss: 6277.78857421875\n",
      "Train Loss: 2490.1025390625\n",
      "Valid Loss: 6252.39453125\n",
      "Train Loss: 2504.0322265625\n",
      "Valid Loss: 5792.03076171875\n",
      "Train Loss: 2497.982177734375\n",
      "Valid Loss: 5786.73681640625\n",
      "Train Loss: 2514.331298828125\n",
      "Valid Loss: 5534.01318359375\n",
      "Train Loss: 2462.358154296875\n",
      "Valid Loss: 5472.755859375\n",
      "Train Loss: 2500.284912109375\n",
      "Valid Loss: 5677.1005859375\n",
      "Train Loss: 2484.76171875\n",
      "Valid Loss: 5493.61279296875\n",
      "Train Loss: 2473.030517578125\n",
      "Valid Loss: 6263.65576171875\n",
      "Train Loss: 2423.92578125\n",
      "Valid Loss: 6231.8115234375\n",
      "Train Loss: 2479.251953125\n",
      "Valid Loss: 5672.9091796875\n",
      "Train Loss: 2487.31689453125\n",
      "Valid Loss: 5778.185546875\n",
      "Train Loss: 2425.7900390625\n",
      "Valid Loss: 5522.0068359375\n",
      "Train Loss: 2441.239501953125\n",
      "Valid Loss: 5496.00732421875\n",
      "Train Loss: 2475.8505859375\n",
      "Valid Loss: 5825.931640625\n",
      "Train Loss: 2482.06640625\n",
      "Valid Loss: 6257.7255859375\n",
      "Train Loss: 2487.0654296875\n",
      "Valid Loss: 5642.2900390625\n",
      "Train Loss: 2468.31298828125\n",
      "Valid Loss: 5521.20361328125\n",
      "Train Loss: 2458.181396484375\n",
      "Valid Loss: 6263.29052734375\n",
      "Train Loss: 2417.70849609375\n",
      "Valid Loss: 5771.98193359375\n",
      "Train Loss: 2470.6591796875\n",
      "Valid Loss: 6359.00048828125\n",
      "Train Loss: 2403.95849609375\n",
      "Valid Loss: 5819.00927734375\n",
      "Train Loss: 2453.27734375\n",
      "Valid Loss: 6248.923828125\n",
      "Train Loss: 2449.64208984375\n",
      "Valid Loss: 5543.80224609375\n",
      "Train Loss: 2471.58056640625\n",
      "Valid Loss: 6243.9306640625\n",
      "Train Loss: 2469.148193359375\n",
      "Valid Loss: 5814.18359375\n",
      "Train Loss: 2479.2880859375\n",
      "Valid Loss: 5513.05859375\n",
      "Train Loss: 2496.31298828125\n",
      "Valid Loss: 5740.46044921875\n",
      "Train Loss: 2441.72607421875\n",
      "Valid Loss: 5440.90576171875\n",
      "Train Loss: 2472.693115234375\n",
      "Valid Loss: 5808.8818359375\n",
      "Train Loss: 2488.4384765625\n",
      "Valid Loss: 5650.63623046875\n",
      "Train Loss: 2468.83203125\n",
      "Valid Loss: 6353.43115234375\n",
      "Train Loss: 2418.60693359375\n",
      "Valid Loss: 5485.3154296875\n",
      "Train Loss: 2473.392578125\n",
      "Valid Loss: 5761.4765625\n",
      "Train Loss: 2463.3515625\n",
      "Valid Loss: 5470.64599609375\n",
      "Train Loss: 2430.400634765625\n",
      "Valid Loss: 5507.92333984375\n",
      "Train Loss: 2477.764404296875\n",
      "Valid Loss: 5768.70703125\n",
      "Train Loss: 2453.89208984375\n",
      "Valid Loss: 5555.36572265625\n",
      "Train Loss: 2469.875732421875\n",
      "Valid Loss: 5468.6923828125\n",
      "Train Loss: 2465.02099609375\n",
      "Valid Loss: 6237.2392578125\n",
      "Train Loss: 2476.713134765625\n",
      "Valid Loss: 5757.75927734375\n",
      "Train Loss: 2447.70849609375\n",
      "Valid Loss: 6214.583984375\n",
      "Train Loss: 2429.17041015625\n",
      "Valid Loss: 5536.02197265625\n",
      "Train Loss: 2469.28173828125\n",
      "Valid Loss: 5437.44091796875\n",
      "Train Loss: 2464.30615234375\n",
      "Valid Loss: 5480.77490234375\n",
      "Train Loss: 2443.970947265625\n",
      "Valid Loss: 5552.4697265625\n",
      "Train Loss: 2460.35986328125\n",
      "Valid Loss: 5532.43701171875\n",
      "Train Loss: 2421.541015625\n",
      "Valid Loss: 5801.93505859375\n",
      "Train Loss: 2475.995849609375\n",
      "Valid Loss: 5479.5283203125\n",
      "Train Loss: 2475.84765625\n",
      "Valid Loss: 5436.36669921875\n",
      "Train Loss: 2454.9638671875\n",
      "Valid Loss: 5477.32373046875\n",
      "Train Loss: 2418.720947265625\n",
      "Valid Loss: 5475.359375\n",
      "Train Loss: 2450.7734375\n",
      "Valid Loss: 5478.69189453125\n",
      "Train Loss: 2461.736083984375\n",
      "Valid Loss: 5449.6142578125\n",
      "Train Loss: 2427.094482421875\n",
      "Valid Loss: 5643.244140625\n",
      "Train Loss: 2460.598876953125\n",
      "Valid Loss: 6324.97021484375\n",
      "Train Loss: 2449.47509765625\n",
      "Valid Loss: 6340.15234375\n",
      "Train Loss: 2483.924560546875\n",
      "Valid Loss: 6221.1865234375\n",
      "Train Loss: 2475.68994140625\n",
      "Valid Loss: 5619.5859375\n",
      "Train Loss: 2457.315673828125\n",
      "Valid Loss: 5563.86572265625\n",
      "Train Loss: 2473.167724609375\n",
      "Valid Loss: 6235.04150390625\n",
      "Train Loss: 2411.3935546875\n",
      "Valid Loss: 5522.978515625\n",
      "Train Loss: 2457.226318359375\n",
      "Valid Loss: 6236.6962890625\n",
      "Train Loss: 2466.047607421875\n",
      "Valid Loss: 5474.35986328125\n",
      "Train Loss: 2470.48779296875\n",
      "Valid Loss: 5792.70068359375\n",
      "Train Loss: 2475.265869140625\n",
      "Valid Loss: 6237.681640625\n",
      "Train Loss: 2441.148193359375\n",
      "Valid Loss: 5472.296875\n",
      "Train Loss: 2479.358154296875\n",
      "Valid Loss: 5440.78466796875\n",
      "Train Loss: 2464.727783203125\n",
      "Valid Loss: 6333.09375\n",
      "Train Loss: 2437.57421875\n",
      "Valid Loss: 5561.861328125\n",
      "Train Loss: 2462.442626953125\n",
      "Valid Loss: 6336.552734375\n",
      "Train Loss: 2466.388916015625\n",
      "Valid Loss: 5744.34423828125\n",
      "Train Loss: 2456.73095703125\n",
      "Valid Loss: 5743.60107421875\n",
      "Train Loss: 2440.917236328125\n",
      "Valid Loss: 5561.05810546875\n",
      "Train Loss: 2458.16259765625\n",
      "Valid Loss: 5741.97705078125\n",
      "Train Loss: 2466.74267578125\n",
      "Valid Loss: 5427.666015625\n",
      "Train Loss: 2470.853515625\n",
      "Valid Loss: 6223.63232421875\n",
      "Train Loss: 2442.77392578125\n",
      "Valid Loss: 5466.31396484375\n",
      "Train Loss: 2429.702880859375\n",
      "Valid Loss: 6341.90771484375\n",
      "Train Loss: 2378.166015625\n",
      "Valid Loss: 5790.66259765625\n",
      "Train Loss: 2468.427978515625\n",
      "Valid Loss: 5463.05224609375\n",
      "Train Loss: 2460.1416015625\n",
      "Valid Loss: 5594.89111328125\n",
      "Train Loss: 2455.725341796875\n",
      "Valid Loss: 6321.951171875\n",
      "Train Loss: 2465.342041015625\n",
      "Valid Loss: 5464.83837890625\n",
      "Train Loss: 2480.315185546875\n",
      "Valid Loss: 5489.21337890625\n",
      "Train Loss: 2480.740478515625\n",
      "Valid Loss: 5487.4140625\n",
      "Train Loss: 2356.619140625\n",
      "Valid Loss: 5490.34423828125\n",
      "Train Loss: 2450.79541015625\n",
      "Valid Loss: 5447.1064453125\n",
      "Train Loss: 2418.55419921875\n",
      "Valid Loss: 5426.8544921875\n",
      "Train Loss: 2473.238525390625\n",
      "Valid Loss: 5742.2216796875\n",
      "Train Loss: 2429.19970703125\n",
      "Valid Loss: 5739.3076171875\n",
      "Train Loss: 2453.843994140625\n",
      "Valid Loss: 5465.193359375\n",
      "Train Loss: 2445.55712890625\n",
      "Valid Loss: 5450.9765625\n",
      "Train Loss: 2456.265625\n",
      "Valid Loss: 5738.13330078125\n",
      "Train Loss: 2443.641357421875\n",
      "Valid Loss: 6323.83740234375\n",
      "Train Loss: 2463.18994140625\n",
      "Valid Loss: 5785.68359375\n",
      "Train Loss: 2402.599853515625\n",
      "Valid Loss: 5447.01806640625\n",
      "Train Loss: 2472.83935546875\n",
      "Valid Loss: 5552.78759765625\n",
      "Train Loss: 2448.298583984375\n",
      "Valid Loss: 5440.87353515625\n",
      "Train Loss: 2449.205078125\n",
      "Valid Loss: 6216.59033203125\n",
      "Train Loss: 2429.06591796875\n",
      "Valid Loss: 5555.7802734375\n",
      "Train Loss: 2455.630859375\n",
      "Valid Loss: 6190.95068359375\n",
      "Train Loss: 2471.363037109375\n",
      "Valid Loss: 5463.578125\n",
      "Train Loss: 2457.11962890625\n",
      "Valid Loss: 5747.84423828125\n",
      "Train Loss: 2441.32373046875\n",
      "Valid Loss: 5555.55322265625\n",
      "Train Loss: 2432.258056640625\n",
      "Valid Loss: 5465.24560546875\n",
      "Train Loss: 2420.434326171875\n",
      "Valid Loss: 5487.3251953125\n",
      "Train Loss: 2449.741455078125\n",
      "Valid Loss: 5463.47705078125\n",
      "Train Loss: 2477.749267578125\n",
      "Valid Loss: 5448.6689453125\n",
      "Train Loss: 2467.83056640625\n",
      "Valid Loss: 5516.3935546875\n",
      "Train Loss: 2435.66455078125\n",
      "Valid Loss: 5437.888671875\n",
      "Train Loss: 2467.72119140625\n",
      "Valid Loss: 5462.85498046875\n",
      "Train Loss: 2459.88232421875\n",
      "Valid Loss: 5629.22412109375\n",
      "Train Loss: 2481.49951171875\n",
      "Valid Loss: 5428.34912109375\n",
      "Train Loss: 2444.528564453125\n",
      "Valid Loss: 5728.20947265625\n",
      "Train Loss: 2452.5439453125\n",
      "Valid Loss: 5443.5654296875\n",
      "Train Loss: 2440.085205078125\n",
      "Valid Loss: 5485.74560546875\n",
      "Train Loss: 2446.22900390625\n",
      "Valid Loss: 5461.5\n",
      "Train Loss: 2429.181396484375\n",
      "Valid Loss: 6318.75732421875\n",
      "Train Loss: 2457.12744140625\n",
      "Valid Loss: 5736.63037109375\n",
      "Train Loss: 2416.0703125\n",
      "Valid Loss: 5445.81494140625\n",
      "Train Loss: 2412.9755859375\n",
      "Valid Loss: 5737.638671875\n",
      "Train Loss: 2419.43603515625\n",
      "Valid Loss: 6200.4541015625\n",
      "Train Loss: 2463.36279296875\n",
      "Valid Loss: 6224.4013671875\n",
      "Train Loss: 2407.67333984375\n",
      "Valid Loss: 6208.51171875\n",
      "Train Loss: 2444.97021484375\n",
      "Valid Loss: 5584.94384765625\n",
      "Train Loss: 2448.05029296875\n",
      "Valid Loss: 6324.2314453125\n",
      "Train Loss: 2448.31591796875\n",
      "Valid Loss: 5486.146484375\n",
      "Train Loss: 2451.615234375\n",
      "Valid Loss: 6329.06689453125\n",
      "Train Loss: 2446.30322265625\n",
      "Valid Loss: 6324.00732421875\n",
      "Train Loss: 2480.041015625\n",
      "Valid Loss: 5607.15673828125\n",
      "Train Loss: 2447.28857421875\n",
      "Valid Loss: 5624.794921875\n",
      "Train Loss: 2438.7158203125\n",
      "Valid Loss: 6216.8798828125\n",
      "Train Loss: 2438.28759765625\n",
      "Valid Loss: 5459.39306640625\n",
      "Train Loss: 2417.63330078125\n",
      "Valid Loss: 5418.2880859375\n",
      "Train Loss: 2455.4951171875\n",
      "Valid Loss: 5461.3857421875\n",
      "Train Loss: 2402.8349609375\n",
      "Valid Loss: 5750.78515625\n",
      "Train Loss: 2473.644775390625\n",
      "Valid Loss: 5484.70654296875\n",
      "Train Loss: 2467.9091796875\n",
      "Valid Loss: 5706.07958984375\n",
      "Train Loss: 2430.189453125\n",
      "Valid Loss: 5463.2314453125\n",
      "Train Loss: 2443.1826171875\n",
      "Valid Loss: 5461.787109375\n",
      "Train Loss: 2466.37744140625\n",
      "Valid Loss: 5463.94140625\n",
      "Train Loss: 2411.593017578125\n",
      "Valid Loss: 5477.57763671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2452.303955078125\n",
      "Valid Loss: 6317.384765625\n",
      "Train Loss: 2405.243896484375\n",
      "Valid Loss: 5463.8271484375\n",
      "Train Loss: 2446.973388671875\n",
      "Valid Loss: 5750.4306640625\n",
      "Train Loss: 2416.931396484375\n",
      "Valid Loss: 5460.466796875\n",
      "Train Loss: 2438.375244140625\n",
      "Valid Loss: 5463.7314453125\n",
      "Train Loss: 2454.86181640625\n",
      "Valid Loss: 5552.01171875\n",
      "Train Loss: 2450.28466796875\n",
      "Valid Loss: 5524.06591796875\n",
      "Train Loss: 2417.056396484375\n",
      "Valid Loss: 5736.02001953125\n",
      "Train Loss: 2457.0078125\n",
      "Valid Loss: 6211.7294921875\n",
      "Train Loss: 2469.66845703125\n",
      "Valid Loss: 6324.119140625\n",
      "Train Loss: 2406.69921875\n",
      "Valid Loss: 6319.1767578125\n",
      "Train Loss: 2442.062744140625\n",
      "Valid Loss: 5450.44140625\n",
      "Train Loss: 2462.68115234375\n",
      "Valid Loss: 5606.015625\n",
      "Train Loss: 2458.708984375\n",
      "Valid Loss: 5602.2041015625\n",
      "Train Loss: 2431.106689453125\n",
      "Valid Loss: 5463.1005859375\n",
      "Train Loss: 2474.4501953125\n",
      "Valid Loss: 5462.1015625\n",
      "Train Loss: 2437.855224609375\n",
      "Valid Loss: 5456.173828125\n",
      "Train Loss: 2453.213623046875\n",
      "Valid Loss: 5743.84228515625\n",
      "Train Loss: 2425.3720703125\n",
      "Valid Loss: 5425.7802734375\n",
      "Train Loss: 2451.587890625\n",
      "Valid Loss: 5548.28173828125\n",
      "Train Loss: 2465.5400390625\n",
      "Valid Loss: 5780.6630859375\n",
      "Train Loss: 2455.4072265625\n",
      "Valid Loss: 5783.04248046875\n",
      "Train Loss: 2469.627197265625\n",
      "Valid Loss: 5749.72998046875\n",
      "Train Loss: 2450.33642578125\n",
      "Valid Loss: 5462.873046875\n",
      "Train Loss: 2426.94482421875\n",
      "Valid Loss: 6212.396484375\n",
      "Train Loss: 2434.285888671875\n",
      "Valid Loss: 5462.6953125\n",
      "Train Loss: 2435.27490234375\n",
      "Valid Loss: 5412.23876953125\n",
      "Train Loss: 2467.0087890625\n",
      "Valid Loss: 6337.11572265625\n",
      "Train Loss: 2455.0771484375\n",
      "Valid Loss: 5585.9130859375\n",
      "Train Loss: 2472.239013671875\n",
      "Valid Loss: 5550.0595703125\n",
      "Train Loss: 2404.346923828125\n",
      "Valid Loss: 5783.13671875\n",
      "Train Loss: 2390.224365234375\n",
      "Valid Loss: 5628.13818359375\n",
      "Train Loss: 2464.191650390625\n",
      "Valid Loss: 5717.33056640625\n",
      "Train Loss: 2453.129638671875\n",
      "Valid Loss: 5460.171875\n",
      "Train Loss: 2414.116943359375\n",
      "Valid Loss: 6335.443359375\n",
      "Train Loss: 2468.80615234375\n",
      "Valid Loss: 6333.8310546875\n",
      "Train Loss: 2453.333251953125\n",
      "Valid Loss: 5438.8095703125\n",
      "Train Loss: 2452.641357421875\n",
      "Valid Loss: 5482.85693359375\n",
      "Train Loss: 2453.789306640625\n",
      "Valid Loss: 6214.87158203125\n",
      "Train Loss: 2406.628662109375\n",
      "Valid Loss: 5459.51025390625\n",
      "Train Loss: 2453.888427734375\n",
      "Valid Loss: 5462.40234375\n",
      "Train Loss: 2447.8359375\n",
      "Valid Loss: 5587.19287109375\n",
      "Train Loss: 2421.294677734375\n",
      "Valid Loss: 5400.63623046875\n",
      "Train Loss: 2423.736083984375\n",
      "Valid Loss: 5457.89306640625\n",
      "Train Loss: 2456.746337890625\n",
      "Valid Loss: 6226.0537109375\n",
      "Train Loss: 2455.466796875\n",
      "Valid Loss: 5733.88232421875\n",
      "Train Loss: 2427.2255859375\n",
      "Valid Loss: 5457.8525390625\n",
      "Train Loss: 2461.7998046875\n",
      "Valid Loss: 5420.3583984375\n",
      "Train Loss: 2415.603515625\n",
      "Valid Loss: 5414.08447265625\n",
      "Train Loss: 2450.743896484375\n",
      "Valid Loss: 6219.09423828125\n",
      "Train Loss: 2454.97412109375\n",
      "Valid Loss: 6304.1640625\n",
      "Train Loss: 2453.50341796875\n",
      "Valid Loss: 5551.38916015625\n",
      "Train Loss: 2460.60546875\n",
      "Valid Loss: 5431.681640625\n",
      "Train Loss: 2426.583984375\n",
      "Valid Loss: 6318.26318359375\n",
      "Train Loss: 2444.352294921875\n",
      "Valid Loss: 6331.681640625\n",
      "Train Loss: 2444.5576171875\n",
      "Valid Loss: 6333.5029296875\n",
      "Train Loss: 2460.368896484375\n",
      "Valid Loss: 5514.3154296875\n",
      "Train Loss: 2400.805419921875\n",
      "Valid Loss: 5457.40087890625\n",
      "Train Loss: 2446.994873046875\n",
      "Valid Loss: 5459.51513671875\n",
      "Train Loss: 2438.72265625\n",
      "Valid Loss: 5420.18896484375\n",
      "Train Loss: 2444.3115234375\n",
      "Valid Loss: 5547.53271484375\n",
      "Train Loss: 2463.20458984375\n",
      "Valid Loss: 5741.50537109375\n",
      "Train Loss: 2458.587646484375\n",
      "Valid Loss: 5454.8935546875\n",
      "Train Loss: 2458.3408203125\n",
      "Valid Loss: 5735.5849609375\n",
      "Train Loss: 2459.276123046875\n",
      "Valid Loss: 5782.42822265625\n",
      "Train Loss: 2457.791015625\n",
      "Valid Loss: 5442.1240234375\n",
      "Train Loss: 2446.78662109375\n",
      "Valid Loss: 5734.7919921875\n",
      "Train Loss: 2462.827880859375\n",
      "Valid Loss: 5733.23193359375\n",
      "Train Loss: 2411.423095703125\n",
      "Valid Loss: 5620.171875\n",
      "Train Loss: 2431.024169921875\n",
      "Valid Loss: 5627.21533203125\n",
      "Train Loss: 2452.99462890625\n",
      "Valid Loss: 5781.9443359375\n",
      "Train Loss: 2473.121826171875\n",
      "Valid Loss: 5777.72216796875\n",
      "Train Loss: 2462.84033203125\n",
      "Valid Loss: 6324.69775390625\n",
      "Train Loss: 2459.0498046875\n",
      "Valid Loss: 5459.15771484375\n",
      "Train Loss: 2474.660888671875\n",
      "Valid Loss: 5419.97607421875\n",
      "Train Loss: 2454.2646484375\n",
      "Valid Loss: 5458.5625\n",
      "Train Loss: 2431.41845703125\n",
      "Valid Loss: 5683.3466796875\n",
      "Train Loss: 2448.5888671875\n",
      "Valid Loss: 5753.37353515625\n",
      "Train Loss: 2380.3818359375\n",
      "Valid Loss: 5457.65576171875\n",
      "Train Loss: 2458.63037109375\n",
      "Valid Loss: 5627.37646484375\n",
      "Train Loss: 2451.69140625\n",
      "Valid Loss: 6326.4951171875\n",
      "Train Loss: 2451.53076171875\n",
      "Valid Loss: 6329.01220703125\n",
      "Train Loss: 2461.8095703125\n",
      "Valid Loss: 5742.75537109375\n",
      "Train Loss: 2463.975341796875\n",
      "Valid Loss: 5444.70849609375\n",
      "Train Loss: 2452.079833984375\n",
      "Valid Loss: 5457.1826171875\n",
      "Train Loss: 2435.092041015625\n",
      "Valid Loss: 5441.28759765625\n",
      "Train Loss: 2365.030517578125\n",
      "Valid Loss: 5731.287109375\n",
      "Train Loss: 2446.7802734375\n",
      "Valid Loss: 5500.64697265625\n",
      "Train Loss: 2467.031005859375\n",
      "Valid Loss: 5459.31982421875\n",
      "Train Loss: 2396.853271484375\n",
      "Valid Loss: 5419.75927734375\n",
      "Train Loss: 2434.98779296875\n",
      "Valid Loss: 5424.64892578125\n",
      "Train Loss: 2458.167236328125\n",
      "Valid Loss: 5461.13916015625\n",
      "Train Loss: 2424.61376953125\n",
      "Valid Loss: 6218.50537109375\n",
      "Train Loss: 2468.58154296875\n",
      "Valid Loss: 5735.158203125\n",
      "Train Loss: 2461.079833984375\n",
      "Valid Loss: 5458.841796875\n",
      "Train Loss: 2429.115966796875\n",
      "Valid Loss: 6334.5283203125\n",
      "Train Loss: 2468.401611328125\n",
      "Valid Loss: 5461.8779296875\n",
      "Train Loss: 2420.12060546875\n",
      "Valid Loss: 5550.77001953125\n",
      "Train Loss: 2395.790283203125\n",
      "Valid Loss: 5475.640625\n",
      "Train Loss: 2431.10205078125\n",
      "Valid Loss: 5458.6650390625\n",
      "Train Loss: 2469.41357421875\n",
      "Valid Loss: 5459.08837890625\n",
      "Train Loss: 2450.930908203125\n",
      "Valid Loss: 5437.13671875\n",
      "Train Loss: 2453.605224609375\n",
      "Valid Loss: 5419.68212890625\n",
      "Train Loss: 2449.421875\n",
      "Valid Loss: 5482.80810546875\n",
      "Train Loss: 2418.253662109375\n",
      "Valid Loss: 5461.5\n",
      "Train Loss: 2444.261962890625\n",
      "Valid Loss: 6211.04443359375\n",
      "Train Loss: 2440.566162109375\n",
      "Valid Loss: 5461.62158203125\n",
      "Train Loss: 2453.02099609375\n",
      "Valid Loss: 5420.0830078125\n",
      "Train Loss: 2404.798095703125\n",
      "Valid Loss: 6330.912109375\n",
      "Train Loss: 2472.20947265625\n",
      "Valid Loss: 5456.6728515625\n",
      "Train Loss: 2458.504638671875\n",
      "Valid Loss: 5584.63720703125\n",
      "Train Loss: 2435.279541015625\n",
      "Valid Loss: 5428.158203125\n",
      "Train Loss: 2471.767333984375\n",
      "Valid Loss: 5509.1240234375\n",
      "Train Loss: 2450.873291015625\n",
      "Valid Loss: 5781.86474609375\n",
      "Train Loss: 2454.15478515625\n",
      "Valid Loss: 5734.77880859375\n",
      "Train Loss: 2419.865234375\n",
      "Valid Loss: 5456.74267578125\n",
      "Train Loss: 2444.9365234375\n",
      "Valid Loss: 5574.04345703125\n",
      "Train Loss: 2419.341552734375\n",
      "Valid Loss: 5732.85498046875\n",
      "Train Loss: 2457.1875\n",
      "Valid Loss: 5428.0546875\n",
      "Train Loss: 2439.96337890625\n",
      "Valid Loss: 5546.77099609375\n",
      "Train Loss: 2432.51220703125\n",
      "Valid Loss: 5461.0947265625\n",
      "Train Loss: 2421.9990234375\n",
      "Valid Loss: 5581.34716796875\n",
      "Train Loss: 2430.798583984375\n",
      "Valid Loss: 5457.6435546875\n",
      "Train Loss: 2452.64892578125\n",
      "Valid Loss: 5547.90869140625\n",
      "Train Loss: 2428.603271484375\n",
      "Valid Loss: 5754.130859375\n",
      "Train Loss: 2445.14111328125\n",
      "Valid Loss: 6214.11865234375\n",
      "Train Loss: 2412.76171875\n",
      "Valid Loss: 5548.9736328125\n",
      "Train Loss: 2469.71044921875\n",
      "Valid Loss: 5731.142578125\n",
      "Train Loss: 2469.567626953125\n",
      "Valid Loss: 5707.64208984375\n",
      "Train Loss: 2446.091552734375\n",
      "Valid Loss: 6320.9091796875\n",
      "Train Loss: 2452.625\n",
      "Valid Loss: 6208.46875\n",
      "Train Loss: 2472.906982421875\n",
      "Valid Loss: 5520.47412109375\n",
      "Train Loss: 2449.955810546875\n",
      "Valid Loss: 5781.57666015625\n",
      "Train Loss: 2432.405517578125\n",
      "Valid Loss: 5731.1103515625\n",
      "Train Loss: 2454.071533203125\n",
      "Valid Loss: 5458.25244140625\n",
      "Train Loss: 2436.83642578125\n",
      "Valid Loss: 5481.48388671875\n",
      "Train Loss: 2453.849609375\n",
      "Valid Loss: 5723.70556640625\n",
      "Train Loss: 2471.551513671875\n",
      "Valid Loss: 5604.06298828125\n",
      "Train Loss: 2452.967529296875\n",
      "Valid Loss: 5626.14208984375\n",
      "Train Loss: 2453.341796875\n",
      "Valid Loss: 5448.34912109375\n",
      "Train Loss: 2458.685546875\n",
      "Valid Loss: 5439.28662109375\n",
      "Train Loss: 2460.407958984375\n",
      "Valid Loss: 5482.26123046875\n",
      "Train Loss: 2455.984375\n",
      "Valid Loss: 5415.3994140625\n",
      "Train Loss: 2394.92529296875\n",
      "Valid Loss: 5456.49462890625\n",
      "Train Loss: 2421.93505859375\n",
      "Valid Loss: 5456.32666015625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2476.608642578125\n",
      "Valid Loss: 5458.23291015625\n",
      "Train Loss: 2452.859375\n",
      "Valid Loss: 6331.06103515625\n",
      "Train Loss: 2466.884765625\n",
      "Valid Loss: 5545.83935546875\n",
      "Train Loss: 2437.33740234375\n",
      "Valid Loss: 5550.11279296875\n",
      "Train Loss: 2468.36669921875\n",
      "Valid Loss: 5548.6376953125\n",
      "Train Loss: 2433.28173828125\n",
      "Valid Loss: 6323.55224609375\n",
      "Train Loss: 2462.040771484375\n",
      "Valid Loss: 5441.7158203125\n",
      "Train Loss: 2470.5263671875\n",
      "Valid Loss: 5421.51708984375\n",
      "Train Loss: 2471.240966796875\n",
      "Valid Loss: 6316.80322265625\n",
      "Train Loss: 2441.282958984375\n",
      "Valid Loss: 6331.98681640625\n",
      "Train Loss: 2465.858154296875\n",
      "Valid Loss: 5508.693359375\n",
      "Train Loss: 2432.349609375\n",
      "Valid Loss: 5455.9814453125\n",
      "Train Loss: 2443.96533203125\n",
      "Valid Loss: 5458.23388671875\n",
      "Train Loss: 2458.98193359375\n",
      "Valid Loss: 5548.4658203125\n",
      "Train Loss: 2421.21923828125\n",
      "Valid Loss: 5481.693359375\n",
      "Train Loss: 2456.505126953125\n",
      "Valid Loss: 5734.0947265625\n",
      "Train Loss: 2422.50390625\n",
      "Valid Loss: 6335.36083984375\n",
      "Train Loss: 2434.0458984375\n",
      "Valid Loss: 5580.71435546875\n",
      "Train Loss: 2412.171875\n",
      "Valid Loss: 5420.681640625\n",
      "Train Loss: 2472.71337890625\n",
      "Valid Loss: 6308.51953125\n",
      "Train Loss: 2456.524169921875\n",
      "Valid Loss: 6331.81591796875\n",
      "Train Loss: 2458.982177734375\n",
      "Valid Loss: 6210.28759765625\n",
      "Train Loss: 2454.921875\n",
      "Valid Loss: 5519.93603515625\n",
      "Train Loss: 2441.750732421875\n",
      "Valid Loss: 6335.251953125\n",
      "Train Loss: 2364.229248046875\n",
      "Valid Loss: 5459.63671875\n",
      "Train Loss: 2411.610595703125\n",
      "Valid Loss: 6327.9873046875\n",
      "Train Loss: 2473.75341796875\n",
      "Valid Loss: 5733.98681640625\n",
      "Train Loss: 2453.056884765625\n",
      "Valid Loss: 5457.9306640625\n",
      "Train Loss: 2457.28466796875\n",
      "Valid Loss: 6211.984375\n",
      "Train Loss: 2466.863525390625\n",
      "Valid Loss: 5479.57666015625\n",
      "Train Loss: 2447.231201171875\n",
      "Valid Loss: 5455.8486328125\n",
      "Train Loss: 2468.222412109375\n",
      "Valid Loss: 5548.955078125\n",
      "Train Loss: 2467.693603515625\n",
      "Valid Loss: 5733.82763671875\n",
      "Train Loss: 2441.540283203125\n",
      "Valid Loss: 6222.87060546875\n",
      "Train Loss: 2444.93017578125\n",
      "Valid Loss: 6214.19970703125\n",
      "Train Loss: 2443.583740234375\n",
      "Valid Loss: 5625.15185546875\n",
      "Train Loss: 2440.8203125\n",
      "Valid Loss: 6197.685546875\n",
      "Train Loss: 2417.591064453125\n",
      "Valid Loss: 5780.7939453125\n",
      "Train Loss: 2450.8466796875\n",
      "Valid Loss: 6208.748046875\n",
      "Train Loss: 2443.03076171875\n",
      "Valid Loss: 5696.693359375\n",
      "Train Loss: 2441.251953125\n",
      "Valid Loss: 5548.78759765625\n",
      "Train Loss: 2437.4501953125\n",
      "Valid Loss: 6327.63134765625\n",
      "Train Loss: 2448.21337890625\n",
      "Valid Loss: 5549.17529296875\n",
      "Train Loss: 2451.959716796875\n",
      "Valid Loss: 5456.85693359375\n",
      "Train Loss: 2465.943115234375\n",
      "Valid Loss: 6195.978515625\n",
      "Train Loss: 2425.13427734375\n",
      "Valid Loss: 5456.998046875\n",
      "Train Loss: 2436.74658203125\n",
      "Valid Loss: 5733.66552734375\n",
      "Train Loss: 2450.633544921875\n",
      "Valid Loss: 5624.70751953125\n",
      "Train Loss: 2452.17236328125\n",
      "Valid Loss: 5443.1376953125\n",
      "Train Loss: 2427.315673828125\n",
      "Valid Loss: 5481.31591796875\n",
      "Train Loss: 2456.50634765625\n",
      "Valid Loss: 6216.982421875\n",
      "Train Loss: 2474.419677734375\n",
      "Valid Loss: 6313.7861328125\n",
      "Train Loss: 2425.883056640625\n",
      "Valid Loss: 5414.55078125\n",
      "Train Loss: 2449.623046875\n",
      "Valid Loss: 6327.43603515625\n",
      "Train Loss: 2467.61181640625\n",
      "Valid Loss: 5779.24365234375\n",
      "Train Loss: 2450.919921875\n",
      "Valid Loss: 5577.791015625\n",
      "Train Loss: 2418.27392578125\n",
      "Valid Loss: 5457.58837890625\n",
      "Train Loss: 2462.3408203125\n",
      "Valid Loss: 6312.42529296875\n",
      "Train Loss: 2434.32275390625\n",
      "Valid Loss: 6221.96533203125\n",
      "Train Loss: 2461.50146484375\n",
      "Valid Loss: 5625.453125\n",
      "Train Loss: 2464.815185546875\n",
      "Valid Loss: 5548.89013671875\n",
      "Train Loss: 2372.247314453125\n",
      "Valid Loss: 5457.126953125\n",
      "Train Loss: 2439.87841796875\n",
      "Valid Loss: 5732.65283203125\n",
      "Train Loss: 2434.648681640625\n",
      "Valid Loss: 5430.82470703125\n",
      "Train Loss: 2461.392822265625\n",
      "Valid Loss: 5455.306640625\n",
      "Train Loss: 2428.25439453125\n",
      "Valid Loss: 6301.74755859375\n",
      "Train Loss: 2459.082763671875\n",
      "Valid Loss: 5625.0322265625\n",
      "Train Loss: 2416.461181640625\n",
      "Valid Loss: 6208.287109375\n",
      "Train Loss: 2444.849609375\n",
      "Valid Loss: 6207.1962890625\n",
      "Train Loss: 2392.848876953125\n",
      "Valid Loss: 5598.900390625\n",
      "Train Loss: 2429.7421875\n",
      "Valid Loss: 5479.30126953125\n",
      "Train Loss: 2443.136962890625\n",
      "Valid Loss: 5458.365234375\n",
      "Train Loss: 2469.3212890625\n",
      "Valid Loss: 5481.23681640625\n",
      "Train Loss: 2439.14990234375\n",
      "Valid Loss: 5733.15478515625\n",
      "Train Loss: 2412.1806640625\n",
      "Valid Loss: 6223.49267578125\n",
      "Train Loss: 2454.787109375\n",
      "Valid Loss: 5481.0302734375\n",
      "Train Loss: 2440.942138671875\n",
      "Valid Loss: 6335.07861328125\n",
      "Train Loss: 2415.77978515625\n",
      "Valid Loss: 6330.31689453125\n",
      "Train Loss: 2439.23828125\n",
      "Valid Loss: 5459.2958984375\n",
      "Train Loss: 2444.211669921875\n",
      "Valid Loss: 5752.5087890625\n",
      "Train Loss: 2393.66455078125\n",
      "Valid Loss: 6199.39599609375\n",
      "Train Loss: 2444.890869140625\n",
      "Valid Loss: 5732.8193359375\n",
      "Train Loss: 2463.6455078125\n",
      "Valid Loss: 5625.0712890625\n",
      "Train Loss: 2421.059814453125\n",
      "Valid Loss: 6322.28662109375\n",
      "Train Loss: 2418.7314453125\n",
      "Valid Loss: 5456.8837890625\n",
      "Train Loss: 2447.841064453125\n",
      "Valid Loss: 5750.9453125\n",
      "Train Loss: 2393.253173828125\n",
      "Valid Loss: 5409.10546875\n",
      "Train Loss: 2406.668212890625\n",
      "Valid Loss: 5456.93603515625\n",
      "Train Loss: 2430.3828125\n",
      "Valid Loss: 5423.64892578125\n",
      "Train Loss: 2451.891845703125\n",
      "Valid Loss: 6196.646484375\n",
      "Train Loss: 2463.30712890625\n",
      "Valid Loss: 6221.9541015625\n",
      "Train Loss: 2425.5771484375\n",
      "Valid Loss: 6213.28173828125\n",
      "Train Loss: 2444.3642578125\n",
      "Valid Loss: 5459.78369140625\n",
      "Train Loss: 2445.056640625\n",
      "Valid Loss: 5456.0166015625\n",
      "Train Loss: 2404.10595703125\n",
      "Valid Loss: 5477.85205078125\n",
      "Train Loss: 2456.56689453125\n",
      "Valid Loss: 6332.7177734375\n",
      "Train Loss: 2449.669189453125\n",
      "Valid Loss: 5480.3818359375\n",
      "Train Loss: 2459.006103515625\n",
      "Valid Loss: 5617.4423828125\n",
      "Train Loss: 2470.629638671875\n",
      "Valid Loss: 6219.21435546875\n",
      "Train Loss: 2436.759765625\n",
      "Valid Loss: 6332.232421875\n",
      "Train Loss: 2426.40966796875\n",
      "Valid Loss: 5624.38671875\n",
      "Train Loss: 2455.7255859375\n",
      "Valid Loss: 5619.87353515625\n",
      "Train Loss: 2370.091796875\n",
      "Valid Loss: 5548.45166015625\n",
      "Train Loss: 2451.776611328125\n",
      "Valid Loss: 5454.8310546875\n",
      "Train Loss: 2465.462890625\n",
      "Valid Loss: 5548.580078125\n",
      "Train Loss: 2445.60888671875\n",
      "Valid Loss: 5624.10546875\n",
      "Train Loss: 2441.96875\n",
      "Valid Loss: 5544.71044921875\n",
      "Train Loss: 2450.5068359375\n",
      "Valid Loss: 6320.08154296875\n",
      "Train Loss: 2426.602783203125\n",
      "Valid Loss: 5456.57080078125\n",
      "Train Loss: 2439.785888671875\n",
      "Valid Loss: 5777.94921875\n",
      "Train Loss: 2408.551025390625\n",
      "Valid Loss: 6326.46435546875\n",
      "Train Loss: 2420.366455078125\n",
      "Valid Loss: 5745.919921875\n",
      "Train Loss: 2467.6845703125\n",
      "Valid Loss: 5459.525390625\n",
      "Train Loss: 2396.395751953125\n",
      "Valid Loss: 5779.734375\n",
      "Train Loss: 2471.89208984375\n",
      "Valid Loss: 5774.7216796875\n",
      "Train Loss: 2418.63232421875\n",
      "Valid Loss: 6207.6279296875\n",
      "Train Loss: 2417.9931640625\n",
      "Valid Loss: 5776.76953125\n",
      "Train Loss: 2462.364013671875\n",
      "Valid Loss: 5459.3017578125\n",
      "Train Loss: 2412.7578125\n",
      "Valid Loss: 5596.64306640625\n",
      "Train Loss: 2459.66943359375\n",
      "Valid Loss: 6325.2119140625\n",
      "Train Loss: 2476.69384765625\n",
      "Valid Loss: 6319.8515625\n",
      "Train Loss: 2445.871337890625\n",
      "Valid Loss: 5423.06396484375\n",
      "Train Loss: 2428.80078125\n",
      "Valid Loss: 5480.1005859375\n",
      "Train Loss: 2457.83544921875\n",
      "Valid Loss: 5745.701171875\n",
      "Train Loss: 2449.494140625\n",
      "Valid Loss: 5441.81103515625\n",
      "Train Loss: 2430.704833984375\n",
      "Valid Loss: 5458.72021484375\n",
      "Train Loss: 2432.81103515625\n",
      "Valid Loss: 6333.23681640625\n",
      "Train Loss: 2389.523193359375\n",
      "Valid Loss: 5539.53564453125\n",
      "Train Loss: 2437.875732421875\n",
      "Valid Loss: 6211.3994140625\n",
      "Train Loss: 2444.18701171875\n",
      "Valid Loss: 5519.4384765625\n",
      "Train Loss: 2450.5087890625\n",
      "Valid Loss: 5416.92431640625\n",
      "Train Loss: 2405.64990234375\n",
      "Valid Loss: 5731.94140625\n",
      "Train Loss: 2450.802001953125\n",
      "Valid Loss: 5663.9853515625\n",
      "Train Loss: 2420.46142578125\n",
      "Valid Loss: 5545.43701171875\n",
      "Train Loss: 2418.951416015625\n",
      "Valid Loss: 5779.09423828125\n",
      "Train Loss: 2403.650634765625\n",
      "Valid Loss: 6194.4580078125\n",
      "Train Loss: 2438.849365234375\n",
      "Valid Loss: 6334.02685546875\n",
      "Train Loss: 2462.615234375\n",
      "Valid Loss: 5445.82568359375\n",
      "Train Loss: 2465.140625\n",
      "Valid Loss: 5451.3623046875\n",
      "Train Loss: 2441.78515625\n",
      "Valid Loss: 6331.52001953125\n",
      "Train Loss: 2398.20458984375\n",
      "Valid Loss: 5455.87548828125\n",
      "Train Loss: 2404.216796875\n",
      "Valid Loss: 5456.11572265625\n",
      "Train Loss: 2430.051513671875\n",
      "Valid Loss: 5704.97509765625\n",
      "Train Loss: 2448.606689453125\n",
      "Valid Loss: 5451.27001953125\n",
      "Train Loss: 2453.3193359375\n",
      "Valid Loss: 6208.11669921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2456.514404296875\n",
      "Valid Loss: 5445.681640625\n",
      "Train Loss: 2452.88525390625\n",
      "Valid Loss: 5430.53759765625\n",
      "Train Loss: 2452.48681640625\n",
      "Valid Loss: 5546.9931640625\n",
      "Train Loss: 2423.75927734375\n",
      "Valid Loss: 6324.68359375\n",
      "Train Loss: 2459.2177734375\n",
      "Valid Loss: 5602.48876953125\n",
      "Train Loss: 2401.29638671875\n",
      "Valid Loss: 5455.54296875\n",
      "Train Loss: 2425.245361328125\n",
      "Valid Loss: 5479.880859375\n",
      "Train Loss: 2447.2294921875\n",
      "Valid Loss: 5445.55419921875\n",
      "Train Loss: 2451.671875\n",
      "Valid Loss: 5441.29296875\n",
      "Train Loss: 2454.382080078125\n",
      "Valid Loss: 5546.64453125\n",
      "Train Loss: 2413.187255859375\n",
      "Valid Loss: 6325.64599609375\n",
      "Train Loss: 2449.38720703125\n",
      "Valid Loss: 5476.7685546875\n",
      "Train Loss: 2451.895751953125\n",
      "Valid Loss: 6322.86962890625\n",
      "Train Loss: 2444.5126953125\n",
      "Valid Loss: 5393.7939453125\n",
      "Train Loss: 2431.49658203125\n",
      "Valid Loss: 6210.845703125\n",
      "Train Loss: 2471.464111328125\n",
      "Valid Loss: 5546.99169921875\n",
      "Train Loss: 2397.10498046875\n",
      "Valid Loss: 5694.74560546875\n",
      "Train Loss: 2441.51220703125\n",
      "Valid Loss: 5547.14013671875\n",
      "Train Loss: 2417.446533203125\n",
      "Valid Loss: 6201.68701171875\n",
      "Train Loss: 2434.841064453125\n",
      "Valid Loss: 5478.62060546875\n",
      "Train Loss: 2454.19580078125\n",
      "Valid Loss: 5750.93310546875\n",
      "Train Loss: 2437.692626953125\n",
      "Valid Loss: 5622.5673828125\n",
      "Train Loss: 2448.695556640625\n",
      "Valid Loss: 6328.22412109375\n",
      "Train Loss: 2449.490234375\n",
      "Valid Loss: 5433.5693359375\n",
      "Train Loss: 2461.26708984375\n",
      "Valid Loss: 5775.7802734375\n",
      "Train Loss: 2417.00537109375\n",
      "Valid Loss: 6195.22216796875\n",
      "Train Loss: 2425.69775390625\n",
      "Valid Loss: 5731.396484375\n",
      "Train Loss: 2437.681640625\n",
      "Valid Loss: 5749.333984375\n",
      "Train Loss: 2440.403564453125\n",
      "Valid Loss: 6299.8134765625\n",
      "Train Loss: 2472.422607421875\n",
      "Valid Loss: 5455.47705078125\n",
      "Train Loss: 2466.524169921875\n",
      "Valid Loss: 5546.87939453125\n",
      "Train Loss: 2441.0869140625\n",
      "Valid Loss: 6332.58544921875\n",
      "Train Loss: 2455.24072265625\n",
      "Valid Loss: 5447.68798828125\n",
      "Train Loss: 2464.94970703125\n",
      "Valid Loss: 5546.6591796875\n",
      "Train Loss: 2431.22314453125\n",
      "Valid Loss: 6201.87451171875\n",
      "Train Loss: 2440.23974609375\n",
      "Valid Loss: 6310.22119140625\n",
      "Train Loss: 2403.103515625\n",
      "Valid Loss: 5457.70654296875\n",
      "Train Loss: 2438.340087890625\n",
      "Valid Loss: 5437.76953125\n",
      "Train Loss: 2464.13916015625\n",
      "Valid Loss: 5730.259765625\n",
      "Train Loss: 2442.97021484375\n",
      "Valid Loss: 5692.61328125\n",
      "Train Loss: 2453.1279296875\n",
      "Valid Loss: 5457.8076171875\n",
      "Train Loss: 2441.896728515625\n",
      "Valid Loss: 5547.1103515625\n",
      "Train Loss: 2434.803955078125\n",
      "Valid Loss: 5456.32861328125\n",
      "Train Loss: 2455.092529296875\n",
      "Valid Loss: 5453.04150390625\n",
      "Train Loss: 2423.2138671875\n",
      "Valid Loss: 6317.38671875\n",
      "Train Loss: 2396.4619140625\n",
      "Valid Loss: 5623.13134765625\n",
      "Train Loss: 2464.92724609375\n",
      "Valid Loss: 5622.11083984375\n",
      "Train Loss: 2443.80810546875\n",
      "Valid Loss: 5479.39697265625\n",
      "Train Loss: 2422.012451171875\n",
      "Valid Loss: 5478.6376953125\n",
      "Train Loss: 2436.493896484375\n",
      "Valid Loss: 5729.056640625\n",
      "Train Loss: 2450.6796875\n",
      "Valid Loss: 6324.97607421875\n",
      "Train Loss: 2414.001953125\n",
      "Valid Loss: 5407.12353515625\n",
      "Train Loss: 2458.720947265625\n",
      "Valid Loss: 5454.64794921875\n",
      "Train Loss: 2442.98046875\n",
      "Valid Loss: 5727.47607421875\n",
      "Train Loss: 2456.44775390625\n",
      "Valid Loss: 6194.669921875\n",
      "Train Loss: 2456.62890625\n",
      "Valid Loss: 5729.88916015625\n",
      "Train Loss: 2406.534912109375\n",
      "Valid Loss: 5577.61962890625\n",
      "Train Loss: 2470.916748046875\n",
      "Valid Loss: 5475.8447265625\n",
      "Train Loss: 2471.98046875\n",
      "Valid Loss: 5409.06494140625\n",
      "Train Loss: 2455.90380859375\n",
      "Valid Loss: 6321.06201171875\n",
      "Train Loss: 2446.716552734375\n",
      "Valid Loss: 5622.07080078125\n",
      "Train Loss: 2421.1064453125\n",
      "Valid Loss: 5457.78857421875\n",
      "Train Loss: 2431.593505859375\n",
      "Valid Loss: 6323.5361328125\n",
      "Train Loss: 2446.34716796875\n",
      "Valid Loss: 5452.6376953125\n",
      "Train Loss: 2463.2177734375\n",
      "Valid Loss: 6219.35498046875\n",
      "Train Loss: 2442.97021484375\n",
      "Valid Loss: 6311.05322265625\n",
      "Train Loss: 2446.64404296875\n",
      "Valid Loss: 5719.74169921875\n",
      "Train Loss: 2455.441650390625\n",
      "Valid Loss: 6305.056640625\n",
      "Train Loss: 2407.417236328125\n",
      "Valid Loss: 6204.60107421875\n",
      "Train Loss: 2405.753662109375\n",
      "Valid Loss: 5697.845703125\n",
      "Train Loss: 2464.84130859375\n",
      "Valid Loss: 5622.1962890625\n",
      "Train Loss: 2454.93603515625\n",
      "Valid Loss: 5453.45849609375\n",
      "Train Loss: 2462.459716796875\n",
      "Valid Loss: 5456.7841796875\n",
      "Train Loss: 2469.1416015625\n",
      "Valid Loss: 6321.78515625\n",
      "Train Loss: 2404.482421875\n",
      "Valid Loss: 5454.34375\n",
      "Train Loss: 2424.48974609375\n",
      "Valid Loss: 6328.216796875\n",
      "Train Loss: 2449.9765625\n",
      "Valid Loss: 5599.7958984375\n",
      "Train Loss: 2428.53076171875\n",
      "Valid Loss: 5444.19873046875\n",
      "Train Loss: 2412.170166015625\n",
      "Valid Loss: 6324.30078125\n",
      "Train Loss: 2443.7236328125\n",
      "Valid Loss: 6196.7763671875\n",
      "Train Loss: 2423.85302734375\n",
      "Valid Loss: 6194.34326171875\n",
      "Train Loss: 2420.66552734375\n",
      "Valid Loss: 6209.55419921875\n",
      "Train Loss: 2430.2666015625\n",
      "Valid Loss: 5575.21484375\n",
      "Train Loss: 2456.8916015625\n",
      "Valid Loss: 5426.376953125\n",
      "Train Loss: 2440.728515625\n",
      "Valid Loss: 5453.4228515625\n",
      "Train Loss: 2461.24853515625\n",
      "Valid Loss: 5621.0224609375\n",
      "Train Loss: 2458.634521484375\n",
      "Valid Loss: 5776.63037109375\n",
      "Train Loss: 2399.19921875\n",
      "Valid Loss: 5546.037109375\n",
      "Train Loss: 2427.1083984375\n",
      "Valid Loss: 5457.0048828125\n",
      "Train Loss: 2432.467041015625\n",
      "Valid Loss: 5621.83642578125\n",
      "Train Loss: 2399.898193359375\n",
      "Valid Loss: 6327.90185546875\n",
      "Train Loss: 2445.095947265625\n",
      "Valid Loss: 5545.30908203125\n",
      "Train Loss: 2443.92578125\n",
      "Valid Loss: 5470.8232421875\n",
      "Train Loss: 2460.56982421875\n",
      "Valid Loss: 5728.19482421875\n",
      "Train Loss: 2448.31005859375\n",
      "Valid Loss: 5437.56884765625\n",
      "Train Loss: 2407.55078125\n",
      "Valid Loss: 5726.60791015625\n",
      "Train Loss: 2364.3466796875\n",
      "Valid Loss: 5777.189453125\n",
      "Train Loss: 2437.969482421875\n",
      "Valid Loss: 5476.69482421875\n",
      "Train Loss: 2454.471923828125\n",
      "Valid Loss: 6209.20556640625\n",
      "Train Loss: 2418.2099609375\n",
      "Valid Loss: 6331.19775390625\n",
      "Train Loss: 2426.52392578125\n",
      "Valid Loss: 5478.048828125\n",
      "Train Loss: 2461.0947265625\n",
      "Valid Loss: 5776.30322265625\n",
      "Train Loss: 2455.7978515625\n",
      "Valid Loss: 5727.615234375\n",
      "Train Loss: 2465.83984375\n",
      "Valid Loss: 5414.6298828125\n",
      "Train Loss: 2412.265869140625\n",
      "Valid Loss: 6331.0908203125\n",
      "Train Loss: 2436.48779296875\n",
      "Valid Loss: 5460.87255859375\n",
      "Train Loss: 2410.7705078125\n",
      "Valid Loss: 6193.615234375\n",
      "Train Loss: 2454.384521484375\n",
      "Valid Loss: 5452.59130859375\n",
      "Train Loss: 2440.59521484375\n",
      "Valid Loss: 5430.06103515625\n",
      "Train Loss: 2435.458740234375\n",
      "Valid Loss: 5477.4013671875\n",
      "Train Loss: 2467.08056640625\n",
      "Valid Loss: 5675.8349609375\n",
      "Train Loss: 2454.402099609375\n",
      "Valid Loss: 5616.298828125\n",
      "Train Loss: 2463.633056640625\n",
      "Valid Loss: 5472.71630859375\n",
      "Train Loss: 2449.872802734375\n",
      "Valid Loss: 5451.93603515625\n",
      "Train Loss: 2377.264404296875\n",
      "Valid Loss: 5545.2197265625\n",
      "Train Loss: 2453.01025390625\n",
      "Valid Loss: 5427.025390625\n",
      "Train Loss: 2428.2060546875\n",
      "Valid Loss: 5410.673828125\n",
      "Train Loss: 2460.94189453125\n",
      "Valid Loss: 5453.650390625\n",
      "Train Loss: 2442.0556640625\n",
      "Valid Loss: 5724.76708984375\n",
      "Train Loss: 2403.315673828125\n",
      "Valid Loss: 5477.3701171875\n",
      "Train Loss: 2437.227783203125\n",
      "Valid Loss: 5475.6552734375\n",
      "Train Loss: 2444.611083984375\n",
      "Valid Loss: 5726.4580078125\n",
      "Train Loss: 2465.64404296875\n",
      "Valid Loss: 5508.4296875\n",
      "Train Loss: 2460.045166015625\n",
      "Valid Loss: 5728.484375\n",
      "Train Loss: 2436.734619140625\n",
      "Valid Loss: 6216.54296875\n",
      "Train Loss: 2444.93505859375\n",
      "Valid Loss: 6205.77392578125\n",
      "Train Loss: 2458.518798828125\n",
      "Valid Loss: 5476.625\n",
      "Train Loss: 2425.285400390625\n",
      "Valid Loss: 6323.3486328125\n",
      "Train Loss: 2392.46728515625\n",
      "Valid Loss: 5452.63720703125\n",
      "Train Loss: 2437.66015625\n",
      "Valid Loss: 5425.4013671875\n",
      "Train Loss: 2464.5654296875\n",
      "Valid Loss: 5454.1708984375\n",
      "Train Loss: 2456.5986328125\n",
      "Valid Loss: 6211.3984375\n",
      "Train Loss: 2407.2978515625\n",
      "Valid Loss: 5728.283203125\n",
      "Train Loss: 2420.742431640625\n",
      "Valid Loss: 5477.19677734375\n",
      "Train Loss: 2455.0166015625\n",
      "Valid Loss: 6207.37109375\n",
      "Train Loss: 2430.49462890625\n",
      "Valid Loss: 5620.78173828125\n",
      "Train Loss: 2445.15673828125\n",
      "Valid Loss: 5394.04638671875\n",
      "Train Loss: 2457.809814453125\n",
      "Valid Loss: 5710.1982421875\n",
      "Train Loss: 2446.08056640625\n",
      "Valid Loss: 5728.71142578125\n",
      "Train Loss: 2450.141357421875\n",
      "Valid Loss: 5541.1982421875\n",
      "Train Loss: 2444.22900390625\n",
      "Valid Loss: 5453.3037109375\n",
      "Train Loss: 2445.853271484375\n",
      "Valid Loss: 5449.85205078125\n",
      "Train Loss: 2439.203857421875\n",
      "Valid Loss: 6325.07958984375\n",
      "Train Loss: 2450.8310546875\n",
      "Valid Loss: 5728.033203125\n",
      "Train Loss: 2469.03662109375\n",
      "Valid Loss: 5774.25634765625\n",
      "Train Loss: 2423.6376953125\n",
      "Valid Loss: 5620.59130859375\n",
      "Train Loss: 2429.49755859375\n",
      "Valid Loss: 6204.0302734375\n",
      "Train Loss: 2447.236328125\n",
      "Valid Loss: 5774.18505859375\n",
      "Train Loss: 2434.838623046875\n",
      "Valid Loss: 5431.8095703125\n",
      "Train Loss: 2436.91552734375\n",
      "Valid Loss: 5726.56591796875\n",
      "Train Loss: 2450.9150390625\n",
      "Valid Loss: 5418.43603515625\n",
      "Train Loss: 2417.02783203125\n",
      "Valid Loss: 5775.70654296875\n",
      "Train Loss: 2425.71484375\n",
      "Valid Loss: 6215.34423828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2446.332275390625\n",
      "Valid Loss: 5619.7822265625\n",
      "Train Loss: 2424.626708984375\n",
      "Valid Loss: 6217.9033203125\n",
      "Train Loss: 2446.319580078125\n",
      "Valid Loss: 5435.21142578125\n",
      "Train Loss: 2469.562744140625\n",
      "Valid Loss: 6203.95361328125\n",
      "Train Loss: 2440.5537109375\n",
      "Valid Loss: 5404.87890625\n",
      "Train Loss: 2472.308349609375\n",
      "Valid Loss: 5416.5947265625\n",
      "Train Loss: 2418.437744140625\n",
      "Valid Loss: 6314.9013671875\n",
      "Train Loss: 2434.564453125\n",
      "Valid Loss: 5775.47314453125\n",
      "Train Loss: 2464.493896484375\n",
      "Valid Loss: 5727.57958984375\n",
      "Train Loss: 2443.24560546875\n",
      "Valid Loss: 5448.56396484375\n",
      "Train Loss: 2450.6513671875\n",
      "Valid Loss: 5775.41650390625\n",
      "Train Loss: 2440.916748046875\n",
      "Valid Loss: 5660.27392578125\n",
      "Train Loss: 2455.8232421875\n",
      "Valid Loss: 6315.93603515625\n",
      "Train Loss: 2420.470703125\n",
      "Valid Loss: 5544.4814453125\n",
      "Train Loss: 2444.420166015625\n",
      "Valid Loss: 5547.5048828125\n",
      "Train Loss: 2447.0595703125\n",
      "Valid Loss: 6307.33740234375\n",
      "Train Loss: 2426.910400390625\n",
      "Valid Loss: 5434.87353515625\n",
      "Train Loss: 2438.673583984375\n",
      "Valid Loss: 5728.29052734375\n",
      "Train Loss: 2426.769775390625\n",
      "Valid Loss: 5542.494140625\n",
      "Train Loss: 2415.028076171875\n",
      "Valid Loss: 5455.02685546875\n",
      "Train Loss: 2377.294921875\n",
      "Valid Loss: 5476.48291015625\n",
      "Train Loss: 2358.21630859375\n",
      "Valid Loss: 5542.76318359375\n",
      "Train Loss: 2436.27001953125\n",
      "Valid Loss: 5455.21484375\n",
      "Train Loss: 2408.03369140625\n",
      "Valid Loss: 5412.98388671875\n",
      "Train Loss: 2374.397705078125\n",
      "Valid Loss: 6321.75830078125\n",
      "Train Loss: 2424.44677734375\n",
      "Valid Loss: 5544.03369140625\n",
      "Train Loss: 2395.6337890625\n",
      "Valid Loss: 5476.3388671875\n",
      "Train Loss: 2433.08056640625\n",
      "Valid Loss: 5544.14697265625\n",
      "Train Loss: 2362.984375\n",
      "Valid Loss: 5525.0908203125\n",
      "Train Loss: 2429.7177734375\n",
      "Valid Loss: 5735.31005859375\n",
      "Train Loss: 2390.93701171875\n",
      "Valid Loss: 6211.5927734375\n",
      "Train Loss: 2437.29248046875\n",
      "Valid Loss: 5542.4814453125\n",
      "Train Loss: 2435.3720703125\n",
      "Valid Loss: 5450.26513671875\n",
      "Train Loss: 2447.830322265625\n",
      "Valid Loss: 5451.79541015625\n",
      "Train Loss: 2416.35302734375\n",
      "Valid Loss: 5674.0205078125\n",
      "Train Loss: 2449.68505859375\n",
      "Valid Loss: 6203.0458984375\n",
      "Train Loss: 2448.0986328125\n",
      "Valid Loss: 5727.630859375\n",
      "Train Loss: 2442.00244140625\n",
      "Valid Loss: 5454.990234375\n",
      "Train Loss: 2429.53955078125\n",
      "Valid Loss: 5690.7021484375\n",
      "Train Loss: 2424.0263671875\n",
      "Valid Loss: 5429.8203125\n",
      "Train Loss: 2467.61083984375\n",
      "Valid Loss: 5475.9384765625\n",
      "Train Loss: 2413.037841796875\n",
      "Valid Loss: 6302.166015625\n",
      "Train Loss: 2457.389404296875\n",
      "Valid Loss: 5542.72607421875\n",
      "Train Loss: 2440.7158203125\n",
      "Valid Loss: 5618.72802734375\n",
      "Train Loss: 2453.422119140625\n",
      "Valid Loss: 5428.017578125\n",
      "Train Loss: 2432.947265625\n",
      "Valid Loss: 5543.2724609375\n",
      "Train Loss: 2466.054931640625\n",
      "Valid Loss: 5734.86962890625\n",
      "Train Loss: 2457.670654296875\n",
      "Valid Loss: 5578.74560546875\n",
      "Train Loss: 2464.0771484375\n",
      "Valid Loss: 5474.69677734375\n",
      "Train Loss: 2421.5791015625\n",
      "Valid Loss: 6190.021484375\n",
      "Train Loss: 2438.79638671875\n",
      "Valid Loss: 6198.22216796875\n",
      "Train Loss: 2389.360595703125\n",
      "Valid Loss: 5541.9931640625\n",
      "Train Loss: 2416.3525390625\n",
      "Valid Loss: 5774.5869140625\n",
      "Train Loss: 2436.896240234375\n",
      "Valid Loss: 5542.70458984375\n",
      "Train Loss: 2407.70703125\n",
      "Valid Loss: 6216.6337890625\n",
      "Train Loss: 2427.24755859375\n",
      "Valid Loss: 5449.30224609375\n",
      "Train Loss: 2431.40673828125\n",
      "Valid Loss: 5723.93505859375\n",
      "Train Loss: 2455.32568359375\n",
      "Valid Loss: 5774.43994140625\n",
      "Train Loss: 2410.4873046875\n",
      "Valid Loss: 5451.38427734375\n",
      "Train Loss: 2390.455810546875\n",
      "Valid Loss: 6321.2890625\n",
      "Train Loss: 2421.513427734375\n",
      "Valid Loss: 5578.3994140625\n",
      "Train Loss: 2462.7802734375\n",
      "Valid Loss: 5514.55224609375\n",
      "Train Loss: 2442.616455078125\n",
      "Valid Loss: 6328.44873046875\n",
      "Train Loss: 2420.2734375\n",
      "Valid Loss: 5446.4619140625\n",
      "Train Loss: 2454.174560546875\n",
      "Valid Loss: 5726.2060546875\n",
      "Train Loss: 2448.728759765625\n",
      "Valid Loss: 5744.896484375\n",
      "Train Loss: 2380.37548828125\n",
      "Valid Loss: 5542.11962890625\n",
      "Train Loss: 2423.092041015625\n",
      "Valid Loss: 5614.2529296875\n",
      "Train Loss: 2435.4072265625\n",
      "Valid Loss: 5453.71923828125\n",
      "Train Loss: 2430.25830078125\n",
      "Valid Loss: 5542.041015625\n",
      "Train Loss: 2442.10791015625\n",
      "Valid Loss: 5453.5771484375\n",
      "Train Loss: 2457.318359375\n",
      "Valid Loss: 5452.92041015625\n",
      "Train Loss: 2430.37841796875\n",
      "Valid Loss: 5453.708984375\n",
      "Train Loss: 2447.848876953125\n",
      "Valid Loss: 5538.63330078125\n",
      "Train Loss: 2427.845703125\n",
      "Valid Loss: 5411.9267578125\n",
      "Train Loss: 2415.140625\n",
      "Valid Loss: 6211.2890625\n",
      "Train Loss: 2412.633544921875\n",
      "Valid Loss: 5570.9794921875\n",
      "Train Loss: 2427.0146484375\n",
      "Valid Loss: 5773.38525390625\n",
      "Train Loss: 2462.102783203125\n",
      "Valid Loss: 5427.97998046875\n",
      "Train Loss: 2398.229736328125\n",
      "Valid Loss: 5411.181640625\n",
      "Train Loss: 2456.651123046875\n",
      "Valid Loss: 6320.40771484375\n",
      "Train Loss: 2445.56689453125\n",
      "Valid Loss: 6208.91015625\n",
      "Train Loss: 2439.56640625\n",
      "Valid Loss: 5576.2451171875\n",
      "Train Loss: 2454.322998046875\n",
      "Valid Loss: 6322.70556640625\n",
      "Train Loss: 2466.93212890625\n",
      "Valid Loss: 5617.5908203125\n",
      "Train Loss: 2434.1953125\n",
      "Valid Loss: 5452.4404296875\n",
      "Train Loss: 2424.5712890625\n",
      "Valid Loss: 6324.298828125\n",
      "Train Loss: 2430.524169921875\n",
      "Valid Loss: 6202.9931640625\n",
      "Train Loss: 2424.6298828125\n",
      "Valid Loss: 5773.57421875\n",
      "Train Loss: 2442.821533203125\n",
      "Valid Loss: 5432.69970703125\n",
      "Train Loss: 2421.19970703125\n",
      "Valid Loss: 5707.4208984375\n",
      "Train Loss: 2438.563232421875\n",
      "Valid Loss: 5432.64453125\n",
      "Train Loss: 2466.8828125\n",
      "Valid Loss: 5450.447265625\n",
      "Train Loss: 2453.53125\n",
      "Valid Loss: 5474.798828125\n",
      "Train Loss: 2468.3515625\n",
      "Valid Loss: 6323.50341796875\n",
      "Train Loss: 2448.947509765625\n",
      "Valid Loss: 5450.2373046875\n",
      "Train Loss: 2454.8251953125\n",
      "Valid Loss: 5733.69140625\n",
      "Train Loss: 2429.20263671875\n",
      "Valid Loss: 5513.578125\n",
      "Train Loss: 2444.598388671875\n",
      "Valid Loss: 6177.2041015625\n",
      "Train Loss: 2440.107666015625\n",
      "Valid Loss: 5699.296875\n",
      "Train Loss: 2454.47509765625\n",
      "Valid Loss: 5612.7265625\n",
      "Train Loss: 2444.79638671875\n",
      "Valid Loss: 5693.33447265625\n",
      "Train Loss: 2432.52294921875\n",
      "Valid Loss: 5473.40576171875\n",
      "Train Loss: 2433.54150390625\n",
      "Valid Loss: 5682.19775390625\n",
      "Train Loss: 2432.530517578125\n",
      "Valid Loss: 5572.73095703125\n",
      "Train Loss: 2435.13916015625\n",
      "Valid Loss: 6312.29931640625\n",
      "Train Loss: 2426.09814453125\n",
      "Valid Loss: 5449.802734375\n",
      "Train Loss: 2461.387939453125\n",
      "Valid Loss: 5453.0126953125\n",
      "Train Loss: 2458.832763671875\n",
      "Valid Loss: 5424.47412109375\n",
      "Train Loss: 2386.8466796875\n",
      "Valid Loss: 6189.810546875\n",
      "Train Loss: 2442.55224609375\n",
      "Valid Loss: 5722.71044921875\n",
      "Train Loss: 2409.16455078125\n",
      "Valid Loss: 6327.7734375\n",
      "Train Loss: 2446.760498046875\n",
      "Valid Loss: 6316.064453125\n",
      "Train Loss: 2389.49267578125\n",
      "Valid Loss: 5737.5048828125\n",
      "Train Loss: 2441.9189453125\n",
      "Valid Loss: 5513.09423828125\n",
      "Train Loss: 2462.1962890625\n",
      "Valid Loss: 5721.7744140625\n",
      "Train Loss: 2461.004150390625\n",
      "Valid Loss: 5503.07080078125\n",
      "Train Loss: 2445.2861328125\n",
      "Valid Loss: 5423.02490234375\n",
      "Train Loss: 2425.572265625\n",
      "Valid Loss: 5452.11376953125\n",
      "Train Loss: 2452.860595703125\n",
      "Valid Loss: 5447.48486328125\n",
      "Train Loss: 2449.863037109375\n",
      "Valid Loss: 6209.9375\n",
      "Train Loss: 2449.24560546875\n",
      "Valid Loss: 5541.501953125\n",
      "Train Loss: 2447.50146484375\n",
      "Valid Loss: 6323.25732421875\n",
      "Train Loss: 2436.81884765625\n",
      "Valid Loss: 5442.51904296875\n",
      "Train Loss: 2461.177001953125\n",
      "Valid Loss: 5732.6220703125\n",
      "Train Loss: 2427.41748046875\n",
      "Valid Loss: 5411.89453125\n",
      "Train Loss: 2394.57763671875\n",
      "Valid Loss: 5732.78564453125\n",
      "Train Loss: 2470.20849609375\n",
      "Valid Loss: 5449.42529296875\n",
      "Train Loss: 2446.899658203125\n",
      "Valid Loss: 6323.15771484375\n",
      "Train Loss: 2427.14501953125\n",
      "Valid Loss: 5571.9794921875\n",
      "Train Loss: 2458.9951171875\n",
      "Valid Loss: 6311.55859375\n",
      "Train Loss: 2415.422119140625\n",
      "Valid Loss: 6215.802734375\n",
      "Train Loss: 2437.826171875\n",
      "Valid Loss: 5576.53076171875\n",
      "Train Loss: 2448.720703125\n",
      "Valid Loss: 5472.32421875\n",
      "Train Loss: 2433.90380859375\n",
      "Valid Loss: 5472.43994140625\n",
      "Train Loss: 2465.67578125\n",
      "Valid Loss: 5713.9970703125\n",
      "Train Loss: 2444.97509765625\n",
      "Valid Loss: 5521.14453125\n",
      "Train Loss: 2427.491943359375\n",
      "Valid Loss: 6199.2099609375\n",
      "Train Loss: 2447.208984375\n",
      "Valid Loss: 5445.15771484375\n",
      "Train Loss: 2432.72314453125\n",
      "Valid Loss: 5688.0595703125\n",
      "Train Loss: 2433.4453125\n",
      "Valid Loss: 5532.46533203125\n",
      "Train Loss: 2430.180419921875\n",
      "Valid Loss: 6318.82421875\n",
      "Train Loss: 2417.468994140625\n",
      "Valid Loss: 5403.0693359375\n",
      "Train Loss: 2443.5595703125\n",
      "Valid Loss: 5724.82763671875\n",
      "Train Loss: 2425.681640625\n",
      "Valid Loss: 5389.81591796875\n",
      "Train Loss: 2456.162841796875\n",
      "Valid Loss: 5472.50537109375\n",
      "Train Loss: 2427.332275390625\n",
      "Valid Loss: 6324.2587890625\n",
      "Train Loss: 2463.387451171875\n",
      "Valid Loss: 5446.64306640625\n",
      "Train Loss: 2425.820556640625\n",
      "Valid Loss: 5770.97265625\n",
      "Train Loss: 2423.484619140625\n",
      "Valid Loss: 6199.91650390625\n",
      "Train Loss: 2429.297607421875\n",
      "Valid Loss: 5724.7041015625\n",
      "Train Loss: 2455.921630859375\n",
      "Valid Loss: 5771.2763671875\n",
      "Train Loss: 2430.459228515625\n",
      "Valid Loss: 5425.73388671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2418.501708984375\n",
      "Valid Loss: 5386.5546875\n",
      "Train Loss: 2441.634765625\n",
      "Valid Loss: 5539.83984375\n",
      "Train Loss: 2435.770751953125\n",
      "Valid Loss: 5448.17333984375\n",
      "Train Loss: 2426.34228515625\n",
      "Valid Loss: 5448.27294921875\n",
      "Train Loss: 2457.53466796875\n",
      "Valid Loss: 5415.00634765625\n",
      "Train Loss: 2412.841064453125\n",
      "Valid Loss: 5616.39453125\n",
      "Train Loss: 2441.207275390625\n",
      "Valid Loss: 6322.10791015625\n",
      "Train Loss: 2428.130126953125\n",
      "Valid Loss: 5469.46484375\n",
      "Train Loss: 2417.96630859375\n",
      "Valid Loss: 5724.3203125\n",
      "Train Loss: 2436.327392578125\n",
      "Valid Loss: 5448.26513671875\n",
      "Train Loss: 2452.361328125\n",
      "Valid Loss: 5697.15234375\n",
      "Train Loss: 2440.962158203125\n",
      "Valid Loss: 5449.23876953125\n",
      "Train Loss: 2410.1123046875\n",
      "Valid Loss: 5687.2275390625\n",
      "Train Loss: 2446.984619140625\n",
      "Valid Loss: 5697.2392578125\n",
      "Train Loss: 2406.78173828125\n",
      "Valid Loss: 5447.2216796875\n",
      "Train Loss: 2453.379150390625\n",
      "Valid Loss: 5539.82177734375\n",
      "Train Loss: 2364.12353515625\n",
      "Valid Loss: 5723.81640625\n",
      "Train Loss: 2419.265380859375\n",
      "Valid Loss: 6198.18994140625\n",
      "Train Loss: 2465.80517578125\n",
      "Valid Loss: 5723.931640625\n",
      "Train Loss: 2389.272705078125\n",
      "Valid Loss: 5722.9619140625\n",
      "Train Loss: 2446.31298828125\n",
      "Valid Loss: 5502.771484375\n",
      "Train Loss: 2435.85546875\n",
      "Valid Loss: 5433.5595703125\n",
      "Train Loss: 2449.312744140625\n",
      "Valid Loss: 5420.03564453125\n",
      "Train Loss: 2418.58203125\n",
      "Valid Loss: 5768.978515625\n",
      "Train Loss: 2445.57470703125\n",
      "Valid Loss: 5593.013671875\n",
      "Train Loss: 2424.82373046875\n",
      "Valid Loss: 5538.388671875\n",
      "Train Loss: 2442.345703125\n",
      "Valid Loss: 5414.2744140625\n",
      "Train Loss: 2432.979248046875\n",
      "Valid Loss: 5450.52734375\n",
      "Train Loss: 2432.820068359375\n",
      "Valid Loss: 6204.3115234375\n",
      "Train Loss: 2423.53759765625\n",
      "Valid Loss: 6313.904296875\n",
      "Train Loss: 2433.13037109375\n",
      "Valid Loss: 5704.44970703125\n",
      "Train Loss: 2443.27099609375\n",
      "Valid Loss: 5445.36962890625\n",
      "Train Loss: 2415.143310546875\n",
      "Valid Loss: 5567.58056640625\n",
      "Train Loss: 2428.75341796875\n",
      "Valid Loss: 6321.294921875\n",
      "Train Loss: 2422.4384765625\n",
      "Valid Loss: 5723.2939453125\n",
      "Train Loss: 2438.082275390625\n",
      "Valid Loss: 5615.00830078125\n",
      "Train Loss: 2399.44384765625\n",
      "Valid Loss: 6194.19189453125\n",
      "Train Loss: 2427.23779296875\n",
      "Valid Loss: 6303.7001953125\n",
      "Train Loss: 2431.03662109375\n",
      "Valid Loss: 5446.79931640625\n",
      "Train Loss: 2437.55078125\n",
      "Valid Loss: 5770.1357421875\n",
      "Train Loss: 2434.749755859375\n",
      "Valid Loss: 6212.12255859375\n",
      "Train Loss: 2427.52197265625\n",
      "Valid Loss: 5411.23193359375\n",
      "Train Loss: 2402.307373046875\n",
      "Valid Loss: 6206.265625\n",
      "Train Loss: 2442.718017578125\n",
      "Valid Loss: 5588.400390625\n",
      "Train Loss: 2453.88525390625\n",
      "Valid Loss: 6320.912109375\n",
      "Train Loss: 2432.69873046875\n",
      "Valid Loss: 6174.12646484375\n",
      "Train Loss: 2450.9267578125\n",
      "Valid Loss: 5446.2978515625\n",
      "Train Loss: 2373.0859375\n",
      "Valid Loss: 5448.0146484375\n",
      "Train Loss: 2434.24853515625\n",
      "Valid Loss: 5384.91845703125\n",
      "Train Loss: 2411.106689453125\n",
      "Valid Loss: 5734.54345703125\n",
      "Train Loss: 2415.92138671875\n",
      "Valid Loss: 5720.94482421875\n",
      "Train Loss: 2436.4755859375\n",
      "Valid Loss: 5415.0810546875\n",
      "Train Loss: 2399.55419921875\n",
      "Valid Loss: 6202.09716796875\n",
      "Train Loss: 2439.597900390625\n",
      "Valid Loss: 5483.2431640625\n",
      "Train Loss: 2438.38427734375\n",
      "Valid Loss: 5446.39501953125\n",
      "Train Loss: 2465.76806640625\n",
      "Valid Loss: 5719.68701171875\n",
      "Train Loss: 2437.056640625\n",
      "Valid Loss: 5446.58740234375\n",
      "Train Loss: 2397.59375\n",
      "Valid Loss: 5439.65625\n",
      "Train Loss: 2431.79833984375\n",
      "Valid Loss: 5465.638671875\n",
      "Train Loss: 2432.051513671875\n",
      "Valid Loss: 5722.5068359375\n",
      "Train Loss: 2400.592041015625\n",
      "Valid Loss: 5509.91748046875\n",
      "Train Loss: 2462.048583984375\n",
      "Valid Loss: 5722.4345703125\n",
      "Train Loss: 2450.276123046875\n",
      "Valid Loss: 5719.4267578125\n",
      "Train Loss: 2434.119140625\n",
      "Valid Loss: 6313.67724609375\n",
      "Train Loss: 2458.81640625\n",
      "Valid Loss: 5519.50048828125\n",
      "Train Loss: 2462.06396484375\n",
      "Valid Loss: 5606.90576171875\n",
      "Train Loss: 2432.44677734375\n",
      "Valid Loss: 6200.67578125\n",
      "Train Loss: 2459.5498046875\n",
      "Valid Loss: 5446.07470703125\n",
      "Train Loss: 2446.723388671875\n",
      "Valid Loss: 5612.93115234375\n",
      "Train Loss: 2424.337646484375\n",
      "Valid Loss: 5538.25146484375\n",
      "Train Loss: 2390.7998046875\n",
      "Valid Loss: 5448.43310546875\n",
      "Train Loss: 2456.568603515625\n",
      "Valid Loss: 5614.03955078125\n",
      "Train Loss: 2452.555908203125\n",
      "Valid Loss: 5401.94384765625\n",
      "Train Loss: 2406.026611328125\n",
      "Valid Loss: 5721.828125\n",
      "Train Loss: 2434.60302734375\n",
      "Valid Loss: 6304.53076171875\n",
      "Train Loss: 2457.3076171875\n",
      "Valid Loss: 5661.72509765625\n",
      "Train Loss: 2406.75830078125\n",
      "Valid Loss: 5537.70703125\n",
      "Train Loss: 2436.26123046875\n",
      "Valid Loss: 5448.7099609375\n",
      "Train Loss: 2413.8818359375\n",
      "Valid Loss: 5739.6552734375\n",
      "Train Loss: 2432.2353515625\n",
      "Valid Loss: 5443.6298828125\n",
      "Train Loss: 2413.21435546875\n",
      "Valid Loss: 5444.89990234375\n",
      "Train Loss: 2449.617431640625\n",
      "Valid Loss: 5767.93798828125\n",
      "Train Loss: 2433.770751953125\n",
      "Valid Loss: 5409.70068359375\n",
      "Train Loss: 2427.66943359375\n",
      "Valid Loss: 5536.83154296875\n",
      "Train Loss: 2451.986328125\n",
      "Valid Loss: 5466.74169921875\n",
      "Train Loss: 2440.818115234375\n",
      "Valid Loss: 5720.544921875\n",
      "Train Loss: 2435.871826171875\n",
      "Valid Loss: 6316.29052734375\n",
      "Train Loss: 2433.99072265625\n",
      "Valid Loss: 6308.83349609375\n",
      "Train Loss: 2452.778564453125\n",
      "Valid Loss: 5500.58203125\n",
      "Train Loss: 2387.309814453125\n",
      "Valid Loss: 5717.5146484375\n",
      "Train Loss: 2446.26513671875\n",
      "Valid Loss: 6208.20654296875\n",
      "Train Loss: 2440.012939453125\n",
      "Valid Loss: 5443.7177734375\n",
      "Train Loss: 2411.55029296875\n",
      "Valid Loss: 5717.8505859375\n",
      "Train Loss: 2414.393798828125\n",
      "Valid Loss: 5612.84326171875\n",
      "Train Loss: 2439.12841796875\n",
      "Valid Loss: 5536.4208984375\n",
      "Train Loss: 2410.81787109375\n",
      "Valid Loss: 6319.0361328125\n",
      "Train Loss: 2461.26171875\n",
      "Valid Loss: 6318.9970703125\n",
      "Train Loss: 2457.109619140625\n",
      "Valid Loss: 5613.06005859375\n",
      "Train Loss: 2434.9814453125\n",
      "Valid Loss: 5467.2470703125\n",
      "Train Loss: 2348.40234375\n",
      "Valid Loss: 5567.6640625\n",
      "Train Loss: 2435.402587890625\n",
      "Valid Loss: 5766.12744140625\n",
      "Train Loss: 2406.9365234375\n",
      "Valid Loss: 5721.00927734375\n",
      "Train Loss: 2448.9970703125\n",
      "Valid Loss: 6196.4375\n",
      "Train Loss: 2446.27392578125\n",
      "Valid Loss: 5767.62109375\n",
      "Train Loss: 2456.4638671875\n",
      "Valid Loss: 5607.92333984375\n",
      "Train Loss: 2410.052978515625\n",
      "Valid Loss: 6196.169921875\n",
      "Train Loss: 2434.5\n",
      "Valid Loss: 5468.8984375\n",
      "Train Loss: 2440.393310546875\n",
      "Valid Loss: 5709.4501953125\n",
      "Train Loss: 2415.50830078125\n",
      "Valid Loss: 5442.6708984375\n",
      "Train Loss: 2445.377197265625\n",
      "Valid Loss: 5767.4814453125\n",
      "Train Loss: 2412.26708984375\n",
      "Valid Loss: 6199.048828125\n",
      "Train Loss: 2426.712646484375\n",
      "Valid Loss: 5689.10009765625\n",
      "Train Loss: 2393.186767578125\n",
      "Valid Loss: 5467.2421875\n",
      "Min RMSE: 4308.49609375\n"
     ]
    }
   ],
   "source": [
    "Train(1000, model, train_loader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb56e952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f81df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
